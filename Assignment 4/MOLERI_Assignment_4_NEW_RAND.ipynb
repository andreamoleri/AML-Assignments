{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "metadata": {
    "id": "usyApjqJLDbk"
   },
   "cell_type": "markdown",
   "source": [
    "# Next Character Prediction\n",
    "_Student 902011, Moleri Andrea, a.moleri@campus.unimib.it_\n",
    "\n",
    "The task is next character prediction in a text. Starting from the provided skeleton of the code:\n",
    "\n",
    "- properly divide the sequences into training, validation and test. Eventually use an external text for the test to assess the generalization ability\n",
    "- evaluate the trained model in terms of prediction accuracy\n",
    "- tune the chunk length to obtain the best performance\n",
    "- modify the network architecture to obtain the best performance\n",
    "\n",
    "In aggiunta:\n",
    "\n",
    "Comunque vuole sia il notebook che la stampa in PDF del notebook"
   ]
  },
  {
   "metadata": {
    "id": "JDOPy7MULDbo"
   },
   "cell_type": "markdown",
   "source": []
  },
  {
   "metadata": {
    "id": "avJywcNPLDbp"
   },
   "cell_type": "markdown",
   "source": [
    "### Inspecting the Data\n",
    "\n",
    "The assignment we are about to conduct is based on Dante Alighieri's \"The Divine Comedy\", which will serve as the dataset for the analysis in question. First, the text is downloaded from my personal GitHub repository, where it is stored in RAW format, in order to avoid compatibility problems for users who access the notebook from Google Colaboratory and do not have the required file saved locally. The text has then been preprocessed by converting all characters to lowercase to ensure uniformity and prevent discrepancies due to case sensitivity. The analysis begins with a basic examination of the text's characteristics, focusing on dimensionality and distribution. The length of the entire text is `558,240` characters. `40` unique characters are identified across the text, which include standard alphabetic characters, punctuation, and whitespace.\n",
    "\n",
    "Next, sequences of `30` consecutive characters were extracted from the text by moving through the text with a step size of `3` characters at a time. This means that each sequence of `30` characters overlaps with the previous one by `27` characters, as only `3` new characters are included at each step. This sliding window approach allows for the extraction of subsequences that capture local patterns within the text. In total, `186,070` sequences (`558,240/3`) were generated using this method. These sequences represent sliding windows across the text, which will be useful in a short while for predictive modeling. Further observations shows that the most frequently occurring characters, notably whitespaces and common vowels such as *e*, *a*, and *i*, dominate the data distribution. This frequency distribution is visualized with a bar chart showing the `10` most frequent characters in the text corpus. After the results of this analysis, we can move on to the data preparation phase."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Wij-6Z9O_Ch-",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "39b1a1f6-1ef9-4efa-b1b7-ae1df65940d4",
    "ExecuteTime": {
     "end_time": "2024-12-10T13:01:55.862995Z",
     "start_time": "2024-12-10T13:01:53.999329Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# URL of the text file containing Dante's \"La Divina Commedia\"\n",
    "url = 'https://raw.githubusercontent.com/andreamoleri/AML-Assignments/refs/heads/main/Assignment%204/divina_commedia.txt'\n",
    "\n",
    "# Download and preprocess the text\n",
    "text = requests.get(url).text.lower()\n",
    "\n",
    "# Display text length and a sample of the text\n",
    "print(f\"Text Length:\\n{'-' * 35}\\n{len(text)} characters\")\n",
    "print(f\"\\nText Sample:\\n{'-' * 35}\\n{text[30:828]}\")\n",
    "\n",
    "# Analyze unique characters in the text\n",
    "chars = sorted(set(text))\n",
    "print(f\"Total Unique Characters:\\n{'-' * 35}\\n{len(chars)}\")\n",
    "\n",
    "# Create character mappings\n",
    "char_indices = {c: i for i, c in enumerate(chars)}\n",
    "indices_char = {i: c for i, c in enumerate(chars)}\n",
    "\n",
    "print(f\"\\nCharacter-to-Index Mapping:\\n{'-' * 35}\")\n",
    "for char, idx in char_indices.items():\n",
    "    print(f\"'{char}': {idx}\")\n",
    "\n",
    "# Define sequence parameters\n",
    "maxlen, step = 30, 3\n",
    "\n",
    "# Generate sequences and corresponding next characters\n",
    "sequences = [(text[i: i + maxlen], text[i + maxlen]) for i in range(0, len(text) - maxlen, step)]\n",
    "sentences, next_chars = zip(*sequences) if sequences else ([], [])\n",
    "\n",
    "print(f\"\\nNumber of Sequences Generated:\\n{'-' * 35}\\n{len(sentences)}\")\n",
    "\n",
    "if sentences:\n",
    "    print(f\"\\nExample of a Sentence:\\n{'-' * 35}\\nSentence: '{sentences[11]}'\\nNext Character: '{next_chars[11]}'\")\n",
    "\n",
    "# Count character frequencies\n",
    "char_counts = Counter(text)\n",
    "sorted_counts = char_counts.most_common()\n",
    "\n",
    "# Display top 10 most frequent characters\n",
    "print(f\"\\nTop 10 Most Frequent Characters:\\n{'-' * 35}\")\n",
    "for char, count in sorted_counts[:10]:\n",
    "    print(f\"{repr(char):>5}: {count} occurrences\")\n",
    "\n",
    "# Plot top 10 most frequent characters\n",
    "char_labels, char_freqs = zip(*sorted_counts[:10])\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(char_labels, char_freqs, color='skyblue')\n",
    "plt.title('Top 10 Most Frequent Characters', fontsize=14)\n",
    "plt.xlabel('Characters', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Length:\n",
      "-----------------------------------\n",
      "558240 characters\n",
      "\n",
      "Text Sample:\n",
      "-----------------------------------\n",
      "nel mezzo del cammin di nostra vita\n",
      "  mi ritrovai per una selva oscura\n",
      "  che' la diritta via era smarrita.\n",
      "\n",
      "ahi quanto a dir qual era e` cosa dura\n",
      "  esta selva selvaggia e aspra e forte\n",
      "  che nel pensier rinova la paura!\n",
      "\n",
      "tant'e` amara che poco e` piu` morte;\n",
      "  ma per trattar del ben ch'i' vi trovai,\n",
      "  diro` de l'altre cose ch'i' v'ho scorte.\n",
      "\n",
      "io non so ben ridir com'i' v'intrai,\n",
      "  tant'era pien di sonno a quel punto\n",
      "  che la verace via abbandonai.\n",
      "\n",
      "ma poi ch'i' fui al pie` d'un colle giunto,\n",
      "  la` dove terminava quella valle\n",
      "  che m'avea di paura il cor compunto,\n",
      "\n",
      "guardai in alto, e vidi le sue spalle\n",
      "  vestite gia` de' raggi del pianeta\n",
      "  che mena dritto altrui per ogne calle.\n",
      "\n",
      "allor fu la paura un poco queta\n",
      "  che nel lago del cor m'era durata\n",
      "  la notte ch'i' passai con tanta pieta.\n",
      "\n",
      "Total Unique Characters:\n",
      "-----------------------------------\n",
      "40\n",
      "\n",
      "Character-to-Index Mapping:\n",
      "-----------------------------------\n",
      "'\n",
      "': 0\n",
      "' ': 1\n",
      "'!': 2\n",
      "'\"': 3\n",
      "''': 4\n",
      "'(': 5\n",
      "')': 6\n",
      "',': 7\n",
      "'-': 8\n",
      "'.': 9\n",
      "':': 10\n",
      "';': 11\n",
      "'<': 12\n",
      "'>': 13\n",
      "'?': 14\n",
      "'`': 15\n",
      "'a': 16\n",
      "'b': 17\n",
      "'c': 18\n",
      "'d': 19\n",
      "'e': 20\n",
      "'f': 21\n",
      "'g': 22\n",
      "'h': 23\n",
      "'i': 24\n",
      "'j': 25\n",
      "'l': 26\n",
      "'m': 27\n",
      "'n': 28\n",
      "'o': 29\n",
      "'p': 30\n",
      "'q': 31\n",
      "'r': 32\n",
      "'s': 33\n",
      "'t': 34\n",
      "'u': 35\n",
      "'v': 36\n",
      "'x': 37\n",
      "'y': 38\n",
      "'z': 39\n",
      "\n",
      "Number of Sequences Generated:\n",
      "-----------------------------------\n",
      "186070\n",
      "\n",
      "Example of a Sentence:\n",
      "-----------------------------------\n",
      "Sentence: ' mezzo del cammin di nostra vi'\n",
      "Next Character: 't'\n",
      "\n",
      "Top 10 Most Frequent Characters:\n",
      "-----------------------------------\n",
      "  ' ': 101150 occurrences\n",
      "  'e': 48639 occurrences\n",
      "  'a': 43487 occurrences\n",
      "  'i': 41658 occurrences\n",
      "  'o': 38827 occurrences\n",
      "  'n': 26664 occurrences\n",
      "  'r': 26066 occurrences\n",
      "  'l': 23508 occurrences\n",
      "  't': 22907 occurrences\n",
      "  's': 22663 occurrences\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "id": "y0SaydPELDbs"
   },
   "cell_type": "markdown",
   "source": [
    "### Preparing the Data\n",
    "\n",
    "The preprocessing stage involves preparing sequences from the text data for training, validation, and testing purposes. This phase builds on the previous data inspection results and ensures the sequences are appropriately structured for machine learning models. The initial steps map each unique character into integer indices. As previously discussed, the data is tokenized into sequences of `30` characters, with a step of `3` between each sequence. This choice tries to balance sequence length for context learning while preventing highly overlapping sequences that could lead to redundancies. The text sequences (`sentences`) and their corresponding target characters (`next_chars`) are one-hot encoded into arrays `x` and `y`. Here, each character is transformed into a binary vector representation across `40` unique characters, with `40` being the total number of unique characters identified during the data inspection phase. After encoding, the sequences are split into training, validation, and test subsets. The proportions are set as follows:\n",
    "\n",
    "- `80%` **for Training**: used to train the machine learning model. As a consequence, the training set is made up of `148,856` sequences with shapes `(30, 40)`\n",
    "- `10%` **for Validation**: used to tune model hyperparameters and prevent overfitting during training. As a consequence, the validation set is made up of `18,607` sequences with shapes `(30, 40)`\n",
    "- `10%` **for Testing**: reserved to assess how well the trained model generalizes on unseen data. As a consequence, the test set is made up of `18,607` sequences with shapes `(30, 40)`"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Define the maximum sequence length to be used for training\n",
    "maxlen = 30\n",
    "\n",
    "# Obtain the unique characters in the text and sort them to ensure consistent encoding\n",
    "chars = sorted(set(text))\n",
    "# Map each unique character to a unique integer index.\n",
    "char_indices = {char: i for i, char in enumerate(chars)}\n",
    "# Map each integer index back to its corresponding character.\n",
    "indices_char = {i: char for char, i in char_indices.items()}\n",
    "# Display the total number of unique characters.\n",
    "print(f\"Total characters: {len(chars)}\")\n",
    "\n",
    "# Initialize lists to store input sequences and their corresponding target characters\n",
    "sentences = []\n",
    "next_chars = []\n",
    "\n",
    "# Generate sequences of length `maxlen` with their respective next characters as targets\n",
    "# Loop with a step of 3 to avoid overly similar sequences\n",
    "for i in range(0, len(text) - maxlen, 3):\n",
    "    sentences.append(text[i : i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "\n",
    "# Display the total number of sequences generated for training\n",
    "print(f\"Number of sequences generated: {len(sentences)}\")\n",
    "\n",
    "# Initialize the input (x) and output (y) arrays for training with boolean values\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=bool)\n",
    "\n",
    "# One-hot encode the sequences and their respective next characters\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1  # One-hot encode the input sequence characters\n",
    "    y[i, char_indices[next_chars[i]]] = 1  # One-hot encode the target characters\n",
    "\n",
    "# Display the shapes of the input and output arrays to verify their structure\n",
    "print('\\nInput (x) and Output (y) shapes:')\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "\n",
    "# Define the proportions of data to be used for training, validation, and testing splits\n",
    "train_split = 0.8\n",
    "val_split = 0.1\n",
    "\n",
    "# Calculate the sizes of the training and validation sets based on the total number of sequences\n",
    "train_size = int(len(sentences) * train_split)\n",
    "val_size = int(len(sentences) * val_split)\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "x_train, y_train = x[:train_size], y[:train_size]\n",
    "x_val, y_val = x[train_size:train_size + val_size], y[train_size:train_size + val_size]\n",
    "x_test, y_test = x[train_size + val_size:], y[train_size + val_size:]\n",
    "\n",
    "# Display the size of each data split to ensure proper partitioning\n",
    "print('\\nData split:')\n",
    "print(f'Training set: {x_train.shape}, {y_train.shape}')\n",
    "print(f'Validation set: {x_val.shape}, {y_val.shape}')\n",
    "print(f'Test set: {x_test.shape}, {y_test.shape}')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VDS1iS9YLKG6",
    "outputId": "6963af57-66e7-4a94-921c-5ef467259234",
    "ExecuteTime": {
     "end_time": "2024-12-10T13:01:59.321757Z",
     "start_time": "2024-12-10T13:01:57.193842Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 40\n",
      "Number of sequences generated: 186070\n",
      "\n",
      "Input (x) and Output (y) shapes:\n",
      "(186070, 30, 40)\n",
      "(186070, 40)\n",
      "\n",
      "Data split:\n",
      "Training set: (148856, 30, 40), (148856, 40)\n",
      "Validation set: (18607, 30, 40), (18607, 40)\n",
      "Test set: (18607, 30, 40), (18607, 40)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "import random\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define the hyperparameter search space\n",
    "param_grid = {\n",
    "    'chunk_length': [30, 35, 40, 50],\n",
    "    'lstm_units': [64, 128, 256],\n",
    "    'dropout_rate': [0.0, 0.2, 0.3],\n",
    "    'learning_rate': [0.01, 0.005, 0.001],\n",
    "    'layers': [1, 2],\n",
    "}\n",
    "\n",
    "# Randomly select hyperparameters for each model\n",
    "n_models = 3\n",
    "random_configs = [\n",
    "    {\n",
    "        'chunk_length': random.choice(param_grid['chunk_length']),\n",
    "        'lstm_units': random.choice(param_grid['lstm_units']),\n",
    "        'dropout_rate': random.choice(param_grid['dropout_rate']),\n",
    "        'learning_rate': random.choice(param_grid['learning_rate']),\n",
    "        'layers': random.choice(param_grid['layers']),\n",
    "    }\n",
    "    for _ in range(n_models)\n",
    "]\n",
    "\n",
    "models = []\n",
    "\n",
    "for i, config in enumerate(random_configs):\n",
    "    print(f\"\\nBuilding Model {i + 1} with config: {config}...\")\n",
    "\n",
    "    # Adjust data based on chunk length\n",
    "    chunk_length = config['chunk_length']\n",
    "    x_train_mod = x_train[:, :chunk_length, :]\n",
    "    x_val_mod = x_val[:, :chunk_length, :]\n",
    "    x_test_mod = x_test[:, :chunk_length, :]\n",
    "\n",
    "    # Build the model\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add LSTM layers\n",
    "    if config['layers'] == 1:\n",
    "        model.add(LSTM(config['lstm_units'], input_shape=(chunk_length, len(chars))))\n",
    "    else:\n",
    "        model.add(LSTM(config['lstm_units'], input_shape=(chunk_length, len(chars)), return_sequences=True))\n",
    "        model.add(Dropout(config['dropout_rate']))  # Add dropout between layers\n",
    "        model.add(LSTM(config['lstm_units'] // 2))  # Halve the units for the second layer\n",
    "\n",
    "    # Add output layer\n",
    "    model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "    # Compile the model\n",
    "    optimizer = tf.keras.optimizers.RMSprop(learning_rate=config['learning_rate'])\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "    models.append(model)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 940
    },
    "id": "PFKiXfmYMoGA",
    "outputId": "bf5e59f3-94d3-4c8c-b1ce-0a2caa692a6c",
    "ExecuteTime": {
     "end_time": "2024-12-10T13:02:01.941727Z",
     "start_time": "2024-12-10T13:02:00.624274Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building Model 1 with config: {'chunk_length': 50, 'lstm_units': 128, 'dropout_rate': 0.0, 'learning_rate': 0.01, 'layers': 1}...\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 128)               86528     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 40)                5160      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 91688 (358.16 KB)\n",
      "Trainable params: 91688 (358.16 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "\n",
      "Building Model 2 with config: {'chunk_length': 50, 'lstm_units': 64, 'dropout_rate': 0.3, 'learning_rate': 0.01, 'layers': 2}...\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_1 (LSTM)               (None, 50, 64)            26880     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 50, 64)            0         \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 32)                12416     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 40)                1320      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 40616 (158.66 KB)\n",
      "Trainable params: 40616 (158.66 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "\n",
      "Building Model 3 with config: {'chunk_length': 30, 'lstm_units': 64, 'dropout_rate': 0.2, 'learning_rate': 0.01, 'layers': 1}...\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_3 (LSTM)               (None, 64)                26880     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 40)                2600      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 29480 (115.16 KB)\n",
      "Trainable params: 29480 (115.16 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZXiFyktNLDbu",
    "outputId": "bca4fd52-2f57-4462-c91a-21870ebc7d18",
    "ExecuteTime": {
     "end_time": "2024-12-10T13:03:56.405111Z",
     "start_time": "2024-12-10T13:03:54.192193Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import LambdaCallback\n",
    "import sys\n",
    "\n",
    "# Helper function to test text generation after each epoch\n",
    "def testAfterEpoch(model, text, char_indices, indices_char, maxlen, epoch, _):\n",
    "    print()\n",
    "    print(f\"******* Epoch: {epoch+1} ********\")\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    generated = ''\n",
    "    sentence = text[start_index: start_index + maxlen]\n",
    "    generated += sentence\n",
    "\n",
    "    print(\"***** Starting sentence *****\")\n",
    "    print(sentence)\n",
    "    print(\"*****************************\")\n",
    "\n",
    "    for _ in range(400):  # Generate 400 characters\n",
    "        x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x_pred[0, t, char_indices[char]] = 1\n",
    "\n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = np.argmax(preds)\n",
    "        next_char = indices_char[next_index]\n",
    "\n",
    "        sentence = sentence[1:] + next_char\n",
    "        generated += next_char\n",
    "\n",
    "    print(\"***** Generated text *****\")\n",
    "    print(generated)\n",
    "    print()\n",
    "\n",
    "# Function to prepare data for a given chunk length\n",
    "def prepare_data_for_chunk_length(text, chars, char_indices, maxlen, step=3):\n",
    "    sentences = []\n",
    "    next_chars = []\n",
    "    for i in range(0, len(text) - maxlen, step):\n",
    "        sentences.append(text[i: i + maxlen])\n",
    "        next_chars.append(text[i + maxlen])\n",
    "    x = np.zeros((len(sentences), maxlen, len(chars)), dtype=bool)\n",
    "    y = np.zeros((len(sentences), len(chars)), dtype=bool)\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        for t, char in enumerate(sentence):\n",
    "            x[i, t, char_indices[char]] = 1\n",
    "        y[i, char_indices[next_chars[i]]] = 1\n",
    "    return x, y\n",
    "\n",
    "# Function to train a model and store the training history for plotting\n",
    "def train_and_store_history(model, x, y, text, char_indices, indices_char, maxlen, batch_size, epochs, model_name):\n",
    "    # Split the data into training, validation, and test sets\n",
    "    split_train = int(len(x) * 0.8)\n",
    "    split_val = int(len(x) * 0.9)\n",
    "    x_train, y_train = x[:split_train], y[:split_train]\n",
    "    x_val, y_val = x[split_train:split_val], y[split_train:split_val]\n",
    "    x_test, y_test = x[split_val:], y[split_val:]\n",
    "\n",
    "    print(f\"Training {model_name}...\")\n",
    "    print_callback = LambdaCallback(on_epoch_end=lambda epoch, _: testAfterEpoch(model, text, char_indices, indices_char, maxlen, epoch, _))\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        x_train, y_train,\n",
    "        validation_data=(x_val, y_val),\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        callbacks=[print_callback],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Evaluate on test set\n",
    "    test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print(f\"\\nTest Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "    return history, test_loss, test_accuracy\n",
    "\n",
    "# Plot training histories\n",
    "def plot_all_histories(histories, model_names):\n",
    "    plt.figure(figsize=(12, 18))\n",
    "    for i, (history, name) in enumerate(zip(histories, model_names)):\n",
    "        plt.subplot(len(histories), 2, 2 * i + 1)\n",
    "        plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "        plt.title(f'{name} - Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(len(histories), 2, 2 * i + 2)\n",
    "        plt.plot(history.history['loss'], label='Train Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.title(f'{name} - Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Train and evaluate each model\n",
    "batch_size = 2048\n",
    "epochs = 2\n",
    "\n",
    "histories = []\n",
    "model_names = []\n",
    "results = []\n",
    "\n",
    "\n",
    "# Model 1 - Chunk length = 30\n",
    "x_1, y_1 = prepare_data_for_chunk_length(text, chars, char_indices, maxlen=30)\n",
    "print(\"Evaluating Model 1...\")\n",
    "history_1, test_loss_1, test_accuracy_1 = train_and_store_history(model_1, x_1, y_1, text, char_indices, indices_char, 30, batch_size, epochs, \"Model 1\")\n",
    "histories.append(history_1)\n",
    "model_names.append(\"Model 1\")\n",
    "results.append((\"Model 1\", test_loss_1, test_accuracy_1))\n",
    "\n",
    "# Model 2 - Chunk length = 50\n",
    "x_2, y_2 = prepare_data_for_chunk_length(text, chars, char_indices, maxlen=35)\n",
    "print(\"\\nEvaluating Model 2...\")\n",
    "history_2, test_loss_2, test_accuracy_2 = train_and_store_history(model_2, x_2, y_2, text, char_indices, indices_char, 35, batch_size, epochs, \"Model 2\")\n",
    "histories.append(history_2)\n",
    "model_names.append(\"Model 2\")\n",
    "results.append((\"Model 2\", test_loss_2, test_accuracy_2))\n",
    "\n",
    "# Model 3 - Chunk length = 70\n",
    "x_3, y_3 = prepare_data_for_chunk_length(text, chars, char_indices, maxlen=40)\n",
    "print(\"\\nEvaluating Model 3...\")\n",
    "history_3, test_loss_3, test_accuracy_3 = train_and_store_history(model_3, x_3, y_3, text, char_indices, indices_char, 40, batch_size, epochs, \"Model 3\")\n",
    "histories.append(history_3)\n",
    "model_names.append(\"Model 3\")\n",
    "results.append((\"Model 3\", test_loss_3, test_accuracy_3))\n",
    "\n",
    "# Plot all histories together at the end\n",
    "plot_all_histories(histories, model_names)\n",
    "\n",
    "# Test Results Section\n",
    "print(\"\\nTest Results:\\n\")\n",
    "results.sort(key=lambda x: x[2], reverse=False)\n",
    "for model_name, test_loss, test_accuracy in results:\n",
    "    print(f\"{model_name} - Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Model 1...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 113\u001B[0m\n\u001B[1;32m    111\u001B[0m x_1, y_1 \u001B[38;5;241m=\u001B[39m prepare_data_for_chunk_length(text, chars, char_indices, maxlen\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m30\u001B[39m)\n\u001B[1;32m    112\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEvaluating Model 1...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 113\u001B[0m history_1, test_loss_1, test_accuracy_1 \u001B[38;5;241m=\u001B[39m train_and_store_history(model_1, x_1, y_1, text, char_indices, indices_char, \u001B[38;5;241m30\u001B[39m, batch_size, epochs, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel 1\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    114\u001B[0m histories\u001B[38;5;241m.\u001B[39mappend(history_1)\n\u001B[1;32m    115\u001B[0m model_names\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel 1\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'model_1' is not defined"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "import sys\n",
    "def testAfterEpoch(epoch, _):\n",
    "  print()\n",
    "  print()\n",
    "  print('******* Epoch: %d ********' % (epoch+1))\n",
    "  start_index = random.randint(0, len(text)-maxlen-1)\n",
    "\n",
    "  generated = ''\n",
    "  sentence = text[start_index :  start_index + maxlen]\n",
    "  generated = generated + sentence\n",
    "\n",
    "  print('***** starting sentence *****')\n",
    "  print(sentence)\n",
    "  print('*****************************')\n",
    "  sys.stdout.write(generated)\n",
    "\n",
    "  for i in range(400):\n",
    "    x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "    for t, char in enumerate(sentence):\n",
    "      x_pred[0, t, char_indices[char]] = 1\n",
    "\n",
    "    preds = model.predict(x_pred, verbose=0)[0]\n",
    "    next_index = np.argmax(preds)\n",
    "    next_char = indices_char[next_index]\n",
    "\n",
    "    sentence = sentence[1:] + next_char\n",
    "\n",
    "    sys.stdout.write(next_char)\n",
    "    sys.stdout.flush()\n",
    "  print()\n",
    "\n"
   ],
   "metadata": {
    "id": "GtZFpefPNw7y",
    "ExecuteTime": {
     "end_time": "2024-12-09T08:16:31.896488Z",
     "start_time": "2024-12-09T08:16:31.890342Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "print_callback = LambdaCallback(on_epoch_end=testAfterEpoch)"
   ],
   "metadata": {
    "id": "RWKmfaQ8QHa1",
    "ExecuteTime": {
     "end_time": "2024-12-09T08:16:35.003406Z",
     "start_time": "2024-12-09T08:16:34.999686Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "model.fit(x, y,\n",
    "          batch_size = 2048,\n",
    "          epochs = 20,\n",
    "          callbacks = [print_callback])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "N2cdYpFbQUqK",
    "outputId": "ee486ee2-4ae6-44cf-f3fe-12470ff9c593",
    "ExecuteTime": {
     "end_time": "2024-12-09T08:46:09.003521Z",
     "start_time": "2024-12-09T08:16:35.768664Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "91/91 [==============================] - ETA: 0s - loss: 2.6218\n",
      "\n",
      "******* Epoch: 1 ********\n",
      "***** starting sentence *****\n",
      "olui che tutto 'l mondo alluma\n",
      "*****************************\n",
      "olui che tutto 'l mondo alluma la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la \n",
      "91/91 [==============================] - 75s 806ms/step - loss: 2.6218\n",
      "Epoch 2/20\n",
      "91/91 [==============================] - ETA: 0s - loss: 2.0454\n",
      "\n",
      "******* Epoch: 2 ********\n",
      "***** starting sentence *****\n",
      "e o lievemente o forte>>.\n",
      "\n",
      "que\n",
      "*****************************\n",
      "e o lievemente o forte>>.\n",
      "\n",
      "quen la per la ser con per la per la per la per la per la per la per la per la per la per la per la per la per la per la per la per la per la per la per la per la per la per la per la per la per la per la per la per la per la per la per la per la per la per la per la per la per la per la per la per la per la per la per la per la per la per la per la per la per la per la per la per la per la per la pe\n",
      "91/91 [==============================] - 83s 920ms/step - loss: 2.0454\n",
      "Epoch 3/20\n",
      "91/91 [==============================] - ETA: 0s - loss: 1.8583\n",
      "\n",
      "******* Epoch: 3 ********\n",
      "***** starting sentence *****\n",
      "si tolse e fe' restarmi,\n",
      "  <<e\n",
      "*****************************\n",
      "si tolse e fe' restarmi,\n",
      "  <<e si focchi si focchi si focchi si focchi si focchi si focchi si focchi si focchi si focchi si focchi si focchi si focchi si focchi si focchi si focchi si focchi si focchi si focchi si focchi si focchi si focchi si focchi si focchi si focchi si focchi si focchi si focchi si focchi si focchi si focchi si focchi si focchi si focchi si focchi si focchi si focchi si focchi si focchi si focchi si focchi\n",
      "91/91 [==============================] - 86s 944ms/step - loss: 1.8583\n",
      "Epoch 4/20\n",
      "91/91 [==============================] - ETA: 0s - loss: 1.7571\n",
      "\n",
      "******* Epoch: 4 ********\n",
      "***** starting sentence *****\n",
      "lo stato primaio non si rinsel\n",
      "*****************************\n",
      "lo stato primaio non si rinsella,\n",
      "  che sa tu la sua cosi` che si scolto se la scolto,\n",
      "  per la sua cosi` che si suo per la sua costa,\n",
      "  per la sua cosi` che si suo per la sua costa,\n",
      "  per la sua cosi` che si suo per la sua costa,\n",
      "  per la sua cosi` che si suo per la sua costa,\n",
      "  per la sua cosi` che si suo per la sua costa,\n",
      "  per la sua cosi` che si suo per la sua costa,\n",
      "  per la sua cosi` che si suo per la sua costa,\n",
      "  per l\n",
      "91/91 [==============================] - 75s 830ms/step - loss: 1.7571\n",
      "Epoch 5/20\n",
      "91/91 [==============================] - ETA: 0s - loss: 1.6879\n",
      "\n",
      "******* Epoch: 5 ********\n",
      "***** starting sentence *****\n",
      "ette;\n",
      "\n",
      "e simigliante poi a la \n",
      "*****************************\n",
      "ette;\n",
      "\n",
      "e simigliante poi a la sua che si marte\n",
      "  che la sua che si scoltra che si scolta\n",
      "  che la sua che si scoltra che si scolta\n",
      "  che la sua che si scoltra che si scolta\n",
      "  che la sua che si scoltra che si scolta\n",
      "  che la sua che si scoltra che si scolta\n",
      "  che la sua che si scoltra che si scolta\n",
      "  che la sua che si scoltra che si scolta\n",
      "  che la sua che si scoltra che si scolta\n",
      "  che la sua che si scoltra che si scolta\n",
      "  che\n",
      "91/91 [==============================] - 81s 888ms/step - loss: 1.6879\n",
      "Epoch 6/20\n",
      "91/91 [==============================] - ETA: 0s - loss: 1.6338\n",
      "\n",
      "******* Epoch: 6 ********\n",
      "***** starting sentence *****\n",
      "accia,\n",
      "  letizia presi a tutte\n",
      "*****************************\n",
      "accia,\n",
      "  letizia presi a tutte la mara,\n",
      "\n",
      "come la mara al suo la mara al salio\n",
      "  che si` che si` che si` che si` che si` che si` che si` che si` che si` che si` che si` che si` che si` che si` che si` che si` che si` che si` che si` che si` che si` che si` che si` che si` che si` che si` che si` che si` che si` che si` che si` che si` che si` che si` che si` che si` che si` che si` che si` che si` che si` che si` che si` che si\n",
      "91/91 [==============================] - 82s 906ms/step - loss: 1.6338\n",
      "Epoch 7/20\n",
      "91/91 [==============================] - ETA: 0s - loss: 1.5904\n",
      "\n",
      "******* Epoch: 7 ********\n",
      "***** starting sentence *****\n",
      "  rispuose, <<quanto piu` potr\n",
      "*****************************\n",
      "  rispuose, <<quanto piu` potrito,\n",
      "  che si forse in quella che si face\n",
      "  di color che si forse in quella scolta,\n",
      "  e disse: <<o a la sua vista in costa, che 'l suo pieno\n",
      "  che si forse in quella che si face\n",
      "  di color che si forse in quella scolta,\n",
      "  e disse: <<o a la sua vista in costa, che 'l suo pieno\n",
      "  che si forse in quella che si face\n",
      "  di color che si forse in quella scolta,\n",
      "  e disse: <<o a la sua vista in costa, che \n",
      "91/91 [==============================] - 80s 881ms/step - loss: 1.5904\n",
      "Epoch 8/20\n",
      "91/91 [==============================] - ETA: 0s - loss: 1.5546\n",
      "\n",
      "******* Epoch: 8 ********\n",
      "***** starting sentence *****\n",
      "anto ragion qui vede,\n",
      "  dir ti\n",
      "*****************************\n",
      "anto ragion qui vede,\n",
      "  dir ti si consente a la sua visa\n",
      "  di qual di qual che si disiri,\n",
      "  che si convien che si disiria,\n",
      "  che si convien che si disiria,\n",
      "  che si convien che si disiria,\n",
      "  che si convien che si disiria,\n",
      "  che si convien che si disiria,\n",
      "  che si convien che si disiria,\n",
      "  che si convien che si disiria,\n",
      "  che si convien che si disiria,\n",
      "  che si convien che si disiria,\n",
      "  che si convien che si disiria,\n",
      "  che si c\n",
      "91/91 [==============================] - 82s 902ms/step - loss: 1.5546\n",
      "Epoch 9/20\n",
      "91/91 [==============================] - ETA: 0s - loss: 1.5230\n",
      "\n",
      "******* Epoch: 9 ********\n",
      "***** starting sentence *****\n",
      " nuvole d'agosto,\n",
      "\n",
      "che color n\n",
      "*****************************\n",
      " nuvole d'agosto,\n",
      "\n",
      "che color non si far le con si misali,\n",
      "  che 'l santo si misi` di pie` di pieta\n",
      "  di sua vista in su la scappa segne,\n",
      "  che 'l santo si misi` di pie` di pieta\n",
      "  di sua vista in su la scappa segne,\n",
      "  che 'l santo si misi` di pie` di pieta\n",
      "  di sua vista in su la scappa segne,\n",
      "  che 'l santo si misi` di pie` di pieta\n",
      "  di sua vista in su la scappa segne,\n",
      "  che 'l santo si misi` di pie` di pieta\n",
      "  di sua vista \n",
      "91/91 [==============================] - 88s 976ms/step - loss: 1.5230\n",
      "Epoch 10/20\n",
      "91/91 [==============================] - ETA: 0s - loss: 1.4951\n",
      "\n",
      "******* Epoch: 10 ********\n",
      "***** starting sentence *****\n",
      "` forte a veder chi piu` si fa\n",
      "*****************************\n",
      "` forte a veder chi piu` si face\n",
      "  de la sua strazia de la sua strazia\n",
      "  de la sua seguanda di quella spena,\n",
      "  che si del di costui di parlar di parlar di parlar di parlar di parlar di parlar di parlar di parlar di parlar di parlar di parlar di parlar di parlar di parlar di parlar di parlar di parlar di parlar di parlar di parlar di parlar di parlar di parlar di parlar di parlar di parlar di parlar di parlar di parlar di parla\n",
      "91/91 [==============================] - 94s 1s/step - loss: 1.4951\n",
      "Epoch 11/20\n",
      "91/91 [==============================] - ETA: 0s - loss: 1.4702\n",
      "\n",
      "******* Epoch: 11 ********\n",
      "***** starting sentence *****\n",
      "ista al vento si movieno.\n",
      "\n",
      "la \n",
      "*****************************\n",
      "ista al vento si movieno.\n",
      "\n",
      "la tempo e con la sua strata a la strancia\n",
      "  che si con lo scoglio di col si mala,\n",
      "  che si con lo scoglio di col si mala,\n",
      "  che si con lo scoglio di col si mala,\n",
      "  che si con lo scoglio di col si mala,\n",
      "  che si con lo scoglio di col si mala,\n",
      "  che si con lo scoglio di col si mala,\n",
      "  che si con lo scoglio di col si mala,\n",
      "  che si con lo scoglio di col si mala,\n",
      "  che si con lo scoglio di col si mala,\n",
      "\n",
      "91/91 [==============================] - 93s 1s/step - loss: 1.4702\n",
      "Epoch 12/20\n",
      "91/91 [==============================] - ETA: 0s - loss: 1.4485\n",
      "\n",
      "******* Epoch: 12 ********\n",
      "***** starting sentence *****\n",
      "fu nomato sassol mascheroni;\n",
      " \n",
      "*****************************\n",
      "fu nomato sassol mascheroni;\n",
      "  e in che si discende in altro sero,\n",
      "  e che si discende in altro a la vista,\n",
      "  e che si discende in altro a la vista,\n",
      "  e che si discende in altro a la vista,\n",
      "  e che si discende in altro a la vista,\n",
      "  e che si discende in altro a la vista,\n",
      "  e che si discende in altro a la vista,\n",
      "  e che si discende in altro a la vista,\n",
      "  e che si discende in altro a la vista,\n",
      "  e che si discende in altro a la v\n",
      "91/91 [==============================] - 96s 1s/step - loss: 1.4485\n",
      "Epoch 13/20\n",
      "91/91 [==============================] - ETA: 0s - loss: 1.4283\n",
      "\n",
      "******* Epoch: 13 ********\n",
      "***** starting sentence *****\n",
      "glio si` caduto,\n",
      "  ch'e' si la\n",
      "*****************************\n",
      "glio si` caduto,\n",
      "  ch'e' si la schianti l'altro piedo\n",
      "  si risposto che si serra se' tra spirto\n",
      "  di quella spera se la mente amore,\n",
      "  che si rispuose al suo morto scorto\n",
      "  di quella spera se la mente amore,\n",
      "  che si rispuose al suo morto scorto\n",
      "  di quella spera se la mente amore,\n",
      "  che si rispuose al suo morto scorto\n",
      "  di quella spera se la mente amore,\n",
      "  che si rispuose al suo morto scorto\n",
      "  di quella spera se la mente amor\n",
      "91/91 [==============================] - 91s 1000ms/step - loss: 1.4283\n",
      "Epoch 14/20\n",
      "91/91 [==============================] - ETA: 0s - loss: 1.4086\n",
      "\n",
      "******* Epoch: 14 ********\n",
      "***** starting sentence *****\n",
      "dormentai;\n",
      "  ma qual vuol sia \n",
      "*****************************\n",
      "dormentai;\n",
      "  ma qual vuol sia fatto che si convenno\n",
      "  che si converse in su la sua solenta\n",
      "  come la sua sole in con la sua solenta\n",
      "  come la sua sole in con la sua solenta\n",
      "  come la sua sole in con la sua solenta\n",
      "  come la sua sole in con la sua solenta\n",
      "  come la sua sole in con la sua solenta\n",
      "  come la sua sole in con la sua solenta\n",
      "  come la sua sole in con la sua solenta\n",
      "  come la sua sole in con la sua solenta\n",
      "  come la s\n",
      "91/91 [==============================] - 91s 1s/step - loss: 1.4086\n",
      "Epoch 15/20\n",
      "91/91 [==============================] - ETA: 0s - loss: 1.3913\n",
      "\n",
      "******* Epoch: 15 ********\n",
      "***** starting sentence *****\n",
      "no ad uno\n",
      "  tra 'l quinto di` \n",
      "*****************************\n",
      "no ad uno\n",
      "  tra 'l quinto di` altro a la sua sole amore.\n",
      "\n",
      "e io a lui: <<se' la sua virtuta\n",
      "  di se' con la sua sanza che si segno,\n",
      "  che si convenne a l'altro colle\n",
      "  con la sua sempita` di se' con l'altro santo altro a la sua sole amore.\n",
      "\n",
      "e io a lui: <<se' la sua virtuta\n",
      "  di se' con la sua sanza che si segno,\n",
      "  che si convenne a l'altro colle\n",
      "  con la sua sempita` di se' con l'altro santo altro a la sua sole amore.\n",
      "\n",
      "e io a l\n",
      "91/91 [==============================] - 96s 1s/step - loss: 1.3913\n",
      "Epoch 16/20\n",
      "91/91 [==============================] - ETA: 0s - loss: 1.3746\n",
      "\n",
      "******* Epoch: 16 ********\n",
      "***** starting sentence *****\n",
      "ominanza e` color d'erba,\n",
      "  ch\n",
      "*****************************\n",
      "ominanza e` color d'erba,\n",
      "  che si che tu se' ch'io disse: <<perche' la farbia stalla,\n",
      "  che si che tu se' ch'io disse: <<perche' la farbia stalla,\n",
      "  che si che tu se' ch'io disse: <<perche' la farbia stalla,\n",
      "  che si che tu se' ch'io disse: <<perche' la farbia stalla,\n",
      "  che si che tu se' ch'io disse: <<perche' la farbia stalla,\n",
      "  che si che tu se' ch'io disse: <<perche' la farbia stalla,\n",
      "  che si che tu se' ch'io disse: <<per\n",
      "91/91 [==============================] - 97s 1s/step - loss: 1.3746\n",
      "Epoch 17/20\n",
      "91/91 [==============================] - ETA: 0s - loss: 1.3587\n",
      "\n",
      "******* Epoch: 17 ********\n",
      "***** starting sentence *****\n",
      "<a man destra per la riva\n",
      "  co\n",
      "*****************************\n",
      "<a man destra per la riva\n",
      "  con li altri a la cosa ne la riva\n",
      "  che la carce in cielo a la mia contenta\n",
      "  di quella che tutti in cielo a le gembe il disiro,\n",
      "  e come l'anima che si` che l'anchia e 'l mondo e di colui\n",
      "  che la cora che la cora che la scala\n",
      "  che la cita di colui che l'anchia e 'l mondo e di colui\n",
      "  che la cora che la cora che la scala\n",
      "  che la cita di colui che l'anchia e 'l mondo e di colui\n",
      "  che la cora che l\n",
      "91/91 [==============================] - 98s 1s/step - loss: 1.3587\n",
      "Epoch 18/20\n",
      "91/91 [==============================] - ETA: 0s - loss: 1.3442\n",
      "\n",
      "******* Epoch: 18 ********\n",
      "***** starting sentence *****\n",
      " s'accrebbe,\n",
      "  quando parlai, \n",
      "*****************************\n",
      " s'accrebbe,\n",
      "  quando parlai, e come dietro a la mente risposta>>.\n",
      "\n",
      "<<o trasse in quella che tutta il presso al posco,\n",
      "  e con la sua virtu` che tu conventa\n",
      "  di mortali a la mana contenta\n",
      "  di mortali a la mana contenta\n",
      "  di mortali a la mana contenta\n",
      "  di mortali a la mana contenta\n",
      "  di mortali a la mana contenta\n",
      "  di mortali a la mana contenta\n",
      "  di mortali a la mana contenta\n",
      "  di mortali a la mana contenta\n",
      "  di mortali a la\n",
      "91/91 [==============================] - 97s 1s/step - loss: 1.3442\n",
      "Epoch 19/20\n",
      "91/91 [==============================] - ETA: 0s - loss: 1.3304\n",
      "\n",
      "******* Epoch: 19 ********\n",
      "***** starting sentence *****\n",
      "sai riconobb'io.\n",
      "\n",
      "cosi` a piu`\n",
      "*****************************\n",
      "sai riconobb'io.\n",
      "\n",
      "cosi` a piu` la sua schiera e disse:\n",
      "  <<se tu non si mostra ch'a la strada,\n",
      "  che si convenne a la sua virtuta\n",
      "  che l'altra ch'io disse: <<o tu avea semenza,\n",
      "  che si convenne a la sua virtuta\n",
      "  che l'altra ch'io disse: <<o tu avea semenza,\n",
      "  che si convenne a la sua virtuta\n",
      "  che l'altra ch'io disse: <<o tu avea semenza,\n",
      "  che si convenne a la sua virtuta\n",
      "  che l'altra ch'io disse: <<o tu avea semenza,\n",
      "  c\n",
      "91/91 [==============================] - 94s 1s/step - loss: 1.3304\n",
      "Epoch 20/20\n",
      "91/91 [==============================] - ETA: 0s - loss: 1.3173\n",
      "\n",
      "******* Epoch: 20 ********\n",
      "***** starting sentence *****\n",
      "za\n",
      "  che fece niccolo` a le pu\n",
      "*****************************\n",
      "za\n",
      "  che fece niccolo` a le punte morte\n",
      "  che l'altra spessa di piu` di piedi>>.\n",
      "\n",
      "e io a lui: <<sentimilen di piena\n",
      "  di qua e 'l mondo a lui disprore\n",
      "  di qua di la` dove l'altro disposto,\n",
      "  e per la rima spera se la sua virtuta\n",
      "  di piu` di pien d'un colle si face;\n",
      "  e sol di mi dicendo, e quella fosse,\n",
      "  che si` che l'altro disse: <<o tu ancor si morta\n",
      "  con l'altro carchio con l'altro santo amore.\n",
      "\n",
      "lo duca mio disse: <<o t\n",
      "91/91 [==============================] - 94s 1s/step - loss: 1.3173\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x15921de90>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "wi5pdR2ELDbw"
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": []
  }
 ]
}
