{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "colab": {
   "provenance": []
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FH5ja_uiJbr6"
   },
   "source": [
    "# Predicting Default Payments with Fully-Connected NNs\n",
    "\n",
    "The dataset contains information on default payments, demographic factors, credit data, history of payment, and bill statements of credit card clients in Taiwan from April 2005 to September 2005."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0-FgzT_cJbsH"
   },
   "source": [
    "## Inspecting the data\n",
    "\n",
    "any comment about data dimensionality/distribution goes here"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8vSiz47HXYYM",
    "ExecuteTime": {
     "end_time": "2024-10-23T06:46:19.672282Z",
     "start_time": "2024-10-23T06:46:19.421451Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Suppress TensorFlow logging warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "\n",
    "def load_data(path, train=True):\n",
    "    \"\"\"\n",
    "    Load data from a CSV file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path: str\n",
    "        The path to the CSV file.\n",
    "\n",
    "    train: bool (default True)\n",
    "        Specifies whether the data are training data.\n",
    "        If True, random shuffling is applied.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X: numpy.ndarray\n",
    "        The features as a multi-dimensional array of floats.\n",
    "    labels/ids: numpy.ndarray\n",
    "        The target labels for training data or IDs for test data.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path, encoding=\"ISO-8859-2\")\n",
    "    \n",
    "    if train:\n",
    "        # Shuffle training data to prevent ordering bias\n",
    "        data = df.sample(frac=1, random_state=42).values\n",
    "        X, labels = data[:, 1:-1].astype(np.float32), data[:, -1]\n",
    "        return X, labels\n",
    "    else:\n",
    "        X, ids = df.iloc[:, 1:].values.astype(np.float32), df.iloc[:, 0].astype(str)\n",
    "        return X, ids\n",
    "\n",
    "def preprocess_data(X, scaler=None):\n",
    "    \"\"\"\n",
    "    Preprocess input data by standardizing features to have zero mean and unit variance.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: numpy.ndarray\n",
    "        The input features to preprocess.\n",
    "    \n",
    "    scaler: StandardScaler, optional\n",
    "        A pre-fitted scaler for transformation (used in testing).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X_scaled: numpy.ndarray\n",
    "        The standardized features.\n",
    "    scaler: StandardScaler\n",
    "        The fitted scaler.\n",
    "    \"\"\"\n",
    "    if not scaler:\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(X)\n",
    "    X_scaled = scaler.transform(X)\n",
    "    return X_scaled, scaler\n",
    "\n",
    "def preprocess_labels(labels, encoder=None, categorical=True):\n",
    "    \"\"\"\n",
    "    Encode labels as integers and optionally convert them to categorical one-hot encoding.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    labels: numpy.ndarray\n",
    "        The target labels to preprocess.\n",
    "    \n",
    "    encoder: LabelEncoder, optional\n",
    "        A pre-fitted encoder for consistent label encoding.\n",
    "    \n",
    "    categorical: bool (default True)\n",
    "        Whether to convert labels to one-hot encoding.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    y: numpy.ndarray\n",
    "        The encoded labels, either as integers or one-hot vectors.\n",
    "    encoder: LabelEncoder\n",
    "        The fitted label encoder.\n",
    "    \"\"\"\n",
    "    if not encoder:\n",
    "        encoder = LabelEncoder()\n",
    "        encoder.fit(labels)\n",
    "    y = encoder.transform(labels).astype(np.int32)\n",
    "    if categorical:\n",
    "        y = to_categorical(y)\n",
    "    return y, encoder\n",
    "\n",
    "# Load the dataset\n",
    "url_train = './train.csv'\n",
    "url_test = './test.csv'\n",
    "\n",
    "# Load training data and labels\n",
    "X_train, labels = load_data(url_train, train=True)\n",
    "\n",
    "# Dimensionality inspection\n",
    "print(\"Training set dimensions (rows, columns):\", X_train.shape)\n",
    "print(\"Number of features:\", X_train.shape[1])\n",
    "\n",
    "# Inspect the first few rows of the training set\n",
    "print(\"\\nFirst 5 samples of the training data:\")\n",
    "print(pd.DataFrame(X_train).head())\n",
    "\n",
    "print(\"\\nTraining set labels distribution:\")\n",
    "print(pd.Series(labels).value_counts())\n",
    "\n",
    "# Additional EDA: Summary statistics for training data\n",
    "df_train = pd.read_csv(url_train)\n",
    "print(\"\\nSummary statistics for training data:\")\n",
    "print(df_train.describe())\n",
    "\n",
    "# Inspecting test data dimensionality\n",
    "X_test, ids = load_data(url_test, train=False)\n",
    "print(\"\\nTest set dimensions (rows, columns):\", X_test.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set dimensions (rows, columns): (25500, 23)\n",
      "Number of features: 23\n",
      "\n",
      "First 5 samples of the training data:\n",
      "         0    1    2    3     4    5    6    7    8    9   ...       13  \\\n",
      "0   70000.0  2.0  3.0  2.0  26.0  0.0  0.0  0.0  0.0  0.0  ...   8948.0   \n",
      "1  320000.0  2.0  2.0  2.0  28.0 -1.0 -1.0 -1.0 -1.0 -1.0  ...    944.0   \n",
      "2   30000.0  2.0  2.0  2.0  36.0  0.0 -1.0 -1.0  0.0  0.0  ...  30452.0   \n",
      "3   20000.0  2.0  3.0  1.0  35.0  0.0  0.0  2.0  2.0  0.0  ...  18621.0   \n",
      "4   80000.0  1.0  2.0  2.0  32.0  1.0  2.0  0.0  0.0  0.0  ...  28242.0   \n",
      "\n",
      "        14       15       16      17       18      19      20      21      22  \n",
      "0   9006.0  10570.0  11421.0  2000.0   1200.0  1500.0  2000.0  1000.0  2000.0  \n",
      "1    473.0   1747.0   1193.0   390.0    944.0   473.0  5000.0  1200.0   980.0  \n",
      "2  29667.0  28596.0  29180.0   490.0  33299.0  1400.0   572.0   584.0   400.0  \n",
      "3  18024.0  18434.0  19826.0  3000.0   1000.0     0.0   700.0  1700.0     0.0  \n",
      "4  21400.0      0.0      0.0     7.0   1200.0  1408.0     0.0     0.0     0.0  \n",
      "\n",
      "[5 rows x 23 columns]\n",
      "\n",
      "Training set labels distribution:\n",
      "0    19815\n",
      "1     5685\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Summary statistics for training data:\n",
      "                ID       LIMIT_BAL           SEX     EDUCATION      MARRIAGE  \\\n",
      "count  25500.00000    25500.000000  25500.000000  25500.000000  25500.000000   \n",
      "mean   14956.95702   167569.007059      1.604118      1.852353      1.550392   \n",
      "std     8667.36982   130002.156470      0.489049      0.787991      0.522757   \n",
      "min        1.00000    10000.000000      1.000000      0.000000      0.000000   \n",
      "25%     7432.75000    50000.000000      1.000000      1.000000      1.000000   \n",
      "50%    14942.50000   140000.000000      2.000000      2.000000      2.000000   \n",
      "75%    22431.25000   240000.000000      2.000000      2.000000      2.000000   \n",
      "max    30000.00000  1000000.000000      2.000000      6.000000      3.000000   \n",
      "\n",
      "                AGE         PAY_0         PAY_2         PAY_3         PAY_4  \\\n",
      "count  25500.000000  25500.000000  25500.000000  25500.000000  25500.000000   \n",
      "mean      35.509294     -0.013098     -0.130784     -0.163294     -0.218235   \n",
      "std        9.200408      1.126314      1.199481      1.199697      1.169681   \n",
      "min       21.000000     -2.000000     -2.000000     -2.000000     -2.000000   \n",
      "25%       28.000000     -1.000000     -1.000000     -1.000000     -1.000000   \n",
      "50%       34.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "75%       41.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "max       79.000000      8.000000      8.000000      8.000000      8.000000   \n",
      "\n",
      "       ...      BILL_AMT4      BILL_AMT5      BILL_AMT6       PAY_AMT1  \\\n",
      "count  ...   25500.000000   25500.000000   25500.000000   25500.000000   \n",
      "mean   ...   43336.952196   40307.121059   38924.328157    5594.010863   \n",
      "std    ...   64433.082446   60870.691089   59659.509920   16235.253410   \n",
      "min    ... -170000.000000  -81334.000000 -339603.000000       0.000000   \n",
      "25%    ...    2338.750000    1767.250000    1266.750000    1000.000000   \n",
      "50%    ...   19111.000000   18112.500000   17150.000000    2100.000000   \n",
      "75%    ...   54475.000000   50178.250000   49132.500000    5006.000000   \n",
      "max    ...  891586.000000  927171.000000  961664.000000  873552.000000   \n",
      "\n",
      "           PAY_AMT2       PAY_AMT3       PAY_AMT4       PAY_AMT5  \\\n",
      "count  2.550000e+04   25500.000000   25500.000000   25500.000000   \n",
      "mean   5.934389e+03    5319.529647    4812.161373    4812.480431   \n",
      "std    2.381277e+04   18157.653215   15560.524538   15206.108094   \n",
      "min    0.000000e+00       0.000000       0.000000       0.000000   \n",
      "25%    8.270000e+02     396.000000     291.000000     251.000000   \n",
      "50%    2.002000e+03    1800.000000    1500.000000    1500.000000   \n",
      "75%    5.000000e+03    4560.500000    4000.000000    4071.500000   \n",
      "max    1.684259e+06  896040.000000  621000.000000  426529.000000   \n",
      "\n",
      "            PAY_AMT6  default payment next month  \n",
      "count   25500.000000                25500.000000  \n",
      "mean     5236.509176                    0.222941  \n",
      "std     17958.888070                    0.416227  \n",
      "min         0.000000                    0.000000  \n",
      "25%       125.750000                    0.000000  \n",
      "50%      1500.000000                    0.000000  \n",
      "75%      4000.000000                    0.000000  \n",
      "max    528666.000000                    1.000000  \n",
      "\n",
      "[8 rows x 25 columns]\n",
      "\n",
      "Test set dimensions (rows, columns): (4500, 23)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PjWrQr5vWTTG"
   },
   "source": [
    "## Preparing the data\n",
    "\n",
    "describe the choice made during the preprocessing operations, also taking into account the previous considerations during the data inspection."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "J84aUJVUJbsI",
    "ExecuteTime": {
     "end_time": "2024-10-23T06:47:52.361617Z",
     "start_time": "2024-10-23T06:47:51.411146Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Step 1: Load the data\n",
    "url_train = './train.csv'\n",
    "X_train_raw, labels_raw = load_data(url_train, train=True)\n",
    "\n",
    "# Step 2: Handle missing values (if any)\n",
    "df_train = pd.read_csv(url_train)\n",
    "\n",
    "# Inspect if there are any missing values\n",
    "print(\"Missing values in training data:\")\n",
    "print(df_train.isnull().sum())\n",
    "\n",
    "# Assuming no missing values from previous inspection; if found, we could impute or drop.\n",
    "# For now, we move forward without explicit handling.\n",
    "\n",
    "# Step 3: Split the data into training and validation sets\n",
    "# This step is crucial to evaluate the model performance on unseen validation data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_raw, labels_raw, test_size=0.2, random_state=42, stratify=labels_raw)\n",
    "\n",
    "# Step 4: Scale the features using StandardScaler\n",
    "# Standardizing both the training and validation features\n",
    "X_train_scaled, scaler = preprocess_data(X_train)\n",
    "X_val_scaled, _ = preprocess_data(X_val, scaler)\n",
    "\n",
    "# Step 5: Encode the labels\n",
    "# Convert the labels to a numerical format and then to categorical (one-hot encoding)\n",
    "y_train_encoded, encoder = preprocess_labels(y_train)\n",
    "y_val_encoded, _ = preprocess_labels(y_val, encoder)\n",
    "\n",
    "# Step 6: Preprocessing test data\n",
    "# We apply the same scaler and encoder used on training data to ensure consistency\n",
    "url_test = './test.csv'\n",
    "X_test_raw, ids_test = load_data(url_test, train=False)\n",
    "X_test_scaled, _ = preprocess_data(X_test_raw, scaler)\n",
    "\n",
    "# Final datasets ready for model training\n",
    "print(\"Training data (X_train_scaled):\", X_train_scaled.shape)\n",
    "print(\"Validation data (X_val_scaled):\", X_val_scaled.shape)\n",
    "print(\"Test data (X_test_scaled):\", X_test_scaled.shape)\n",
    "\n",
    "# You can also check the shapes of labels\n",
    "print(\"Encoded training labels (y_train_encoded):\", y_train_encoded.shape)\n",
    "print(\"Encoded validation labels (y_val_encoded):\", y_val_encoded.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in training data:\n",
      "ID                            0\n",
      "LIMIT_BAL                     0\n",
      "SEX                           0\n",
      "EDUCATION                     0\n",
      "MARRIAGE                      0\n",
      "AGE                           0\n",
      "PAY_0                         0\n",
      "PAY_2                         0\n",
      "PAY_3                         0\n",
      "PAY_4                         0\n",
      "PAY_5                         0\n",
      "PAY_6                         0\n",
      "BILL_AMT1                     0\n",
      "BILL_AMT2                     0\n",
      "BILL_AMT3                     0\n",
      "BILL_AMT4                     0\n",
      "BILL_AMT5                     0\n",
      "BILL_AMT6                     0\n",
      "PAY_AMT1                      0\n",
      "PAY_AMT2                      0\n",
      "PAY_AMT3                      0\n",
      "PAY_AMT4                      0\n",
      "PAY_AMT5                      0\n",
      "PAY_AMT6                      0\n",
      "default payment next month    0\n",
      "dtype: int64\n",
      "Training data (X_train_scaled): (20400, 23)\n",
      "Validation data (X_val_scaled): (5100, 23)\n",
      "Test data (X_test_scaled): (4500, 23)\n",
      "Encoded training labels (y_train_encoded): (20400, 2)\n",
      "Encoded validation labels (y_val_encoded): (5100, 2)\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eb9aljYxJbsK"
   },
   "source": [
    "## Building the network\n",
    "\n",
    "any description/comment about the procedure you followed in the choice of the network structure and hyperparameters goes here, together with consideration about the training/optimization procedure (e.g. optimizer choice, final activations, loss functions, training metrics)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "AS3zsRlBYHKW"
   },
   "source": [],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8w7UIdtIWuCD"
   },
   "source": [
    "## Analyze and comment the training results\n",
    "\n",
    "here goes any comment/visualization of the training history and any initial consideration on the training results  "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4brGmh1BJbsL"
   },
   "source": [],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2jiOZzvyJbsN"
   },
   "source": [
    "## Validate the model and comment the results\n",
    "\n",
    "please describe the evaluation procedure on a validation set, commenting the generalization capability of your model (e.g. under/overfitting). You may also describe the performance metrics that you choose: what is the most suitable performance measure (or set of performance measures) in this case/dataset, according to you? Why?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "sgGlAIaEJbsO"
   },
   "source": [],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5MhCwXroWmf9"
   },
   "source": [
    "## Make predictions (on the provided test set)\n",
    "\n",
    "Based on the results obtained and analyzed during the training and the validation phases, what are your (rather _personal_) expectations with respect to the performances of your model on the blind external test set? Briefly motivate your answer."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fbtA2vJRWpMY"
   },
   "source": [],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9w-sa4AlaBJg"
   },
   "source": [
    "# OPTIONAL -- Export the predictions in the format indicated in the assignment release page and verify you prediction on the [assessment page](https://aml-assignmentone-2425.streamlit.app/)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tTPSYsbVaAQ_"
   },
   "source": [],
   "execution_count": null,
   "outputs": []
  }
 ]
}
