{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "colab": {
   "provenance": []
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FH5ja_uiJbr6"
   },
   "source": [
    "# Predicting Default Payments with Fully-Connected NNs\n",
    "\n",
    "The dataset contains information on default payments, demographic factors, credit data, history of payment, and bill statements of credit card clients in Taiwan from April 2005 to September 2005."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0-FgzT_cJbsH"
   },
   "source": [
    "## Inspecting the data\n",
    "\n",
    "any comment about data dimensionality/distribution goes here"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8vSiz47HXYYM",
    "ExecuteTime": {
     "end_time": "2024-10-24T09:24:19.976840Z",
     "start_time": "2024-10-24T09:24:19.662627Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Suppress TensorFlow logging warnings to reduce console output clutter\n",
    "# This is useful for improving the readability of logs during model training\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "\n",
    "def load_data(path, train=True):\n",
    "    \"\"\"\n",
    "    Load and preprocess data from a specified CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    path (str): The file path to the CSV data.\n",
    "    train (bool): A flag indicating whether the data is for training (True) or testing (False).\n",
    "                  If True, the function will return features and labels; otherwise, it returns the features and IDs.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing:\n",
    "        - np.ndarray: A 2D array of features (X).\n",
    "        - np.ndarray: A 1D array of labels (y) if train is True, otherwise a 1D array of IDs.\n",
    "    \"\"\"\n",
    "\n",
    "    # Read CSV file into a DataFrame, using ISO-8859-2 encoding to handle special characters\n",
    "    df = pd.read_csv(path, encoding=\"ISO-8859-2\")\n",
    "    \n",
    "    # Clip the values in the EDUCATION and MARRIAGE columns to ensure they fall within specified ranges\n",
    "    # EDUCATION: [1, 4] and MARRIAGE: [1, 3] are considered valid categories\n",
    "    df['EDUCATION'] = df['EDUCATION'].clip(lower=1, upper=4)\n",
    "    df['MARRIAGE'] = df['MARRIAGE'].clip(lower=1, upper=3)\n",
    "    \n",
    "    # Clip the values for payment columns PAY_0 to PAY_6 to restrict them to the range [0, 9]\n",
    "    # This ensures that all payment values are valid and within expected limits\n",
    "    for col in ['PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']:\n",
    "        df[col] = df[col].clip(lower=0, upper=9)\n",
    "        \n",
    "    # TODO: DA VERIFICARE CORRETTEZZA!\n",
    "    df.drop([\"ID\", \"LIMIT_BAL\", \"SEX\", \"EDUCATION\", \"MARRIAGE\", \"AGE\",\n",
    "         \"PAY_0\", \"PAY_2\", \"PAY_3\", \"PAY_4\", \"PAY_5\", \"PAY_6\",\n",
    "         \"BILL_AMT1\", \"BILL_AMT2\", \"BILL_AMT3\", \"BILL_AMT4\",\n",
    "         \"BILL_AMT5\", \"BILL_AMT6\", \"PAY_AMT1\", \"PAY_AMT2\",\n",
    "         \"PAY_AMT3\"], axis=1, inplace=True)\n",
    "\n",
    "    \n",
    "    # For training data, shuffle the DataFrame and separate features and labels\n",
    "    # The random_state ensures reproducibility of results\n",
    "    if train:\n",
    "        data = df.sample(frac=1, random_state=42).values\n",
    "        return data[:, 1:-1].astype(np.float32), data[:, -1]\n",
    "    \n",
    "    # For testing data, return the features and IDs without shuffling\n",
    "    return df.iloc[:, 1:].values.astype(np.float32), df.iloc[:, 0].astype(str)\n",
    "\n",
    "# Load training and test datasets\n",
    "X_train, labels = load_data('./train.csv')\n",
    "X_test, ids = load_data('./test.csv', train=False)\n",
    "\n",
    "# Print the dimensions of the training set and the number of features\n",
    "# This provides an overview of the dataset's structure\n",
    "print(f\"Training set dimensions: {X_train.shape}\\nFeatures count: {X_train.shape[1]}\")\n",
    "print(\"\\nFirst 5 training samples:\\n\", pd.DataFrame(X_train).head())\n",
    "\n",
    "# Display the distribution of labels in the training set\n",
    "# This helps in understanding class balance which is critical for modeling\n",
    "print(\"\\nLabels distribution:\\n\", pd.Series(labels).value_counts())\n",
    "\n",
    "# Check for missing values in the training set\n",
    "# Identifying missing values is essential for ensuring data quality before model training\n",
    "print(\"\\nMissing values:\\n\", pd.read_csv('./train.csv').isnull().sum())\n",
    "\n",
    "# Output summary statistics for the training dataset\n",
    "# This provides insights into data characteristics such as mean, min, max, etc.\n",
    "print(\"\\nSummary statistics:\\n\", pd.read_csv('./train.csv').describe())\n",
    "\n",
    "# Print dimensions of the test set to confirm data loading\n",
    "print(\"\\nTest set dimensions:\", X_test.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set dimensions: (25500, 3)\n",
      "Features count: 3\n",
      "\n",
      "First 5 training samples:\n",
      "         0       1       2\n",
      "0  2000.0  1000.0  2000.0\n",
      "1  5000.0  1200.0   980.0\n",
      "2   572.0   584.0   400.0\n",
      "3   700.0  1700.0     0.0\n",
      "4     0.0     0.0     0.0\n",
      "\n",
      "Labels distribution:\n",
      " 0    19815\n",
      "1     5685\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Missing values:\n",
      " ID                            0\n",
      "LIMIT_BAL                     0\n",
      "SEX                           0\n",
      "EDUCATION                     0\n",
      "MARRIAGE                      0\n",
      "AGE                           0\n",
      "PAY_0                         0\n",
      "PAY_2                         0\n",
      "PAY_3                         0\n",
      "PAY_4                         0\n",
      "PAY_5                         0\n",
      "PAY_6                         0\n",
      "BILL_AMT1                     0\n",
      "BILL_AMT2                     0\n",
      "BILL_AMT3                     0\n",
      "BILL_AMT4                     0\n",
      "BILL_AMT5                     0\n",
      "BILL_AMT6                     0\n",
      "PAY_AMT1                      0\n",
      "PAY_AMT2                      0\n",
      "PAY_AMT3                      0\n",
      "PAY_AMT4                      0\n",
      "PAY_AMT5                      0\n",
      "PAY_AMT6                      0\n",
      "default payment next month    0\n",
      "dtype: int64\n",
      "\n",
      "Summary statistics:\n",
      "                 ID       LIMIT_BAL           SEX     EDUCATION      MARRIAGE  \\\n",
      "count  25500.00000    25500.000000  25500.000000  25500.000000  25500.000000   \n",
      "mean   14956.95702   167569.007059      1.604118      1.852353      1.550392   \n",
      "std     8667.36982   130002.156470      0.489049      0.787991      0.522757   \n",
      "min        1.00000    10000.000000      1.000000      0.000000      0.000000   \n",
      "25%     7432.75000    50000.000000      1.000000      1.000000      1.000000   \n",
      "50%    14942.50000   140000.000000      2.000000      2.000000      2.000000   \n",
      "75%    22431.25000   240000.000000      2.000000      2.000000      2.000000   \n",
      "max    30000.00000  1000000.000000      2.000000      6.000000      3.000000   \n",
      "\n",
      "                AGE         PAY_0         PAY_2         PAY_3         PAY_4  \\\n",
      "count  25500.000000  25500.000000  25500.000000  25500.000000  25500.000000   \n",
      "mean      35.509294     -0.013098     -0.130784     -0.163294     -0.218235   \n",
      "std        9.200408      1.126314      1.199481      1.199697      1.169681   \n",
      "min       21.000000     -2.000000     -2.000000     -2.000000     -2.000000   \n",
      "25%       28.000000     -1.000000     -1.000000     -1.000000     -1.000000   \n",
      "50%       34.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "75%       41.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "max       79.000000      8.000000      8.000000      8.000000      8.000000   \n",
      "\n",
      "       ...      BILL_AMT4      BILL_AMT5      BILL_AMT6       PAY_AMT1  \\\n",
      "count  ...   25500.000000   25500.000000   25500.000000   25500.000000   \n",
      "mean   ...   43336.952196   40307.121059   38924.328157    5594.010863   \n",
      "std    ...   64433.082446   60870.691089   59659.509920   16235.253410   \n",
      "min    ... -170000.000000  -81334.000000 -339603.000000       0.000000   \n",
      "25%    ...    2338.750000    1767.250000    1266.750000    1000.000000   \n",
      "50%    ...   19111.000000   18112.500000   17150.000000    2100.000000   \n",
      "75%    ...   54475.000000   50178.250000   49132.500000    5006.000000   \n",
      "max    ...  891586.000000  927171.000000  961664.000000  873552.000000   \n",
      "\n",
      "           PAY_AMT2       PAY_AMT3       PAY_AMT4       PAY_AMT5  \\\n",
      "count  2.550000e+04   25500.000000   25500.000000   25500.000000   \n",
      "mean   5.934389e+03    5319.529647    4812.161373    4812.480431   \n",
      "std    2.381277e+04   18157.653215   15560.524538   15206.108094   \n",
      "min    0.000000e+00       0.000000       0.000000       0.000000   \n",
      "25%    8.270000e+02     396.000000     291.000000     251.000000   \n",
      "50%    2.002000e+03    1800.000000    1500.000000    1500.000000   \n",
      "75%    5.000000e+03    4560.500000    4000.000000    4071.500000   \n",
      "max    1.684259e+06  896040.000000  621000.000000  426529.000000   \n",
      "\n",
      "            PAY_AMT6  default payment next month  \n",
      "count   25500.000000                25500.000000  \n",
      "mean     5236.509176                    0.222941  \n",
      "std     17958.888070                    0.416227  \n",
      "min         0.000000                    0.000000  \n",
      "25%       125.750000                    0.000000  \n",
      "50%      1500.000000                    0.000000  \n",
      "75%      4000.000000                    0.000000  \n",
      "max    528666.000000                    1.000000  \n",
      "\n",
      "[8 rows x 25 columns]\n",
      "\n",
      "Test set dimensions: (4500, 3)\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PjWrQr5vWTTG"
   },
   "source": [
    "## Preparing the data\n",
    "\n",
    "describe the choice made during the preprocessing operations, also taking into account the previous considerations during the data inspection."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "J84aUJVUJbsI",
    "ExecuteTime": {
     "end_time": "2024-10-24T09:24:21.552893Z",
     "start_time": "2024-10-24T09:24:21.158675Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def preprocess_data(X, scaler=None):\n",
    "    if scaler is None:\n",
    "        scaler = StandardScaler().fit(X)\n",
    "    return scaler.transform(X), scaler\n",
    "\n",
    "def preprocess_labels(y, encoder=None):\n",
    "    if encoder is None:\n",
    "        encoder = LabelEncoder().fit(y)\n",
    "    return to_categorical(encoder.transform(y)), encoder\n",
    "\n",
    "# Load data\n",
    "df_train = pd.read_csv('./train.csv')\n",
    "\n",
    "# Separare le features e le etichette\n",
    "labels_raw = df_train['default payment next month']  # Sostituisci con il nome corretto della colonna target\n",
    "\n",
    "columns_to_drop = [\"ID\"]\n",
    "\n",
    "# columns_to_drop = [\"ID\", \"SEX\", \"EDUCATION\", \"MARRIAGE\", \"AGE\", \"PAY_0\", \"PAY_2\", \"PAY_3\", \"PAY_4\", \"PAY_5\", \"PAY_6\"]\n",
    "\n",
    "# Creare il DataFrame delle features\n",
    "X_train_raw = df_train.drop(columns=columns_to_drop + ['default payment next month'])  # Escludere la colonna target\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing values:\\n\", df_train.isnull().sum())\n",
    "\n",
    "# Plot variance\n",
    "plt.figure(figsize=(10, 6))\n",
    "df_train.var().plot(kind='bar')\n",
    "plt.title('Variance of Each Feature')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Variance')\n",
    "plt.xticks(rotation=90)\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_raw, labels_raw, \n",
    "                                                  test_size=0.3, random_state=42, stratify=labels_raw)\n",
    "\n",
    "# Scale features and encode labels\n",
    "X_train_scaled, scaler = preprocess_data(X_train)\n",
    "X_val_scaled, _ = preprocess_data(X_val, scaler)\n",
    "y_train_encoded, encoder = preprocess_labels(y_train)\n",
    "y_val_encoded, _ = preprocess_labels(y_val, encoder)\n",
    "\n",
    "# Preprocess test data\n",
    "X_test_raw = pd.read_csv('./test.csv').drop(columns=columns_to_drop)\n",
    "X_test_scaled, _ = preprocess_data(X_test_raw, scaler)\n",
    "\n",
    "# Print shapes of datasets\n",
    "print(f\"Shapes -> Train: {X_train_scaled.shape}, Val: {X_val_scaled.shape}, Test: {X_test_scaled.shape}, \"\n",
    "      f\"Encoded Train: {y_train_encoded.shape}, Encoded Val: {y_val_encoded.shape}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values:\n",
      " ID                            0\n",
      "LIMIT_BAL                     0\n",
      "SEX                           0\n",
      "EDUCATION                     0\n",
      "MARRIAGE                      0\n",
      "AGE                           0\n",
      "PAY_0                         0\n",
      "PAY_2                         0\n",
      "PAY_3                         0\n",
      "PAY_4                         0\n",
      "PAY_5                         0\n",
      "PAY_6                         0\n",
      "BILL_AMT1                     0\n",
      "BILL_AMT2                     0\n",
      "BILL_AMT3                     0\n",
      "BILL_AMT4                     0\n",
      "BILL_AMT5                     0\n",
      "BILL_AMT6                     0\n",
      "PAY_AMT1                      0\n",
      "PAY_AMT2                      0\n",
      "PAY_AMT3                      0\n",
      "PAY_AMT4                      0\n",
      "PAY_AMT5                      0\n",
      "PAY_AMT6                      0\n",
      "default payment next month    0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA90AAAJOCAYAAACqS2TfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAABzFklEQVR4nO3deZyNdeP/8fc5M2MWs9jHYDCESMmSUJJkGUspbkpZR3dSCTdF7rIlpciewlBI6Q6lVHyzU91ZRguFsmaGkBlLxiyf3x9+zt00ixlnrrmu0ev5eJxHXfv7nHHmzPtcm8sYYwQAAAAAAPKd2+4AAAAAAABcqyjdAAAAAABYhNINAAAAAIBFKN0AAAAAAFiE0g0AAAAAgEUo3QAAAAAAWITSDQAAAACARSjdAAAAAABYhNINAAAAAIBFKN0AANvcd999CgwM1OnTp7Od56GHHpKfn5+OHTvm9fYOHDggl8ul+fPne72uwuLUqVN64IEHVKZMGblcLnXs2DHbee+88065XK4sH5UrV7Yso8vl0hNPPHFVy17+mWb1aNCgQT4nveT8+fMaNWqU1q1bZ8n6AQDXFl+7AwAA/r5iYmK0fPlyvfPOO+rfv3+m6YmJiVq2bJnat2+v8PBwr7cXERGhL7/8UlWrVvV6XYXF2LFjtWzZMsXGxqpq1aoqUaJEjvNXqVJFixYtyjTe39/fqoj54sknn1S3bt0yjAsODrZkW+fPn9fo0aMlXfqiAgCAnFC6AQC2iY6OVrly5RQbG5tl6V68eLH++OMPxcTEeLWdtLQ0paamyt/fX40aNfJqXYXN999/r6pVq+qhhx7K1fyBgYGF8jWqWLFiocz9Z8YYXbhwQYGBgXZHAQDkIw4vBwDYxsfHRz179tS2bdv03XffZZo+b948RUREKDo6Wr/99pv69++vWrVqKTg4WGXKlNFdd92ljRs3Zljm8uHGEyZM0AsvvKCoqCj5+/tr7dq1WR5evm/fPvXu3VvVqlVTUFCQypcvrw4dOmTKs27dOrlcLi1evFgjRoxQuXLlFBoaqrvvvls//fRTpuyfffaZWrRoobCwMAUFBalmzZoaP358hnm2bt2qe+65RyVKlFBAQIDq1q2rJUuW5Oq1O3XqlPr376/y5curSJEiqlKlikaMGKHk5OQMr8P//d//affu3Z5DrvPjkOjc/iwkKTk5WWPGjFHNmjUVEBCgkiVLqnnz5tqyZUumeRcsWKCaNWsqKChIderU0ccff+x11sty81rn5nkdOHBApUuXliSNHj3a87r26tVLktSrV68sD8UfNWqUXC5XhnGXD6ufNWuWatasKX9/f7311luSpL1796pbt24qU6aM/P39VbNmTc2YMSPfXg8AQMFhTzcAwFZ9+vTRSy+9pNjYWL322mue8bt27dJ///tfDRs2TD4+Pjp16pQkaeTIkSpbtqzOnj2rZcuW6c4779QXX3yR6TDfqVOnqnr16nr11VcVGhqqatWqZbn9o0ePqmTJknrppZdUunRpnTp1Sm+99ZZuvfVW7dixQzVq1Mgw/7PPPqvbbrtNc+bMUVJSkp555hl16NBBu3fvlo+PjyRp7ty5euSRR9SsWTPNmjVLZcqU0Z49e/T999971rN27Vq1adNGt956q2bNmqWwsDC9++676tq1q86fP+8pcVm5cOGCmjdvrp9//lmjR4/WTTfdpI0bN2r8+PGKi4vTJ5984jmUvn///kpMTPQcMl6rVq0r/kxSU1MzjXO73XK7L31Xn9ufRWpqqqKjo7Vx40YNHDhQd911l1JTU/XVV1/p0KFDatKkiWf9n3zyib755huNGTNGwcHBmjBhgu677z799NNPqlKlyhUzp6enZ8rt4+Mjl8uV69c6N88rIiJCn332mdq0aaOYmBj17dtXkjxFPK+WL1+ujRs36vnnn1fZsmVVpkwZ7dq1S02aNFHFihU1ceJElS1bVp9//rkGDBigEydOaOTIkVe1LQCATQwAADZr1qyZKVWqlLl48aJn3L/+9S8jyezZsyfLZVJTU01KSopp0aKFue+++zzj9+/fbySZqlWrZljfn6fNmzcv2yypqanm4sWLplq1ambQoEGe8WvXrjWSTNu2bTPMv2TJEiPJfPnll8YYY86cOWNCQ0PN7bffbtLT07PdzvXXX2/q1q1rUlJSMoxv3769iYiIMGlpadkuO2vWLCPJLFmyJMP4l19+2Ugyq1at8oxr1qyZueGGG7Jd1581a9bMSMryERMTk+1y2f0s3n77bSPJzJ49O8ftSjLh4eEmKSnJMy4hIcG43W4zfvz4HJe9/DPN6rF69WpjzNW/1tk9r99++81IMiNHjsy0TM+ePU2lSpUyjR85cqT5659dkkxYWJg5depUhvGtW7c2FSpUMImJiRnGP/HEEyYgICDT/AAAZ/tbH16+YcMGdejQQeXKlZPL5dLy5cvztPyFCxfUq1cv3XjjjfL19c32irDr169X/fr1FRAQoCpVqmjWrFnehweAa0hMTIxOnDihjz76SNKlPaQLFy5U06ZNM+yhnjVrlurVq6eAgAD5+vrKz89PX3zxhXbv3p1pnffcc4/8/PyuuO3U1FS9+OKLqlWrlooUKSJfX18VKVJEe/fuzXa9f3bTTTdJkg4ePChJ2rJli5KSktS/f/9MhxNftm/fPv3444+e86xTU1M9j7Zt2yo+Pj7LQ9YvW7NmjYoWLarOnTtnGH95j+0XX3xxxeednapVq+qbb77J9HjuuecyzJebn8Wnn36qgIAA9enT54rbbd68uUJCQjzD4eHhKlOmjOd1vZKnnnoqU+Zbb701z691Xv6N5Ye77rpLxYsX9wxfuHBBX3zxhe677z4FBQVlynvhwgV99dVXlmQBAFjjb126z507pzp16mj69OlXtXxaWpoCAwM1YMAA3X333VnOs3//frVt21ZNmzbVjh079Oyzz2rAgAH64IMPvIkOANeUzp07KywsTPPmzZMkrVy5UseOHctwAbVJkybpscce06233qoPPvhAX331lb755hu1adNGf/zxR6Z1RkRE5GrbgwcP1nPPPaeOHTtqxYoV+vrrr/XNN9+oTp06Wa63ZMmSGYYvX9X78ry//fabJKlChQrZbvPy7c+GDBkiPz+/DI/LF5Q7ceJEtsufPHlSZcuWzVTqy5QpI19fX508efJKTztbAQEBatCgQaZHpUqVPPPk9mfx22+/qVy5cp7D0nPy19dVuvTaZvUzyEqFChUyZQ4JCcnTa53Xf2P54a//Tk+ePKnU1FRNmzYtU962bdtmyAsAKBz+1ud0R0dHKzo6OtvpFy9e1L///W8tWrRIp0+fVu3atfXyyy97zlUrWrSoXn/9dUnS5s2bs7zP7KxZs1SxYkVNnjxZklSzZk1t3bpVr776qjp16pTfTwkACqXAwEA9+OCDmj17tuLj4xUbG6uQkBD94x//8MyzcOFC3XnnnZ7fu5edOXMmy3Vmt5f5rxYuXKgePXroxRdfzDD+xIkTKlasWN6eiP53bu+RI0eynadUqVKSpOHDh+v+++/Pcp6/nkv+ZyVLltTXX38tY0yG53n8+HGlpqZ61m+V3P4sSpcurU2bNik9PT1XxdsKeXmt8/pvLCsBAQGei9n9WXZF+a//TosXLy4fHx91795djz/+eJbLREVF5ToPAMB+f+s93VfSu3dvbd68We+++66+/fZb/eMf/1CbNm20d+/eXK/jyy+/VKtWrTKMa926tbZu3aqUlJT8jgwAhVZMTIzS0tL0yiuvaOXKlXrggQcUFBTkme5yuTLdK/rbb7/Vl19+6dV2s1rvJ598ol9//fWq1tekSROFhYVp1qxZMsZkOU+NGjVUrVo17dy5M8u9ypf30manRYsWOnv2bKbTot5++23PdCvl9mcRHR2tCxcuZLhafEHLy2ud2+f116Mb/qxy5co6fvy4Zw+7dOlL/M8//zxXeYOCgtS8eXPt2LFDN910U5Z5szoqAADgXH/rPd05+fnnn7V48WIdOXJE5cqVk3Tp0LTPPvtM8+bNy7RHJDsJCQkKDw/PMC48PFypqak6ceJErg9/BIBrXYMGDXTTTTdp8uTJMsZkujd3+/btNXbsWI0cOVLNmjXTTz/9pDFjxigqKirLq23nVvv27TV//nxdf/31uummm7Rt2za98sorOR4enpPg4GBNnDhRffv21d13361HHnlE4eHh2rdvn3bu3Ok5pemNN95QdHS0WrdurV69eql8+fI6deqUdu/ere3bt+v999/Pdhs9evTQjBkz1LNnTx04cEA33nijNm3apBdffFFt27bN9pSn3Pjjjz+yPWf48n2wc/uzePDBBzVv3jz169dPP/30k5o3b6709HR9/fXXqlmzph544IGrzpkXuX2tc/u8QkJCVKlSJX344Ydq0aKFSpQooVKlSqly5crq2rWrnn/+eT3wwAMaOnSoLly4oKlTpyotLS3XeadMmaLbb79dTZs21WOPPabKlSvrzJkz2rdvn1asWKE1a9bk+2sEALAOpTsb27dvlzFG1atXzzA+OTk5z98w//XQsct7PnJ76CMA/F3ExMToqaeeUq1atXTrrbdmmDZixAidP39ec+fO1YQJE1SrVi3NmjVLy5Yt8+re01OmTJGfn5/Gjx+vs2fPql69elq6dKn+/e9/e/U8ypUrp5dffll9+/aVMUaVK1dWz549PfM0b95c//3vfzVu3DgNHDhQv//+u0qWLKlatWqpS5cuOa4/ICBAa9eu1YgRI/TKK6/ot99+U/ny5TVkyBCvbyf1yy+/qHHjxllOS0lJka+vb65/Fr6+vlq5cqXGjx+vxYsXa/LkyQoJCVGdOnXUpk0br3LmRW5f67z8G5s7d66GDh2qe+65R8nJyerZs6fmz5+vqKgoffjhh3r22WfVuXNnRUREaPDgwfrtt980evToXOWtVauWtm/frrFjx+rf//63jh8/rmLFiqlatWqe87oBAIWHy2R37NvfjMvl0rJlyzxXIH/vvff00EMP6YcffvDcd/Wy4OBglS1bNsO4Xr166fTp05kO9bvjjjtUt25dTZkyxTNu2bJl6tKli86fP5+rK+sCAAAAAAon9nRno27dukpLS9Px48fVtGnTq15P48aNtWLFigzjVq1apQYNGlC4AQAAAOAa97cu3WfPntW+ffs8w/v371dcXJxKlCih6tWr66GHHlKPHj00ceJE1a1bVydOnNCaNWt04403eg7v2rVrly5evKhTp07pzJkziouLkyTdfPPNkqR+/fpp+vTpGjx4sB555BF9+eWXmjt3rhYvXlzQTxcAAAAAUMD+1oeXr1u3Ts2bN880/vJ5WSkpKXrhhRf09ttv69dff1XJkiXVuHFjjR49WjfeeKOkS1cpPXjwYKZ1/PllXb9+vQYNGqQffvhB5cqV0zPPPKN+/fpZ98QAAAAAAI7wty7dAAAAAABYift0AwAAAABgEUo3AAAAAAAW+dtdSC09PV1Hjx5VSEgI98kGAAAAAFwVY4zOnDmjcuXKye3Ofn/23650Hz16VJGRkXbHAAAAAABcAw4fPqwKFSpkO/1vV7pDQkIkXXphQkNDbU4DAAAAACiMkpKSFBkZ6emY2fnble7Lh5SHhoZSugEAAAAAXrnSactcSA0AAAAAAItQugEAAAAAsAilGwAAAAAAi1C6AQAAAACwCKUbAAAAAACLULoBAAAAALAIpRsAAAAAAItQugEAAAAAsAilGwAAAAAAi1C6AQAAAACwCKUbAAAAAACLULoBAAAAALAIpRsAAAAAAItQugEAAAAAsAilGwAAAAAAi1C6AQAAAACwCKUbAAAAAACLULoBAAAAALCIr90BnKzysE/ydX0HXmqXr+sDAAAAADgbe7oBAAAAALAIpRsAAAAAAItQugEAAAAAsAilGwAAAAAAi1C6AQAAAACwCKUbAAAAAACLULoBAAAAALAIpRsAAAAAAItQugEAAAAAsAilGwAAAAAAi1C6AQAAAACwCKUbAAAAAACLULoBAAAAALAIpRsAAAAAAItQugEAAAAAsAilGwAAAAAAi1C6AQAAAACwiK2le8OGDerQoYPKlSsnl8ul5cuXX3GZ5ORkjRgxQpUqVZK/v7+qVq2q2NhY68MCAAAAAJBHvnZu/Ny5c6pTp4569+6tTp065WqZLl266NixY5o7d66uu+46HT9+XKmpqRYnBQAAAAAg72wt3dHR0YqOjs71/J999pnWr1+vX375RSVKlJAkVa5c2aJ0AAAAAAB4x9bSnVcfffSRGjRooAkTJmjBggUqWrSo7rnnHo0dO1aBgYFZLpOcnKzk5GTPcFJSkiQpJSVFKSkpOW7P38fkX/j/v00AAAAAQOGX235XqEr3L7/8ok2bNikgIEDLli3TiRMn1L9/f506dSrb87rHjx+v0aNHZxq/atUqBQUF5bi9CQ3zJbbHypUr83eFAAAAAABbnD9/PlfzuYwx+bs79yq5XC4tW7ZMHTt2zHaeVq1aaePGjUpISFBYWJgkaenSpercubPOnTuX5d7urPZ0R0ZG6sSJEwoNDc0xU+1Rn1/dk8nG96Na5+v6AAAAAAD2SEpKUqlSpZSYmJhjtyxUe7ojIiJUvnx5T+GWpJo1a8oYoyNHjqhatWqZlvH395e/v3+m8X5+fvLz88txe8lpLu9D/2WbAAAAAIDCL7f9rlDdp/u2227T0aNHdfbsWc+4PXv2yO12q0KFCjYmAwAAAAAgM1tL99mzZxUXF6e4uDhJ0v79+xUXF6dDhw5JkoYPH64ePXp45u/WrZtKliyp3r17a9euXdqwYYOGDh2qPn36ZHshNQAAAAAA7GJr6d66davq1q2runXrSpIGDx6sunXr6vnnn5ckxcfHewq4JAUHB2v16tU6ffq0GjRooIceekgdOnTQ1KlTbckPAAAAAEBOHHMhtYKSlJSksLCwK57sLkmVh32Sr9s+8FK7fF0fAAAAAMAeue2WheqcbgAAAAAAChNKNwAAAAAAFqF0AwAAAABgEUo3AAAAAAAWoXQDAAAAAGARSjcAAAAAABahdAMAAAAAYBFKNwAAAAAAFqF0AwAAAABgEUo3AAAAAAAWoXQDAAAAAGARSjcAAAAAABahdAMAAAAAYBFKNwAAAAAAFqF0AwAAAABgEUo3AAAAAAAWoXQDAAAAAGARSjcAAAAAABahdAMAAAAAYBFKNwAAAAAAFqF0AwAAAABgEUo3AAAAAAAWoXQDAAAAAGARSjcAAAAAABahdAMAAAAAYBFKNwAAAAAAFqF0AwAAAABgEUo3AAAAAAAWoXQDAAAAAGARSjcAAAAAABahdAMAAAAAYBFKNwAAAAAAFqF0AwAAAABgEUo3AAAAAAAWoXQDAAAAAGARSjcAAAAAABahdAMAAAAAYBFKNwAAAAAAFrG1dG/YsEEdOnRQuXLl5HK5tHz58lwvu3nzZvn6+urmm2+2LB8AAAAAAN6wtXSfO3dOderU0fTp0/O0XGJionr06KEWLVpYlAwAAAAAAO/52rnx6OhoRUdH53m5Rx99VN26dZOPj0+e9o4DAAAAAFCQCt053fPmzdPPP/+skSNH2h0FAAAAAIAc2bqnO6/27t2rYcOGaePGjfL1zV305ORkJScne4aTkpIkSSkpKUpJSclxWX8fc/Vhs3Cl7QEAAAAACofc9rtCU7rT0tLUrVs3jR49WtWrV8/1cuPHj9fo0aMzjV+1apWCgoJyXHZCwzzHzNHKlSvzd4UAAAAAAFucP38+V/O5jDH5uzv3KrlcLi1btkwdO3bMcvrp06dVvHhx+fj4eMalp6fLGCMfHx+tWrVKd911V6blstrTHRkZqRMnTig0NDTHTLVHfX51TyYb349qna/rAwAAAADYIykpSaVKlVJiYmKO3bLQ7OkODQ3Vd999l2HczJkztWbNGv3nP/9RVFRUlsv5+/vL398/03g/Pz/5+fnluM3kNNfVB87ClbYHAAAAACgcctvvbC3dZ8+e1b59+zzD+/fvV1xcnEqUKKGKFStq+PDh+vXXX/X222/L7Xardu3aGZYvU6aMAgICMo0HAAAAAMAJbC3dW7duVfPmzT3DgwcPliT17NlT8+fPV3x8vA4dOmRXPAAAAAAAvOKYc7oLSlJSksLCwq543L0kVR72Sb5u+8BL7fJ1fQAAAAAAe+S2Wxa6+3QDAAAAAFBYULoBAAAAALAIpRsAAAAAAItQugEAAAAAsAilGwAAAAAAi1C6AQAAAACwCKUbAAAAAACLULoBAAAAALAIpRsAAAAAAItQugEAAAAAsAilGwAAAAAAi1C6AQAAAACwCKUbAAAAAACLULoBAAAAALAIpRsAAAAAAItQugEAAAAAsAilGwAAAAAAi1C6AQAAAACwCKUbAAAAAACLULoBAAAAALAIpRsAAAAAAItQugEAAAAAsAilGwAAAAAAi1C6AQAAAACwCKUbAAAAAACLULoBAAAAALAIpRsAAAAAAItQugEAAAAAsAilGwAAAAAAi1C6AQAAAACwCKUbAAAAAACLULoBAAAAALAIpRsAAAAAAItQugEAAAAAsAilGwAAAAAAi1C6AQAAAACwCKUbAAAAAACLULoBAAAAALAIpRsAAAAAAIvYWro3bNigDh06qFy5cnK5XFq+fHmO8y9dulQtW7ZU6dKlFRoaqsaNG+vzzz8vmLAAAAAAAOSRraX73LlzqlOnjqZPn56r+Tds2KCWLVtq5cqV2rZtm5o3b64OHTpox44dFicFAAAAACDvfO3ceHR0tKKjo3M9/+TJkzMMv/jii/rwww+1YsUK1a1bN5/TAQAAAADgHVtLt7fS09N15swZlShRItt5kpOTlZyc7BlOSkqSJKWkpCglJSXH9fv7mPwJ+v9daXsAAAAAgMIht/2uUJfuiRMn6ty5c+rSpUu284wfP16jR4/ONH7VqlUKCgrKcf0TGnodMYOVK1fm7woBAAAAALY4f/58ruZzGWPyd3fuVXK5XFq2bJk6duyYq/kXL16svn376sMPP9Tdd9+d7XxZ7emOjIzUiRMnFBoamuM2ao/K34u0fT+qdb6uDwAAAABgj6SkJJUqVUqJiYk5dstCuaf7vffeU0xMjN5///0cC7ck+fv7y9/fP9N4Pz8/+fn55bhscprLq5xZbRMAAAAAUPjltt8Vuvt0L168WL169dI777yjdu3a2R0HAAAAAIBs2bqn++zZs9q3b59neP/+/YqLi1OJEiVUsWJFDR8+XL/++qvefvttSZcKd48ePTRlyhQ1atRICQkJkqTAwECFhYXZ8hwAAAAAAMiOrXu6t27dqrp163pu9zV48GDVrVtXzz//vCQpPj5ehw4d8sz/xhtvKDU1VY8//rgiIiI8j6eeesqW/AAAAAAA5MTWPd133nmncrqO2/z58zMMr1u3ztpAAAAAAADko0J3TjcAAAAAAIUFpRsAAAAAAItQugEAAAAAsAilGwAAAAAAi1C6AQAAAACwCKUbAAAAAACLULoBAAAAALAIpRsAAAAAAItQugEAAAAAsAilGwAAAAAAi1C6AQAAAACwCKUbAAAAAACLULoBAAAAALAIpRsAAAAAAItQugEAAAAAsAilGwAAAAAAi1C6AQAAAACwCKUbAAAAAACLULoBAAAAALAIpRsAAAAAAItQugEAAAAAsAilGwAAAAAAi1C6AQAAAACwCKUbAAAAAACLULoBAAAAALAIpRsAAAAAAItQugEAAAAAsAilGwAAAAAAi1C6AQAAAACwCKUbAAAAAACLULoBAAAAALAIpRsAAAAAAItQugEAAAAAsAilGwAAAAAAi1C6AQAAAACwCKUbAAAAAACLULoBAAAAALAIpRsAAAAAAIvYWro3bNigDh06qFy5cnK5XFq+fPkVl1m/fr3q16+vgIAAValSRbNmzbI+KAAAAAAAV8HW0n3u3DnVqVNH06dPz9X8+/fvV9u2bdW0aVPt2LFDzz77rAYMGKAPPvjA4qQAAAAAAOSdr50bj46OVnR0dK7nnzVrlipWrKjJkydLkmrWrKmtW7fq1VdfVadOnSxKCQAAAADA1SlU53R/+eWXatWqVYZxrVu31tatW5WSkmJTKgAAAAAAsmbrnu68SkhIUHh4eIZx4eHhSk1N1YkTJxQREZFpmeTkZCUnJ3uGk5KSJEkpKSlXLOr+PiYfUv8PXwwAAAAAwLUht/3uqkp3amqq1q1bp59//lndunVTSEiIjh49qtDQUAUHB1/NKnPN5XJlGDbGZDn+svHjx2v06NGZxq9atUpBQUE5bmtCw6sMmY2VK1fm7woBAAAAALY4f/58rubLc+k+ePCg2rRpo0OHDik5OVktW7ZUSEiIJkyYoAsXLlh6NfGyZcsqISEhw7jjx4/L19dXJUuWzHKZ4cOHa/DgwZ7hpKQkRUZGqlWrVgoNDc1xe7VHfe596D/5flTrfF0fAAAAAMAel4+ivpI8l+6nnnpKDRo00M6dOzMU3fvuu099+/bN6+rypHHjxlqxYkWGcatWrVKDBg3k5+eX5TL+/v7y9/fPNN7Pzy/bZS5LTst67/nVutL2AAAAAACFQ277XZ5L96ZNm7R582YVKVIkw/hKlSrp119/zdO6zp49q3379nmG9+/fr7i4OJUoUUIVK1bU8OHD9euvv+rtt9+WJPXr10/Tp0/X4MGD9cgjj+jLL7/U3LlztXjx4rw+DQAAAAAALJfn0p2enq60tLRM448cOaKQkJA8rWvr1q1q3ry5Z/jyYeA9e/bU/PnzFR8fr0OHDnmmR0VFaeXKlRo0aJBmzJihcuXKaerUqdwuDAAAAADgSC5z+UpkudS1a1eFhYXpzTffVEhIiL799luVLl1a9957rypWrKh58+ZZlTVfJCUlKSwsTImJiVc8p7vysE/yddsHXmqXr+sDAAAAANgjt90yz3u6X3vtNTVv3ly1atXShQsX1K1bN+3du1elSpXiMG8AAAAAAP4kz6W7XLlyiouL07vvvqtt27YpPT1dMTExeuihhxQYGGhFRgAAAAAACqWruk93YGCgevfurd69e+d3HgAAAAAArhnuvC4wfvx4xcbGZhofGxurl19+OV9CAQAAAABwLchz6X7jjTd0/fXXZxp/ww03aNasWfkSCgAAAACAa0GeS3dCQoIiIiIyjS9durTi4+PzJRQAAAAAANeCPJfuyMhIbd68OdP4zZs3q1y5cvkSCgAAAACAa0GeL6TWt29fDRw4UCkpKbrrrrskSV988YWefvpp/etf/8r3gAAAAAAAFFZ5Lt1PP/20Tp06pf79++vixYuSpICAAD3zzDMaPnx4vgcEAAAAAKCwynPpdrlcevnll/Xcc89p9+7dCgwMVLVq1eTv729FPgAAAAAACq2ruk+3JAUHB+uWW27JzywAAAAAAFxT8ly6z507p5deeklffPGFjh8/rvT09AzTf/nll3wLBwAAAABAYXZVF1Jbv369unfvroiICLlcLityAQAAAABQ6OW5dH/66af65JNPdNttt1mRBwAAAACAa0ae79NdvHhxlShRwoosAAAAAABcU/JcuseOHavnn39e58+ftyIPAAAAAADXjDwfXj5x4kT9/PPPCg8PV+XKleXn55dh+vbt2/MtHAAAAAAAhVmeS3fHjh0tiAEAAAAAwLUnz6V75MiRVuQAAAAAAOCak+dzugEAAAAAQO7keU93WlqaXnvtNS1ZskSHDh3SxYsXM0w/depUvoUDAAAAAKAwy/Oe7tGjR2vSpEnq0qWLEhMTNXjwYN1///1yu90aNWqUBREBAAAAACic8ly6Fy1apNmzZ2vIkCHy9fXVgw8+qDlz5uj555/XV199ZUVGAAAAAAAKpTyX7oSEBN14442SpODgYCUmJkqS2rdvr08++SR/0wEAAAAAUIjluXRXqFBB8fHxkqTrrrtOq1atkiR988038vf3z990AAAAAAAUYnku3ffdd5+++OILSdJTTz2l5557TtWqVVOPHj3Up0+ffA8IAAAAAEBhleerl7/00kue/+/cubMqVKigLVu26LrrrtM999yTr+EAAAAAACjM8ly6/6pRo0Zq1KhRfmQBAAAAAOCakqvS/dFHHyk6Olp+fn766KOPcpyXvd0AAAAAAFySq9LdsWNHJSQkqEyZMurYsWO287lcLqWlpeVXNgAAAAAACrVcle709PQs/x8AAAAAAGQvT1cvT0lJUfPmzbVnzx6r8gAAAAAAcM3IU+n28/PT999/L5fLZVUeAAAAAACuGXm+T3ePHj00d+5cK7IAAAAAAHBNyfMtwy5evKg5c+Zo9erVatCggYoWLZph+qRJk/ItHAAAAAAAhVmeS/f333+vevXqSVKmc7s57BwAAAAAgP/Jc+leu3atFTkAAAAAALjm5PmcbgAAAAAAkDt53tMtSd98843ef/99HTp0SBcvXswwbenSpfkSDAAAAACAwi7Pe7rfffdd3Xbbbdq1a5eWLVumlJQU7dq1S2vWrFFYWFieA8ycOVNRUVEKCAhQ/fr1tXHjxhznX7RokerUqaOgoCBFRESod+/eOnnyZJ63CwAAAACA1fJcul988UW99tpr+vjjj1WkSBFNmTJFu3fvVpcuXVSxYsU8reu9997TwIEDNWLECO3YsUNNmzZVdHS0Dh06lOX8mzZtUo8ePRQTE6MffvhB77//vr755hv17ds3r08DAAAAAADL5bl0//zzz2rXrp0kyd/fX+fOnZPL5dKgQYP05ptv5mldkyZNUkxMjPr27auaNWtq8uTJioyM1Ouvv57l/F999ZUqV66sAQMGKCoqSrfffrseffRRbd26Na9PAwAAAAAAy+X5nO4SJUrozJkzkqTy5cvr+++/14033qjTp0/r/PnzuV7PxYsXtW3bNg0bNizD+FatWmnLli1ZLtOkSRONGDFCK1euVHR0tI4fP67//Oc/ni8BspKcnKzk5GTPcFJSkiQpJSVFKSkpOWb09zG5fTq5cqXtAQAAAAAKh9z2u1yX7ri4ON18881q2rSpVq9erRtvvFFdunTRU089pTVr1mj16tVq0aJFrgOeOHFCaWlpCg8PzzA+PDxcCQkJWS7TpEkTLVq0SF27dtWFCxeUmpqqe+65R9OmTct2O+PHj9fo0aMzjV+1apWCgoJyzDihYS6eSB6sXLkyf1cIAAAAALBFbnc657p016tXT3Xr1lXHjh314IMPSpKGDx8uPz8/bdq0Sffff7+ee+65PAd1uVwZho0xmcZdtmvXLg0YMEDPP/+8Wrdurfj4eA0dOlT9+vXT3Llzs1xm+PDhGjx4sGc4KSlJkZGRatWqlUJDQ3PMVnvU53l8Njn7flTrfF0fAAAAAMAel4+ivpJcl+7NmzcrNjZWr776qsaPH6/7779fMTExevrpp/X000/nOWCpUqXk4+OTaa/28ePHM+39vmz8+PG67bbbNHToUEnSTTfdpKJFi6pp06Z64YUXFBERkWkZf39/+fv7Zxrv5+cnPz+/HDMmp2Vd/q/WlbYHAAAAACgcctvvcn0htcaNG2v27NlKSEjQ66+/riNHjujuu+9W1apVNW7cOB05ciRPAYsUKaL69etr9erVGcavXr1aTZo0yXKZ8+fPy+3OGNnHx0fSpT3kAAAAAAA4SZ6vXh4YGKiePXtq3bp12rNnjx588EG98cYbioqKUtu2bfO0rsGDB2vOnDmKjY3V7t27NWjQIB06dEj9+vWTdOnQ8B49enjm79Chg5YuXarXX39dv/zyizZv3qwBAwaoYcOGKleuXF6fCgAAAAAAlsrz1cv/rGrVqho2bJgiIyP17LPP6vPP83YOdNeuXXXy5EmNGTNG8fHxql27tlauXKlKlSpJkuLj4zPcs7tXr146c+aMpk+frn/9618qVqyY7rrrLr388svePA0AAAAAACzhMld5XPb69esVGxurDz74QD4+PurSpYtiYmLUqFGj/M6Yr5KSkhQWFqbExMQrXkit8rBP8nXbB17K/tZmAAAAAIDCI7fdMk97ug8fPqz58+dr/vz52r9/v5o0aaJp06apS5cuKlq0qNehAQAAAAC4luS6dLds2VJr165V6dKl1aNHD/Xp00c1atSwMhsAAAAAAIVarkt3YGCgPvjgA7Vv395zxXAAAAAAAJC9XJfujz76yMocAAAAAABcc/J8yzAAAAAAAJA7lG4AAAAAACxC6QYAAAAAwCKUbgAAAAAALELpBgAAAADAIpRuAAAAAAAsQukGAAAAAMAilG4AAAAAACxC6QYAAAAAwCKUbgAAAAAALELpBgAAAADAIpRuAAAAAAAsQukGAAAAAMAilG4AAAAAACxC6QYAAAAAwCKUbgAAAAAALELpBgAAAADAIpRuAAAAAAAsQukGAAAAAMAilG4AAAAAACxC6QYAAAAAwCKUbgAAAAAALELpBgAAAADAIpRuAAAAAAAsQukGAAAAAMAilG4AAAAAACxC6QYAAAAAwCKUbgAAAAAALELpBgAAAADAIpRuAAAAAAAsQukGAAAAAMAilG4AAAAAACxC6QYAAAAAwCKUbgAAAAAALGJ76Z45c6aioqIUEBCg+vXra+PGjTnOn5ycrBEjRqhSpUry9/dX1apVFRsbW0BpAQAAAADIPV87N/7ee+9p4MCBmjlzpm677Ta98cYbio6O1q5du1SxYsUsl+nSpYuOHTumuXPn6rrrrtPx48eVmppawMkBAAAAALgylzHG2LXxW2+9VfXq1dPrr7/uGVezZk117NhR48ePzzT/Z599pgceeEC//PKLSpQocVXbTEpKUlhYmBITExUaGprjvJWHfXJV28jOgZfa5ev6AAAAAAD2yG23tO3w8osXL2rbtm1q1apVhvGtWrXSli1bslzmo48+UoMGDTRhwgSVL19e1atX15AhQ/THH38URGQAAAAAAPLEtsPLT5w4obS0NIWHh2cYHx4eroSEhCyX+eWXX7Rp0yYFBARo2bJlOnHihPr3769Tp05le153cnKykpOTPcNJSUmSpJSUFKWkpOSY0d8nfw8CuNL2AAAAAACFQ277na3ndEuSy+XKMGyMyTTusvT0dLlcLi1atEhhYWGSpEmTJqlz586aMWOGAgMDMy0zfvx4jR49OtP4VatWKSgoKMdsExrm9lnkzsqVK/N3hQAAAAAAW5w/fz5X89lWukuVKiUfH59Me7WPHz+eae/3ZRERESpfvryncEuXzgE3xujIkSOqVq1apmWGDx+uwYMHe4aTkpIUGRmpVq1aXfGc7tqjPs/LU7qi70e1ztf1AQAAAADscfko6iuxrXQXKVJE9evX1+rVq3Xfffd5xq9evVr33ntvlsvcdtttev/993X27FkFBwdLkvbs2SO3260KFSpkuYy/v7/8/f0zjffz85Ofn1+OGZPTst7jfrWutD0AAAAAQOGQ235n6326Bw8erDlz5ig2Nla7d+/WoEGDdOjQIfXr10/Spb3UPXr08MzfrVs3lSxZUr1799auXbu0YcMGDR06VH369Mny0HIAAAAAAOxk6zndXbt21cmTJzVmzBjFx8erdu3aWrlypSpVqiRJio+P16FDhzzzBwcHa/Xq1XryySfVoEEDlSxZUl26dNELL7xg11MAAAAAACBbtt6n2w7cpxsAAAAA4C3H36cbAAAAAIBrHaUbAAAAAACLULoBAAAAALAIpRsAAAAAAItQugEAAAAAsAilGwAAAAAAi1C6AQAAAACwiK/dAQAAQMGqPOyTfF3fgZfa5ev6AAC4lrCnGwAAAAAAi1C6AQAAAACwCKUbAAAAAACLULoBAAAAALAIpRsAAAAAAItQugEAAAAAsAilGwAAAAAAi1C6AQAAAACwCKUbAAAAAACL+NodAAAA4M8qD/skX9d34KV2+bo+AADygj3dAAAAAABYhNINAAAAAIBFKN0AAAAAAFiE0g0AAAAAgEW4kBoAAEAecKE3AEBesKcbAAAAAACLULoBAAAAALAIpRsAAAAAAItQugEAAAAAsAilGwAAAAAAi1C6AQAAAACwCKUbAAAAAACLULoBAAAAALAIpRsAAAAAAItQugEAAAAAsAilGwAAAAAAi1C6AQAAAACwCKUbAAAAAACLULoBAAAAALAIpRsAAAAAAItQugEAAAAAsIiv3QFmzpypV155RfHx8brhhhs0efJkNW3a9IrLbd68Wc2aNVPt2rUVFxdnfVAAAIBCoPKwT/J1fQdeapev6wOAvxtb93S/9957GjhwoEaMGKEdO3aoadOmio6O1qFDh3JcLjExUT169FCLFi0KKCkAAAAAAHlna+meNGmSYmJi1LdvX9WsWVOTJ09WZGSkXn/99RyXe/TRR9WtWzc1bty4gJICAAAAAJB3th1efvHiRW3btk3Dhg3LML5Vq1basmVLtsvNmzdPP//8sxYuXKgXXnjhittJTk5WcnKyZzgpKUmSlJKSopSUlByX9fcxV1x/XlxpewAAFASnf76RzztOzwcA14rc/n60rXSfOHFCaWlpCg8PzzA+PDxcCQkJWS6zd+9eDRs2TBs3bpSvb+6ijx8/XqNHj840ftWqVQoKCspx2QkNc7WJXFu5cmX+rhAAgKvg9M838nnH6fkA4Fpx/vz5XM1n+4XUXC5XhmFjTKZxkpSWlqZu3bpp9OjRql69eq7XP3z4cA0ePNgznJSUpMjISLVq1UqhoaE5Llt71Oe53k5ufD+qdb6uDwCAq+H0zzfyecfp+QDgWnH5KOorsa10lypVSj4+Ppn2ah8/fjzT3m9JOnPmjLZu3aodO3boiSeekCSlp6fLGCNfX1+tWrVKd911V6bl/P395e/vn2m8n5+f/Pz8csyYnJa5/HvjStsDAKAgOP3zjXzecXo+ALhW5Pb3o20XUitSpIjq16+v1atXZxi/evVqNWnSJNP8oaGh+u677xQXF+d59OvXTzVq1FBcXJxuvfXWgooOAAAAAECu2Hp4+eDBg9W9e3c1aNBAjRs31ptvvqlDhw6pX79+ki4dGv7rr7/q7bffltvtVu3atTMsX6ZMGQUEBGQaDwAAAACAE9haurt27aqTJ09qzJgxio+PV+3atbVy5UpVqlRJkhQfH3/Fe3YDAAAAAOBUtl9IrX///urfv3+W0+bPn5/jsqNGjdKoUaPyPxQAAAAAAPnA9tINAACAv4fKwz7J1/UdeKldvq4PAKxg24XUAAAAAAC41lG6AQAAAACwCKUbAAAAAACLcE43AAAAIM45B2AN9nQDAAAAAGARSjcAAAAAABahdAMAAAAAYBFKNwAAAAAAFqF0AwAAAABgEUo3AAAAAAAWoXQDAAAAAGARSjcAAAAAABahdAMAAAAAYBFKNwAAAAAAFqF0AwAAAABgEUo3AAAAAAAWoXQDAAAAAGARSjcAAAAAABahdAMAAAAAYBFKNwAAAAAAFqF0AwAAAABgEUo3AAAAAAAWoXQDAAAAAGARSjcAAAAAABahdAMAAAAAYBFKNwAAAAAAFqF0AwAAAABgEUo3AAAAAAAWoXQDAAAAAGARSjcAAAAAABahdAMAAAAAYBFKNwAAAAAAFqF0AwAAAABgEUo3AAAAAAAWoXQDAAAAAGARSjcAAAAAABahdAMAAAAAYBHbS/fMmTMVFRWlgIAA1a9fXxs3bsx23qVLl6ply5YqXbq0QkND1bhxY33++ecFmBYAAAAAgNyztXS/9957GjhwoEaMGKEdO3aoadOmio6O1qFDh7Kcf8OGDWrZsqVWrlypbdu2qXnz5urQoYN27NhRwMkBAAAAALgyW0v3pEmTFBMTo759+6pmzZqaPHmyIiMj9frrr2c5/+TJk/X000/rlltuUbVq1fTiiy+qWrVqWrFiRQEnBwAAAADgymwr3RcvXtS2bdvUqlWrDONbtWqlLVu25God6enpOnPmjEqUKGFFRAAAAAAAvOJr14ZPnDihtLQ0hYeHZxgfHh6uhISEXK1j4sSJOnfunLp06ZLtPMnJyUpOTvYMJyUlSZJSUlKUkpKS4/r9fUyucuTWlbYHAEBBcPrnG/m84+R8Ts4mOT8fAGfJ7XvcZYzJ398uuXT06FGVL19eW7ZsUePGjT3jx40bpwULFujHH3/McfnFixerb9+++vDDD3X33XdnO9+oUaM0evToTOPfeecdBQUFXf0TAAAAAAD8bZ0/f17dunVTYmKiQkNDs53Ptj3dpUqVko+PT6a92sePH8+09/uv3nvvPcXExOj999/PsXBL0vDhwzV48GDPcFJSkiIjI9WqVascXxhJqj0qf6+M/v2o1vm6PgAArobTP9/I5x0n53NyNsn5+QA4y+WjqK/EttJdpEgR1a9fX6tXr9Z9993nGb969Wrde++92S63ePFi9enTR4sXL1a7du2uuB1/f3/5+/tnGu/n5yc/P78cl01Oc11x/Xlxpe0BAFAQnP75Rj7vODmfk7NJzs8HwFly+x63rXRL0uDBg9W9e3c1aNBAjRs31ptvvqlDhw6pX79+ki7tpf7111/19ttvS7pUuHv06KEpU6aoUaNGnr3kgYGBCgsLs+15AAAAAACQFVtLd9euXXXy5EmNGTNG8fHxql27tlauXKlKlSpJkuLj4zPcs/uNN95QamqqHn/8cT3++OOe8T179tT8+fMLOj4AAAAAADmytXRLUv/+/dW/f/8sp/21SK9bt876QAAAAAAA5BPb7tMNAAAAAMC1jtINAAAAAIBFKN0AAAAAAFiE0g0AAAAAgEUo3QAAAAAAWITSDQAAAACARSjdAAAAAABYhNINAAAAAIBFKN0AAAAAAFiE0g0AAAAAgEUo3QAAAAAAWITSDQAAAACARSjdAAAAAABYhNINAAAAAIBFKN0AAAAAAFiE0g0AAAAAgEUo3QAAAAAAWITSDQAAAACARSjdAAAAAABYhNINAAAAAIBFKN0AAAAAAFiE0g0AAAAAgEUo3QAAAAAAWITSDQAAAACARSjdAAAAAABYhNINAAAAAIBFKN0AAAAAAFiE0g0AAAAAgEUo3QAAAAAAWMTX7gAAAAAACr/Kwz7J1/UdeKldvq4PsAt7ugEAAAAAsAilGwAAAAAAi1C6AQAAAACwCKUbAAAAAACLULoBAAAAALAIVy8HAAAAcM3j6urXNif/fCndAAAAQCHg5FIBIHuUbgAAAACwmdO/VHF6Piez/ZzumTNnKioqSgEBAapfv742btyY4/zr169X/fr1FRAQoCpVqmjWrFkFlBQAAAAAgLyxtXS/9957GjhwoEaMGKEdO3aoadOmio6O1qFDh7Kcf//+/Wrbtq2aNm2qHTt26Nlnn9WAAQP0wQcfFHByAAAAAACuzNbSPWnSJMXExKhv376qWbOmJk+erMjISL3++utZzj9r1ixVrFhRkydPVs2aNdW3b1/16dNHr776agEnBwAAAADgymwr3RcvXtS2bdvUqlWrDONbtWqlLVu2ZLnMl19+mWn+1q1ba+vWrUpJSbEsKwAAAAAAV8O2C6mdOHFCaWlpCg8PzzA+PDxcCQkJWS6TkJCQ5fypqak6ceKEIiIiMi2TnJys5ORkz3BiYqIk6dSpU1cs6r6p53L1XHLr5MmT+bo+AACuhtM/38jnHSfnc3I2iXzeIp93yOcdO/KdOXNGkmSMyXE+269e7nK5MgwbYzKNu9L8WY2/bPz48Ro9enSm8VFRUXmN6rVSEwt8kwAAWM7pn2/k846T8zk5m0Q+b5HPO+TzTl7ynTlzRmFhYdlOt610lypVSj4+Ppn2ah8/fjzT3uzLypYtm+X8vr6+KlmyZJbLDB8+XIMHD/YMp6en69SpUypZsmSO5T63kpKSFBkZqcOHDys0NNTr9eU38nmHfN4hn3fI5x3yeYd83iHf1XNyNol83iKfd8jnnfzOZ4zRmTNnVK5cuRzns610FylSRPXr19fq1at13333ecavXr1a9957b5bLNG7cWCtWrMgwbtWqVWrQoIH8/PyyXMbf31/+/v4ZxhUrVsy78FkIDQ115D+sy8jnHfJ5h3zeIZ93yOcd8nmHfFfPydkk8nmLfN4hn3fyM19Oe7gvs/Xq5YMHD9acOXMUGxur3bt3a9CgQTp06JD69esn6dJe6h49enjm79evnw4ePKjBgwdr9+7dio2N1dy5czVkyBC7ngIAAAAAANmy9Zzurl276uTJkxozZozi4+NVu3ZtrVy5UpUqVZIkxcfHZ7hnd1RUlFauXKlBgwZpxowZKleunKZOnapOnTrZ9RQAAAAAAMiW7RdS69+/v/r375/ltPnz52ca16xZM23fvt3iVLnn7++vkSNHZjqE3SnI5x3yeYd83iGfd8jnHfJ5h3xXz8nZJPJ5i3zeIZ937MrnMle6vjkAAAAAALgqtp7TDQAAAADAtYzSDQAAAACARSjdAAAAAABYhNINAAAAAIBFKN0AAAAAAFiE0p3PUlNTM9xbvCAdP378ivNs3LixAJIgv7Vt21aJiYme4XHjxun06dOe4ZMnT6pWrVo2JAMAAACQE24Zls927typevXqKS0trcC3XaZMGc2cOVOdO3fONO2PP/7QM888o1mzZunixYsFnu2yt99+O1fz9ejRw+IkWWvbtq0WL16ssLAwSZfK7eOPP65ixYpJulRumzZtql27dhVoLh8fH8XHx6tMmTKSpNDQUMXFxalKlSqSpGPHjqlcuXK2/LvLysGDB5WQkCCXy6Xw8HBVqlTJ7kiFljFGxhi53c78jnT+/Pm67777PO8Z5N7evXt16NAhVapUSdddd53dcfA3tG7dOt16660KDAy0O0qhk5ycrCNHjqhChQqOvR/xZampqTp69KgqVqxod5RC5dixY0pOTnbs6zZ69Gg9/vjjKlWqlN1RMvntt99UrFgx+fn52R0lg9TUVK1du9bz2du8eXP5+PgUzMYN8lVcXJxxu922bPuVV14xgYGB5oEHHjAnT570jN+wYYOpWrWqqV69utm0aZMt2S4rVqxYto/ixYubIkWK2Pb6GWOM2+02x44d8wyHhISYn3/+2TOckJBgSz6Xy5UhV3BwsCNy/dWkSZNMhQoVjNvtNi6Xy7hcLuN2u02FChXMa6+9Znc88/HHH5uYmBgzdOhQs3v37gzTTp06ZZo3b25TMmNSUlLMiBEjzB133GGef/55Y4wxEyZMMEFBQaZIkSKmR48eJjk52bZ82fHz8zO7du2yO4b5/PPPTUpKimd40aJFpk6dOiYoKMhUrVrVTJkyxcZ0xowfP9588cUXxphL/9ZatGiR4T3Spk0b8/vvv9uaMSe7du0yUVFRtmaIi4szY8eONTNmzDC//fZbhmmJiYmmd+/eNiW7ZPbs2aZHjx4mNjbWGGPMu+++a66//noTFRXleU87jVPevz/99JNJT0/3DG/cuNHce++9platWqZFixZm+fLlNqYzZt68eebLL780xhjzxx9/mJiYGOPj42Pcbrfx9fU1jz76qLlw4YKtGXNi59+ml82YMcO0aNHC/OMf//D8Lrzst99+s/X3S1JSknnooYdMxYoVPZ+1/fv39/x+vuOOO0xiYqJt+RITEzM9Tp8+bfz8/MzXX3/tGWeHN954w/NvPz093YwbN84UK1bMuN1uExQUZAYNGmTS0tJsyWaMMU8++aT5+OOPjTHGHD582Fx//fXGx8fHhIeHGx8fH3PjjTeaI0eOFEgWSnc+s/sX265du0yDBg1MRESEef/9982AAQOMr6+vGThwoDl//rxtua7k6NGj5tFHHzV+fn6mdevWtuVwarl1aq4/GzNmjAkNDTUvvfSS2bFjhzl69Kj59ddfzY4dO8xLL71kwsLCzNixY23Lt2jRIuPj42PatWtnbr/9dhMQEGAWLlzomW73a/jvf//bhIeHm8GDB5tatWqZfv36mcjISLNw4ULz9ttvmwoVKpiXX37ZtnzFixfP8uFyuUxYWJhn2C5//sLsP//5j/Hx8TFPPvmkWbRokfnXv/5l/P39zTvvvGNbvooVK5qdO3caY4zp27evqVu3rtm+fbv5448/TFxcnGnUqJGJiYmxLd+V2P3Z9vnnn5siRYqYG264wVSsWNGUKlXKrFmzxjPd7vfva6+9ZooWLWruv/9+ExERYV544QVTsmRJ88ILL5gxY8aYsLAw88Ybb9iWr27dulk+XC6XqVmzpmfYLn9+/65du9a43W7ToUMHM27cONOpUyfjdrvNZ599Zlu+6667znzzzTfGGGOGDBliKleubJYuXWp2795tli9fbqpXr26GDh1qW74rsfv9O2XKFBMUFGQef/xx8/DDDxt/f3/z4osveqbb/f594oknzPXXX2+mTp1q7rzzTnPvvfea2rVrm02bNpkNGzaY2rVrm2effda2fG63O8vH5S8FLv/XrmyX37uzZs0yRYsWNRMnTjSbN28206ZNM2FhYWbatGm2ZDPGmIiICM8Xi126dDF3332350vbkydPmvbt25vOnTsXSBZKdz6z+xebMcakpqaarl27GrfbbYKDg82GDRtszZOTpKQkM2LECBMcHGxuvfXWDH9E2cGp5dbtdpvjx49nyPXLL7/YnuvPKlSoYJYtW5bt9KVLl5py5coVXKC/qFu3rpk6dapn+P333zfBwcFmzpw5xhj7X8MqVaqYFStWGGOM2bt3r3G73ebdd9/1TF+yZImpXbu2XfFMcHCwadeunZk/f77nMW/ePOPj42PGjRvnGWeXP793b7vttkx7Fl955RVzyy232BHNGGOMv7+/OXDggDHGmMqVK5v169dnmL5161YTERFhRzRjjDGDBg3K8fHwww/b+v5o3Lix54/e9PR0M2HCBBMcHGw+/fRTY4z979/rr7/eLFq0yBhjzPbt242vr6/nd4sxxsTGxpr69evbFc/4+vqaNm3amFGjRnkeI0eONG632/Tv398zzi5/fv+2aNHC9O/fP8P0YcOGmTvuuMOOaMaYS+/fgwcPGmOMqV69uuff3WXr1683FStWtCOaMSb7L1UuP66//npb3x+1atXyvD+MMWbLli2mTJky5rnnnjPG2P/+jYyM9Pz9+euvvxqXy2U++ugjz/RPPvnE1KhRw654pnz58qZdu3ZmzZo1Zt26dWbdunVm7dq1xsfHx8ybN88zzg5/fu/ecsstZtKkSRmmz54929x00012RDPGGBMQEOD5e7lChQrm66+/zjD9u+++M6VKlSqQLL4FcxD7tePbb7/NcfpPP/1UQEmylpKSopEjR2rp0qXq2rWrPvvsM40aNUrz5s1z1DkpFy9e1PTp0/Xiiy+qVKlSmjdvXpbnohc0l8sll8uVaZzdjDHq1auX57yxCxcuqF+/fipatKikS+eW2e3kyZOqUaNGttOrV6+u33//vQATZbRnzx61b9/eM9y5c2eVKlVK99xzj1JSUnTffffZlk2Sjh49qjp16kiSrrvuOhUpUsQzLEkNGjTQwYMH7YqnHTt2qFu3blqzZo1mzJih4OBgSdIjjzyijh07OupCfnv37tXUqVMzjLvnnnv0wgsv2JRIqlSpkr7//ntVqlRJLpdLvr4ZP359fHx07tw5m9JJU6ZM0c0336zQ0NAsp589e7aAE2X0ww8/aMGCBZIu/U4eOnSoKlSooM6dO2vx4sVq2LChrfkOHjyo22+/XZJUt25d+fj4qFGjRp7pTZs21eDBg+2Kp3Xr1qlnz55q2LChRo4c6blGxOXrljjp/btr1y6NGzcuw7ju3btr9uzZNiWSypYtq59//lkVK1bUuXPnMp1DW7p0aZ08edKmdJdeswceeEBRUVFZTo+Pj9eePXsKONX/7N+/X02aNPEMN27cWGvWrFGLFi2UkpKigQMH2pZNunQh4svX1ShXrpwCAwMz/D1zww036PDhw3bF07fffquYmBiNHTtWCxYsUPny5SVd+l3YsGFD29+/l/9O3r9/v1q0aJFh2l133aVBgwbZEUvSpb89//vf/yoqKkohISFKSkrKMP3MmTNKT08vkCyU7jy6+eab5XK5ZLK4/tzl8XaVtLi4OHXv3l3nzp3T559/rubNm+vo0aPq27evbrzxRk2cOFF9+/a1Jdtlxhi9/fbbev7555WamqoXX3xRMTExBXcRgytwarnt2bNnhuGHH3440zx2XXzusoYNG2rcuHGaP39+pkJx+Wdt5x/GoaGhOnbsWIY/Su68806tWLFC7du315EjR2zLJklhYWE6ffq0IiMjJUn16tVTSEiIZ3pycrKtXwBdd9112rJli0aMGKGbb75Zb731lm677Tbb8mRl165dSkhIUGBgYKYP0fT0dFsvNPjII49o6NChqlGjhp544gkNGTJECxYsUNWqVbV//34NGjRIrVq1si1ftWrVNGjQoCx/t0iXPl/q169fwKn+x9/fP8MdGyTpwQcflNvt1gMPPKCJEyfaE+z/CwoKyvClSenSpT1fTF2Wmppa0LE8brvtNm3fvl2PPvqoGjdurHfeeUdVq1a1LU9Wzpw5o4CAAAUGBma6MFmRIkX0xx9/2JRMeuihhzRixAitXLlS3bt315gxY/TOO+8oODhY58+f16hRo2z9fVi7dm3deuuteuyxx7KcHhcXZ+uXFqVKldLhw4dVuXJlz7gbbrhBa9as0V133aVff/3VtmySVLJkSf3222+ez997773XcwFd6dKXjnZeLK9EiRJatmyZXn/9dTVs2FCvvvqqHnzwQdvy/NVnn32msLAwBQYGZnqf/vHHH7ZeCHbQoEEaMmSIwsPDNXz4cA0YMEDTpk1TzZo19dNPP+mpp57S/fffXyBZKN15tH//frsjZOvWW29Vz549NWnSJM+Hfbly5bRy5UrNmTNHQ4YM0dKlS7Vy5UrbMtapU0c///yznnzySQ0cODDTHyqXZbe3xWpOLbfz5s0r8G3m1bRp09SqVSuVKVNGzZo1U3h4uFwulxISErRhwwb5+/tr9erVtuVr2LChPv300wx7nySpWbNmnuJtp1q1amn79u268cYbJUmbN2/OMP27775TtWrV7Ijm4evrq5dfflmtW7dWt27d9NBDDzniSJDLWrRo4flCdPPmzWrQoIFn2o4dO2w92mfIkCE6dOiQatWqpapVq+rAgQOqXr26fH19lZqaqnr16mnx4sW25atfv762bduWbenO7svmgnLzzTdr7dq1mYp/165dlZ6enul3d0G7/vrr9e2336pmzZqSlGmv2I8//pihcNghNDRUixcv1rx583T77bdr9OjRjnr/Vq9eXdKlL7+3bdumm2++2TPthx9+8Ozds8PIkSP1/fffq0qVKmrQoIE2btyo8PBwlS9fXkePHlXJkiVt/Xy7/fbbczzSMiQkRHfccUcBJsro9ttv1wcffKCmTZtmGF+rVi198cUXat68uU3JLrnpppv0zTffqF69epKkd955J8P0b775xvPettNjjz2mZs2aqVu3blqxYoXdcTz+/Pv3iy++0K233uoZ/vLLL239gq9Xr146deqU2rVrJ2OM0tLSMnzBfc899+i1114rkCzcMswCcXFxGT4sCsqnn36q6OjobKcfOnRIMTExtn4w/Pnbrqw+7C8fKeCUW18hb86cOaOFCxfqq6++UkJCgqRLh+U1btxY3bp1s+3LFElav369tmzZouHDh2c5fd26dXrrrbds+4Jjz5498vPzy/bwwHfeeUe+vr7q0qVLASfL2smTJ/XII49o7dq1+uqrr3I8taAg/PXQ++DgYJUsWdIzfPl2hXYfEbJ79259/PHH+uWXX5Senq6IiAjddtttuvvuu20tQAkJCUpOTnbs7f2WLVumDRs2ZPvH0eLFi/Xmm29q7dq1BZzsks2bN6to0aLZfvbPnDlT6enpeuKJJwo2WDb27t2rhx56SFu3btX3339v++Gp69evzzAcERHhKeHSpdMfLl68qKFDhxZ0tAw+++wzrVixItP7t1u3bp4j4pDZt99+q23btql3795ZTv/hhx/0n//8RyNHjizgZJecOnVKbrc7w97tP/v0008VGBioO++8s0BzZefixYsaNmyY1q5dq6VLl2b7d4MTfPzxx/Lz81Pr1q1tzXH69GmtXr0603u3IHdmULrzSWJiohYtWqQ5c+Zo586dlMZs/PWDNTvNmjWzOEnhEh8fr+nTp3vOc7v99tt1/vx5z3QfHx8tX77c1j0BefXSSy+pX79+2X7I2Y183iGfdwo634YNG9SkSZNMp4Y4xYYNG9S4cWPH3fP1ssLw+v01X3p6us6cOaPQ0FDb93gXxtfPScaMGaMhQ4YoKCjI7ihZIp93nJzPydkkZ+WjdHtpzZo1io2N1dKlS1WpUiV16tRJnTp1Ut26dQs8y4QJE/Tkk08qMDBQ0qUPiVtvvdVzHsqZM2f0zDPPaObMmQWerbBwarl97rnndOrUKc2YMUPSpUPF+vTpoxIlSki69C3s7bffrldffbVAc3kjNDRUcXFxqlKlit1RskQ+75DPOwWdz8fHR/Hx8SpTpkyBbC+vyOcd8nmHfN4hn3ecnM/J2SRn5XPmV3YOd+TIEc2fP1+xsbE6d+6cunTpopSUFH3wwQe2HqI1fPhw9erVy1O627dvn+GPtvPnz+uNN95wROn+9ddf9cEHH2jPnj1yuVyqXr267r//ftv31M6cOTPDxXp27tyZqdy+9tprBV5uV6xYoVdeeSXDuKeeesrzs23UqJEGDx5cqEq307/vI593yOedgs7H6+Ed8nmHfN4hn3fId/WcnE1yVj5Kdx61bdtWmzZtUvv27TVt2jS1adNGPj4+mjVrlt3RMv3DctI/tD+bOXOmBg8erIsXLyosLEzGGCUlJWno0KGaNGmS+vfvb1s2p5bbAwcOZLgQRcuWLTOcP1ajRg1HX+QPgPPZfYjxlZDPO+TzDvm8Qz7vODmfk7NJzslH6c6jVatWacCAAXrsscdsv5JwYfTJJ59owIABGjhwoP71r38pIiJC0qXDul955RU99dRTqly5stq2bWtLPqeW29TUVCUmJnqGly5dmmH677//bustGQAUfs8999wVz3ubNGlSAaXJjHzeIZ93nJ6vRYsWVzznfPv27QWUJjPyecfJ+ZycTXJOPkp3Hm3cuFGxsbFq0KCBrr/+enXv3l1du3a1O1ahMWHCBA0bNkwvvPBChvERERGaNGmSgoKC9PLLL9tWup1abmvUqKEtW7Zke62AjRs3ZrjSKwDk1XfffaciRYpkO93uvQXk8w75vOP0fK1bt850b3gnIZ93nJzPydkk5+SjdOdR48aN1bhxY02ZMkXvvvuuYmNjNXjwYKWnp2v16tWKjIxUSEiIbfnmzJnj+YeVmpqq+fPnq1SpUpIuXUjNbjt27NCbb76Z7fTu3btrypQpBZgoI6eW2wceeEDPP/+8mjZtqptuuinDtJ07d2r06NEaNmxYgecCcO1YtmyZIy42kx3yeYd83nF6vqFDh5LPC+S7ek7OJjknH6X7KgUFBalPnz7q06ePfvrpJ82dO1cvvfSShg0bppYtW+qjjz4q8EwVK1bU7NmzPcNly5bVggULMs1jp/T09Bxv+eLn52fruehOLbcDBw7Uxx9/rPr166tly5aqUaOGXC6XfvzxR61evVqNGjXSwIEDCzzXnx05ckQVKlTI9fxNmzb1XPSvIJDPO+TzjtPz2b2X7krI5x3yeYd83iGfd5ycz8nZJIflM8g3qampZtmyZaZDhw52R3Gshg0bmkmTJmU7feLEiaZhw4YFmCijixcvmjvuuMP4+vqa6OhoM3DgQDNo0CATHR1tfH19TdOmTc3FixdtyZacnGzGjx9v6tSpYwIDA01gYKC56aabzPjx401ycrLZsWOHLbkuCwsLM2+//batGXJCPu+QzztOz+dyucyxY8fsjpEt8nmHfN4hn3fI5x0n53NyNmOclY/SfQ356quvzMqVKzOMe+utt0zlypVN6dKlzSOPPGIuXLhgU7pL5s+fbwIDA82MGTNMSkqKZ3xKSoqZPn26CQwMNPPmzbMvoMm53Nr9+v3V77//bmbMmGHq1atn3G63rVlmzJhhQkJCzP33329OnDhha5askM875POO0/PNnz/fcb/f/ox83iGfd5ye78CBAyY9Pd3uGNkin3ecnM/J2YxxVj6XMQ69r5SDrV27Vtu3b1ejRo1022236Y033tC4ceP0xx9/qGPHjpo6dWqBHhZ4WZs2bdS8eXM988wzki5d9KNevXrq1auXatasqVdeeUWPPvqoRo0aVeDZ/mzIkCGaNGmSQkJCPFcK//nnn3X27FkNGDBAr732mq35riQuLk4333yzrRnWrFmj2NhYLV26VJUqVVKnTp3UqVOnbM9FLyj79+9XTEyMdu3apTfffFP33HOPrXn+inzeIZ93nJxvw4YNuZrvjjvusDhJ1sjnHfJ5x+n5xowZk6v5nn/+eYuTZI183nFyPidnk5yVj9KdR7Nnz9Zjjz2mypUr68iRIxo5cqTGjRun7t27y+12a+HChXrsscf00ksvFXi2iIgIrVixQg0aNJAkjRgxQuvXr9emTZskSe+//75GjhypXbt2FXi2v/rqq6+0ePFi7d27V5JUvXp1PfDAA2rUqJHNybKWmJioRYsWac6cOdq5c6fS0tIKPMORI0c0f/58xcbG6ty5c+rSpYtmzZqlnTt3qlatWgWeJyfTp0/XoEGDVLNmzUy3abDzthGXkc875POOE/O53W7PuW/Z/Vngcrls+d0nkc9b5PNOYchXrlw5lSlTJsd8dv5+Id/Vc3I+J2eTnJWPC6nl0ZQpU/Taa6/pySef1GeffaYOHTpozpw56tmzpyTpzjvv1PDhw20p3b///rvCw8M9w+vXr1ebNm08w7fccosOHz5c4Lmy0qhRI8cW7D9bs2aN5s6dq2XLlnn2KM+dO7fAc7Rt21abNm1S+/btNW3aNLVp00Y+Pj6aNWtWgWe5koMHD+qDDz5QiRIldO+9917x3ogFjXzeIZ93nJqvePHiCgkJUa9evdS9e3fPXS+cgnzeIZ93nJ6vTZs2Wrt2rRo0aKA+ffqoXbt28vHxsTuWB/m84+R8Ts4mOSxfwR/RXrgFBgaaAwcOeIb9/PzMrl27PMMHDx40RYoUsSOaqVixolm/fr0x5tJ5yYGBgeb//u//PNO//fZbU7x4cVuyXbZz585cPex0+PBhM3bsWBMVFWXKlCljnnjiCePr62t++OEH2zL5+PiYQYMGmT179mQYb3euv3rzzTdNSEiIue+++8zx48ftjpMJ+bxDPu84OV9ycrJ59913TatWrUxgYKDp1KmTWblypWPOhSOfd8jnHafnM8aYo0ePmhdffNFUr17dlC1b1jz99NPmxx9/tDuWB/m84+R8Ts5mjHPyUbrz6K9XwQsODjY///yzZzghIcG2C1r985//NI0bNzYbNmwwgwcPNiVLljTJycme6QsXLjQNGjSwJdtlLpfLuN1u43K5sn3YeUGw6OhoExISYh588EHz8ccfm9TUVGOM/eV2y5Ytpm/fviY0NNQ0bNjQTJs2zRw/ftz2XH/WunVrU7x4cfPWW2/ZHSVL5PMO+bzj9Hx/dujQITN69GhTpUoVU758efPss89muPCl3cjnHfJ5x+n5jDFm/fr1plevXiYkJMQ0adLEnD9/3u5IGZDPO07O5+Rsxtibj9KdR2632+zbt88kJiaa06dPm5CQELNz506TmJhoEhMTzZ49e2wrjcePHze33367cblcJiQkxCxdujTD9Lvuuss8++yztmS77MCBA7l62MXpe5TPnTtn5s6da2677Tbj5+dn3G63mTx5sklKSrI7mrn77rvN4cOHs5x26tQpM3XqVFOnTp2CDfUn5PMO+bzj9HxZ+eWXX0zz5s2N2+02J0+etDtOJuTzDvm84+R858+fN2+99ZZp2LChCQwMNImJiXZHyoB83nFyPidnM8befJTuPLq8J/byI7thO50+fdqzh/bPTp48mWHPtx1Gjx5tzp07Z2uGnBSGPcqX/fjjj2bo0KGmbNmyJiAgwJH3h1+9erV54IEHTEBAgKlQoYIZMGCA3ZEyIJ93yOcdJ+a7cOGCWbRokWnRooUJCgoy//jHP8ynn35qdywP8nmHfN5xer4//w3ToEEDM2PGDPP777/bHcuDfN5xcj4nZzPGGfko3Xm0bt26XD2QNbfb7Zib1OfEyXuU/yo1NdUsW7bMMaX74MGDZtSoUaZSpUqmZMmSxu12m//85z92x/Ign3fI5x2n5vv6669Nv379TLFixUzdunXNlClTHLX3jnzeIZ93nJ7v5ZdfNtdff70pXbq0GThwoPn222/tjpQB+bzj5HxOzmaMs/JxyzCLvfTSS+rXr5+KFStmdxRHcLvdSkhIUJkyZeyOkms//fST5s6dqwULFuj06dNq2bKlPvroI7tjOc6SJUs0Z84cbd68WW3bttXDDz+s6OhoFS1a1BG3NSMf+ciXPbfbrYoVK6pnz56qX79+tvPZdW9x8nmHfN4pLPnat2+vIkWKZDvfpEmTCjDV/5DPO07O5+RskrPyUbotFhoaqri4OFWpUsXuKI7gdrt17NgxlS5d2u4oeZaWlqYVK1YoNjaW0p0FX19fPf300xo+fLhCQkI84/38/BxRKsjnHfJ5x+n53G73Feex+z7EV0K+7JHPO07Pd+edd3ruI56TtWvXFkCazMjnHSfnc3I2yVn5KN0WCwkJ0c6dOynd/5/b7Vbt2rWveG/agrhJPfLXP//5Ty1ZskQ33HCDunfvrq5du6p48eKOKRXkIx/5AACAHa781R2Qz1q3bq177703x4dd7r///is+OnXqZFs+J3vzzTcVHx+vf/7zn1q8eLEiIiJ07733yhij9PR0u+ORj3zk80JaWpqWL19ud4xskc875POO0/N99913GjhwoN0xskU+7zg5n5OzSQWbjz3dFmNPd0ZOP6e7d+/euZpv3rx5Ficp/Pbt26c5c+ZowYIFOnv2rNq1a6fOnTvr/vvvtzuaJPJ5i3zecXq+y3788UfFxsbqrbfe0u+//66LFy/aHSkD8nmHfN5xcr6kpCQtXrxYc+fO1datW3XTTTcpLi7O7lge5POOk/M5OZtkYz47rt72dxIcHGx+/vlnu2M4RmG5ejny7ty5c6Z///6mXLlypnTp0ubBBx80v/32m0lLSzMfffSRuffee02RIkXIRz7yOTDfn509e9bMnTvXNGnSxLjdbtOiRQsze/Zs89tvv9kdzRhDPm+RzztOz7du3TrTvXt3ExQUZNxut3nmmWfM3r177Y7lQT7vODmfk7MZY38+SrfFKN0ZuVyubEv3n//4dLL333/f7giONGTIEBMUFGQeeeQR8+STT5pSpUqZzp07Z5jHzi9cyOcd8nnH6fmMuXQf0z59+pjg4GBTt25d8+qrrxofHx/zww8/2JrrMvJ5h3zecXK+o0ePmnHjxpmqVauasmXLmkGDBplvvvnG+Pr6ko98f9tsTstH6c6jt956y1y4cCHX80dHR5ujR49amKhwOXDggElLS8swbs+ePWbYsGEmIiLCBAQE2F66U1JSzPfff29++umnDOOXL19ubrrpJsfsjXKaKlWqmMWLF3uGv/76a+Pr62tSU1NtTPU/5PMO+bzj9Hw1a9Y0lSpVMsOHD8/wh4hT/nAin3fI5x2n5/P39zcPP/yw+eyzzzL8jUW+3CHf1XNyNmOclY8LqeVR7969lZiYmOv5V65cqYiICAsTFS6VKlWS2+3WH3/8obfeekt33HGHbrjhBk2YMEHDhg3Tb7/9ZuvFSHbt2qXq1avrpptuUs2aNXX//ffr2LFjatasmXr27KmWLVtq3759tuVzssOHD6tp06ae4YYNG8rX11dHjx61MdX/kM875POO0/Pt27dPd9xxh5o3b66aNWvaHScT8nmHfN5xer5KlSpp06ZN2rBhg/bs2WN3nEzI5x0n53NyNslZ+SjdeWS47pxX/vvf/+qf//ynypYtq+nTp6tTp046fPiw3G637r77bgUHB9uab9iwYYqKitKHH36oLl26aPny5WratKlatGihw4cP69VXX1VkZKStGZ0qLS1NRYoUyTDO19dXqampNiXKiHzeIZ93nJ5v//79qlGjhh577DFVqFBBQ4YM0Y4dO3J1f9OCQD7vkM87Ts/3008/aeHChYqPj9ctt9yi+vXr67XXXpMkR2Qkn3ecnM/J2SRn5ePq5Xnkdrt17NgxlS5d2u4ohZKvr6+efPJJ9evXTzVq1PCMd8q9asuWLauVK1eqXr16On36tEqUKKE33nhDjzzyiK25CgO3263o6Gj5+/t7xq1YsUJ33XWXihYt6hm3dOlSO+KRz0vk847T8/3ZmjVrFBsbq6VLl+rChQsaMmSI+vbtq+rVq9sdTRL5vEU+7zg939mzZ7V48WLFxsbq66+/VrNmzdStWzd17NjREX+7ku/azefkbE7IR+nOo6z+cMqKE/5wcqJWrVrpq6++UocOHdS9e3e1bt1aLpfLMaXb7XYrPj5e4eHhkqTg4GBt377dMR+mTub0262Rzzvk847T82UlMTFRixYtUmxsrLZv367atWvr22+/tTuWB/m8Qz7vOD2fJO3evVtz587VggULdOrUKaWkpNgdKQPyecfJ+ZycTbIpX4GeQX4NcLlcpmvXrqZXr145PpC9Q4cOmdGjR5vKlSub8PBwM2DAAOPr62t27dpldzTjdrvN8ePHPcMhISHml19+sTERANhvx44d5sknn/QMb9q0KU8XFbUa+bxDPu84PV9KSor54IMPPMPjx483v//+u32B/oJ83nFyPidnM6Zg87GnO4/cbrcSEhJUpkwZu6NcE1avXq3Y2FgtX75ckZGR6ty5szp37qx69erZksftdissLMxznsfp06cVGhoqtzvj5Q9OnTplRzwAcITQ0FDFxcWpSpUqdkfJEvm8Qz7vkM875POOk/M5OZtkbT7ffF/jNc4JFwW4lrRs2VItW7bU77//roULFyo2NlYvv/yy0tLSbMnjpEM7AcCpnP59Pfm8Qz7vkM875POOk/M5OZtkbT5Kdx45/R9LYVW8eHE9+eSTevLJJ7V9+3bbcvTs2dO2bQMAAAC49nDLsDxau3atSpQoYXeMQmvv3r168MEHlZSUlGlaYmKiunXrpmLFihV8sFyKj4/XE088YXcMAAAAAIUEe7rzaOfOndq5c+cV5xswYEABpCl8XnnlFUVGRio0NDTTtLCwMEVGRuqVV17R66+/bkO6S3bt2qW1a9fKz89PXbp0UbFixXTixAmNGzdOs2bNUlRUlG3ZAAAAABQulO48unxD9Zy4XC5KdzY2bNigBQsWZDu9S5cu6tatWwEmyujjjz9Wp06dPLcOmDBhgmbPnq0uXbqodu3aev/999W+fXvb8gGAEzj9+ibk8w75vOP0fAAKHqU7j/bv3293hELt4MGDOV75vVSpUjp8+HABJspo3Lhx6tevn8aNG6c333xTQ4YMUb9+/fTBBx/ojjvusC0XADiJ069vQj7vkM87BZ3vyJEjqlChQq7nb9q0qQIDAy1MlBH5vOPkfE7OJjkrH7cMQ4EqW7as3nnnHd11111ZTv/iiy/00EMPKSEhoYCTXVKsWDH997//VfXq1ZWamqqAgACtWLFC0dHRtuQBACfYuXOn6tWrZ9udJa6EfN4hn3fszlesWDFNmzZN3bt3t2X7V0I+7zg5n5OzSc7Kx57uPHr77bdzNV+PHj0sTlI43XHHHZo2bVq2pXvq1Klq2rRpAaf6n6SkJM+F3Hx9fRUYGKjq1avblgcAnMLp39GTzzvk846d+V588UU9/vjjWr58ud58802VLFnStixZIZ93nJzPydkkZ+VjT3ceud1uBQcHy9fXN9tfsC6XS6dOnSrgZIXDjh071LhxY7Vv315PP/20atSoIUn68ccfNWHCBH3yySfasmWL6tWrZ0s+t9utNWvWeK5Q36RJEy1ZsiTToSk33XSTHfEAwBZ278m7EvJ5h3zecUK+/fv3KyYmRrt27dKbb76pe+65x7YsWSGfd5ycz8nZJOfko3Tn0Q033KBjx47p4YcfVp8+fShfV+Hjjz9Wnz59dPLkyQzjS5YsqTlz5tj6ZnW73XK5XFl+oXJ5vMvlcuwHPwBYwQmlIifk8w75vOOkfNOnT9egQYNUs2ZN+fpmPKB1+/btNqX6H/J5x8n5nJxNsj8fh5fn0Q8//KCvv/5asbGxuuOOO3TdddcpJiZGDz30UJa3wUJm7du318GDB/XZZ59p3759MsaoevXqatWqlYKCgmzNxoXyAPwdJSUl5Tj9zJkzBZQka+TzDvm84/R8lx08eFAffPCBSpQooXvvvTdTsbAb+bzj5HxOziY5JJ/BVTt//rx56623zJ133mmCgoJMt27dzIULF+yOBYvt2LHD7ggAkK9cLpdxu93ZPi5PJx/5yOe8fMYY8+abb5qQkBBz3333mePHj9uaJSvk846T8zk5mzHOyeesryEKmcDAQPXo0UOVK1fWyJEj9e6772r69Ony9/e3O5pjjRkzJsvxYWFhqlGjhlq1aiW3213Aqa4sMTFRixYt0pw5c7Rz505HHEIGAPll7dq1dkfIEfm8Qz7vOD1fmzZt9N///lfTp0935IV8yecdJ+dzcjbJWfko3Vfp119/1VtvvaV58+bp3Llzevjhh/X666+rePHidkdztGXLlmU5/vTp0/r11191ww036PPPP8/xXt4Fac2aNYqNjdXSpUtVqVIlderUSXPnzrU7FgDkq2bNmuU4/dy5c9q2bVsBpcmMfN4hn3ecni8tLU3ffvttlvcj/v3337Vw4ULNnTtXcXFxBR9O5POWk/M5OZvksHy27WMvpN577z3Tpk0bExgYaDp27Gg+/PBDk5qaanesa8LRo0fNnXfeaWJiYmzNcfjwYTN27FgTFRVlypQpY5544gnj6+trfvjhB1tzAYBd4uLibD98Nifk8w75vOPEfKtXrzYPPPCACQgIMBUqVDADBgywO1IG5POOk/M5OZsx9uVjT3cePfDAA6pYsaIGDRqk8PBwHThwQDNmzMg034ABA2xIV7hFRETohRdesPUG9m3bttWmTZvUvn17TZs2TW3atJGPj49mzZplWyYAAIArOXTokObNm6d58+bp7Nmz+v3337VkyRJ16tTJ7miSyOctJ+dzcjbJGfmcd/Ksw1WsWFEul0vvvPOOXnvttSwfkydPtjtmoVW+fHkdP37ctu2vWrVKffv21ejRo9WuXTv5+PjYlgUAAOBKlixZolatWqlmzZr6/vvvNWXKFB09elRut1s1a9a0Ox75ruF8Ts7mtHzs6c6jAwcO2B3hmrZz505VrlzZtu1v3LhRsbGxatCgga6//np1795dXbt2tS0PAABATrp166ann35aH3zwgUJCQuyOkwn5vOPkfE7OJjkrH6UbBSq7e10mJibqm2++0b/+9S/17du3gFP9T+PGjdW4cWNNmTJF7777rmJjYzV48GClp6dr9erVioyMtP1NCwD57aOPPspx+v79+wsoSdbI5x3yecfp+fr06aOZM2dq/fr1np0FTrqwL/m84+R8Ts4mOSufyxhjbNlyITV16tRczcc53Vlzu91yuVxZTnO5XHr00Uc1efJk+fn5FXCy7P3000+aO3euFixYoNOnT6tly5ZX/AAGgMIkN7dqdLlctt0ukXzeIZ93nJ5Pkv744w8tWbJEsbGx+vrrr9W6dWt98skniouLU+3atW3LRb5rP5+TszkpH6U7j6Kioq44j8vl0i+//FIAaQqf9evXZzk+NDRU1apVU3BwcAEnyr20tDR9/PHHio2N1Ycffmh3HAAAgEz27dunOXPmaMGCBTp79qzatWunzp076/7777c7miTyecvJ+ZycTbI3H6U7nx06dEijRo1SbGys3VFwFfr06ZOr+fj5AgAAJzh//ryGDh2q5cuXKyUlRXfffbemTp2qEiVK6JNPPtHcuXP16aefKjk5mXzk+9tkc1o+Snc+27lzp+rVq2frIUaFwd69e/Xhhx/qwIEDcrlcioqKUseOHVWlShVbc7ndblWqVEl169ZVdm8Nl8ulpUuXFnAyALBObk+ZueeeeyxOkjXyeYd83nF6vqFDh2rmzJl66KGHFBAQoMWLF+vOO+/U+++/75nn+PHjKlOmDPnI97fJ5rh8BXI38L+RuLg443a77Y7haC+++KLx9fU1brfblC1b1oSHhxu32238/PzMK6+8Ymu2xx57zBQvXtzUqVPHTJkyxZw8edLWPABQEFwu1xUfdn62kY985MtelSpVzOLFiz3DX3/9tfH19TWpqam2Zfoz8nnHyfmcnM0YZ+VjT3c+Y093ztauXau7775bzz33nJ566inPFQRPnTqlyZMn68UXX9SaNWt0xx132JYxOTlZS5cuVWxsrLZs2aJ27dopJiZGrVq1yvYicAAAAHYoUqSI9u/fr/Lly3vGBQYGas+ePYqMjLQx2SXk846T8zk5m+SsfNwyDAVq1qxZ6tu3r0aNGpVhfIkSJTRmzBglJCTo9ddft7V0+/v768EHH9SDDz6ogwcPav78+erfv79SUlK0a9cuR1/sDQC8cfLkSZUsWVKSdPjwYc2ePVsXLlxQhw4d1LRpU5vTkc9b5POOU/OlpaWpSJEiGcb5+voqNTXVpkQZkc87Ts7n5GySs/KxpzuPrnR1u9OnT2v9+vXs6c5GVFSUFixYoNtvvz3L6Rs3blSPHj1sv+flZYcOHdL8+fM1f/58Xbx4UT/++COlG8A157vvvlOHDh10+PBhVatWTe+++67atGmjc+fOye1269y5c/rPf/6jjh07ko985HNYPrfbrejoaPn7+3vGrVixQnfddZeKFi3qGWfX9WjI5x0n53NyNslZ+SjdedS7d+9czTdv3jyLkxROQUFB2rNnjypUqJDl9CNHjqhatWr6448/CjjZ//z58PJNmzapffv26t27t9q0aZOre3UCQGETHR0tX19fPfPMM1q4cKE+/vhjtWrVSnPmzJEkPfnkk9q2bZu++uor8pGPfA7L5/S/TcnnHSfnc3I2yWH5CvwscvytuVwuc+zYsWynJyQk2Hoxkj9fSG3y5MnmxIkTtmUBgIJSsmRJs3PnTmOMMWfOnDEul8t88803num7d+82YWFhNqUjn7fI5x2n5wPgfJzTjQI3Z86cbA/RPnPmTAGnyWjWrFmqWLGioqKitH79eq1fvz7L+bhlGIBryalTp1S2bFlJUnBwsIoWLaoSJUp4phcvXtzW38/k8w75vOP0fACcj9KNAlWxYkXNnj37ivPYpUePHlyhHMDf0l9/9zntdyH5vEM+7zg9HwBno3SjQB04cMDuCDmaP3++3REAwBa9evXyXGzmwoUL6tevn+dCM8nJyXZGk0Q+b5HPO07PB8DZuJAaClTbtm21ePFihYWFSZLGjRunxx9/XMWKFZN06XYcTZs21a5du2xMCQB/L4662EwWyOcd8nnH6fkAOB+lGwXK7XYrISFBZcqUkSSFhoYqLi5OVapUkSQdO3ZM5cqV45ZrAAAAAK4J3P8ItuI7HwAAAADXMko3AAAAAAAWoXSjQLlcLq4ACgAAAOBvg6uXo0AZY7gCKAAAAIC/DS6khgLFFUABAAAA/J1QugEAAAAAsAjndAMAAAAAYBFKNwAAAAAAFqF0AwAAAABgEUo3AAAAAAAWoXQDAAAAAGARSjcAAIVAr1695HK5Mj327dvn9brnz5+vYsWKeR8SAABk4mt3AAAAkDtt2rTRvHnzMowrXbq0TWmylpKSIj8/P7tjAADgGOzpBgCgkPD391fZsmUzPHx8fLRixQrVr19fAQEBqlKlikaPHq3U1FTPcpMmTdKNN96ookWLKjIyUv3799fZs2clSevWrVPv3r2VmJjo2Xs+atQoSZLL5dLy5cszZChWrJjmz58vSTpw4IBcLpeWLFmiO++8UwEBAVq4cKEkad68eapZs6YCAgJ0/fXXa+bMmZ51XLx4UU888YQiIiIUEBCgypUra/z48da9cAAA2Ig93QAAFGKff/65Hn74YU2dOlVNmzbVzz//rH/+85+SpJEjR0qS3G63pk6dqsqVK2v//v3q37+/nn76ac2cOVNNmjTR5MmT9fzzz+unn36SJAUHB+cpwzPPPKOJEydq3rx58vf31+zZszVy5EhNnz5ddevW1Y4dO/TII4+oaNGi6tmzp6ZOnaqPPvpIS5YsUcWKFXX48GEdPnw4f18YAAAcgtINAEAh8fHHH2coxNHR0Tp27JiGDRumnj17SpKqVKmisWPH6umnn/aU7oEDB3qWiYqK0tixY/XYY49p5syZKlKkiMLCwuRyuVS2bNmryjVw4EDdf//9nuGxY8dq4sSJnnFRUVHatWuX3njjDfXs2VOHDh1StWrVdPvtt8vlcqlSpUpXtV0AAAoDSjcAAIVE8+bN9frrr3uGixYtquuuu07ffPONxo0b5xmflpamCxcu6Pz58woKCtLatWv14osvateuXUpKSlJqaqouXLigc+fOqWjRol7natCggef/f/vtNx0+fFgxMTF65JFHPONTU1MVFhYm6dJF4Vq2bKkaNWqoTZs2at++vVq1auV1DgAAnIjSDQBAIXG5ZP9Zenq6Ro8enWFP82UBAQE6ePCg2rZtq379+mns2LEqUaKENm3apJiYGKWkpOS4PZfLJWNMhnFZLfPn4p6eni5Jmj17tm699dYM8/n4+EiS6tWrp/379+vTTz/V//3f/6lLly66++679Z///CfHPAAAFEaUbgAACrF69erpp59+ylTGL9u6datSU1M1ceJEud2Xrp+6ZMmSDPMUKVJEaWlpmZYtXbq04uPjPcN79+7V+fPnc8wTHh6u8uXL65dfftFDDz2U7XyhoaHq2rWrunbtqs6dO6tNmzY6deqUSpQokeP6AQAobCjdAAAUYs8//7zat2+vyMhI/eMf/5Db7da3336r7777Ti+88IKqVq2q1NRUTZs2TR06dNDmzZs1a9asDOuoXLmyzp49qy+++EJ16tRRUFCQgoKCdNddd2n69Olq1KiR0tPT9cwzz+TqdmCjRo3SgAEDFBoaqujoaCUnJ2vr1q36/fffNXjwYL322muKiIjQzTffLLfbrffff19ly5blXuEAgGsStwwDAKAQa926tT7++GOtXr1at9xyixo1aqRJkyZ5Lk528803a9KkSXr55ZdVu3ZtLVq0KNPtuZo0aaJ+/fqpa9euKl26tCZMmCBJmjhxoiIjI3XHHXeoW7duGjJkiIKCgq6YqW/fvpozZ47mz5+vG2+8Uc2aNdP8+fMVFRUl6dLV0V9++WU1aNBAt9xyiw4cOKCVK1d69sQDAHAtcZm/nqwFAAAAAADyBV8pAwAAAABgEUo3AAAAAAAWoXQDAAAAAGARSjcAAAAAABahdAMAAAAAYBFKNwAAAAAAFqF0AwAAAABgEUo3AAAAAAAWoXQDAAAAAGARSjcAAAAAABahdAMAAAAAYBFKNwAAAAAAFvl/MvFNJftYr84AAAAASUVORK5CYII="
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes -> Train: (17850, 23), Val: (7650, 23), Test: (4500, 23), Encoded Train: (17850, 2), Encoded Val: (7650, 2)\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eb9aljYxJbsK"
   },
   "source": [
    "## Building the network\n",
    "\n",
    "any description/comment about the procedure you followed in the choice of the network structure and hyperparameters goes here, together with consideration about the training/optimization procedure (e.g. optimizer choice, final activations, loss functions, training metrics)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "AS3zsRlBYHKW",
    "ExecuteTime": {
     "end_time": "2024-10-24T09:24:42.133357Z",
     "start_time": "2024-10-24T09:24:23.082914Z"
    }
   },
   "source": [
    "\n",
    "from keras.src.utils import plot_model\n",
    "from keras.src.optimizers import Adam\n",
    "from keras.src.layers import Dense, BatchNormalization, Dropout\n",
    "from keras import Sequential, Input\n",
    "# Importing necessary libraries and modules from Keras and Scikit-learn\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, Callback\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize a StandardScaler object for feature scaling\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the training data and transform it\n",
    "X_train_scaled = scaler.fit_transform(X_train)  \n",
    "\n",
    "# Transform the validation data using the same scaler (to avoid data leakage)\n",
    "X_val_scaled = scaler.transform(X_val) \n",
    "\n",
    "def build_model(input_shape):\n",
    "    \"\"\"\n",
    "    Builds a sequential neural network model.\n",
    "\n",
    "    Parameters:\n",
    "    input_shape (int): The number of input features.\n",
    "\n",
    "    Returns:\n",
    "    model (Sequential): A compiled Keras Sequential model.\n",
    "    \"\"\"\n",
    "    model = Sequential()  # Initialize a Sequential model\n",
    "    model.add(Input(shape=(input_shape,)))  # Input layer with the specified shape\n",
    "\n",
    "    # First hidden layer with 256 units, ReLU activation, and L2 regularization\n",
    "    model.add(Dense(256, activation='relu', kernel_regularizer='l2')) \n",
    "    model.add(BatchNormalization())  # Batch normalization to stabilize training\n",
    "    model.add(Dropout(0.4))  # Dropout layer for regularization to prevent overfitting\n",
    "\n",
    "    # Second hidden layer with 128 units, ReLU activation, and L2 regularization\n",
    "    model.add(Dense(128, activation='relu', kernel_regularizer='l2'))\n",
    "    model.add(BatchNormalization())  # Batch normalization\n",
    "    model.add(Dropout(0.4))  # Dropout layer\n",
    "\n",
    "    # Third hidden layer with 64 units, ReLU activation, and L2 regularization\n",
    "    model.add(Dense(64, activation='relu', kernel_regularizer='l2'))\n",
    "    model.add(BatchNormalization())  # Batch normalization\n",
    "    model.add(Dropout(0.4))  # Dropout layer\n",
    "\n",
    "    # Fourth hidden layer with 32 units, Leaky ReLU activation, and L2 regularization\n",
    "    model.add(Dense(32, activation='leaky_relu', kernel_regularizer='l2'))\n",
    "    model.add(BatchNormalization())  # Batch normalization\n",
    "\n",
    "    # Output layer with a single unit and sigmoid activation for binary classification\n",
    "    model.add(Dense(1, activation='sigmoid'))  \n",
    "\n",
    "    # Compile the model with Adam optimizer, binary cross-entropy loss, and accuracy metric\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001), \n",
    "                  loss='binary_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model  # Return the compiled model\n",
    "\n",
    "# Determine the input shape based on the scaled training data\n",
    "input_shape = X_train_scaled.shape[1]  \n",
    "# Build the model with the defined input shape\n",
    "model = build_model(input_shape)\n",
    "\n",
    "# Set batch size and number of epochs for training\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "\n",
    "# Outputs a model summary table\n",
    "model.summary()\n",
    "\n",
    "# Define an EarlyStopping callback to halt training when validation loss does not improve\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "\n",
    "# Define a ReduceLROnPlateau callback to reduce learning rate when validation loss plateaus\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-6)\n",
    "\n",
    "class ProgressBar(Callback):\n",
    "    \"\"\"\n",
    "    Custom callback to display training progress.\n",
    "\n",
    "    Inherits from Keras Callback to provide additional functionality during training.\n",
    "    \"\"\"\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.epoch = epoch  # Store the current epoch number\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Format and print training metrics at the end of each epoch\n",
    "        metrics = (f\"Epoch {self.epoch + 1}/{self.params['epochs']} - \"\n",
    "                   f\"Loss: {logs['loss']:.4f} - \"\n",
    "                   f\"Val Loss: {logs['val_loss']:.4f} - \"\n",
    "                   f\"Accuracy: {logs['accuracy']:.4f} - \"\n",
    "                   f\"Val Accuracy: {logs['val_accuracy']:.4f}\")\n",
    "        print(\"\\r\" + metrics, end='')  # Print metrics on the same line\n",
    "\n",
    "# Train the model using the fit method with training and validation data\n",
    "history = model.fit(X_train_scaled, y_train, \n",
    "                    validation_data=(X_val_scaled, y_val),\n",
    "                    epochs=epochs, batch_size=batch_size, \n",
    "                    callbacks=[early_stopping, reduce_lr, ProgressBar()],\n",
    "                    verbose=1)  # Set verbose to 0 to suppress output\n",
    "\n",
    "# Evaluate the model's performance on the validation set\n",
    "val_loss, val_accuracy = model.evaluate(X_val_scaled, y_val, verbose=0)\n",
    "\n",
    "# Print results for validation loss and accuracy\n",
    "print()\n",
    "print(\"\\nRESULTS:\")\n",
    "print(f\"{'Validation Loss:':<20} {val_loss:.4f}\")\n",
    "print(f\"{'Validation Accuracy:':<20} {val_accuracy:.4f}\")\n",
    "\n",
    "# Generate predictions on the validation set and convert probabilities to binary labels\n",
    "y_pred_prob = model.predict(X_val_scaled, verbose=0) \n",
    "y_pred = (y_pred_prob > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
    "\n",
    "# Calculate the F1 score to evaluate the model's performance\n",
    "f1 = f1_score(y_val, y_pred)  \n",
    "print(f\"{'F1 Score:':<20} {f1:.4f}\")  # Print the F1 score"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"sequential_6\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_6\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m\n",
       "\n",
       " dense_30 (\u001B[38;5;33mDense\u001B[0m)                 (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m256\u001B[0m)                     \u001B[38;5;34m6,144\u001B[0m \n",
       "\n",
       " batch_normalization_20           (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m256\u001B[0m)                     \u001B[38;5;34m1,024\u001B[0m \n",
       " (\u001B[38;5;33mBatchNormalization\u001B[0m)                                                   \n",
       "\n",
       " dropout_15 (\u001B[38;5;33mDropout\u001B[0m)             (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m256\u001B[0m)                         \u001B[38;5;34m0\u001B[0m \n",
       "\n",
       " dense_31 (\u001B[38;5;33mDense\u001B[0m)                 (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m128\u001B[0m)                    \u001B[38;5;34m32,896\u001B[0m \n",
       "\n",
       " batch_normalization_21           (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m128\u001B[0m)                       \u001B[38;5;34m512\u001B[0m \n",
       " (\u001B[38;5;33mBatchNormalization\u001B[0m)                                                   \n",
       "\n",
       " dropout_16 (\u001B[38;5;33mDropout\u001B[0m)             (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m128\u001B[0m)                         \u001B[38;5;34m0\u001B[0m \n",
       "\n",
       " dense_32 (\u001B[38;5;33mDense\u001B[0m)                 (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m64\u001B[0m)                      \u001B[38;5;34m8,256\u001B[0m \n",
       "\n",
       " batch_normalization_22           (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m64\u001B[0m)                        \u001B[38;5;34m256\u001B[0m \n",
       " (\u001B[38;5;33mBatchNormalization\u001B[0m)                                                   \n",
       "\n",
       " dropout_17 (\u001B[38;5;33mDropout\u001B[0m)             (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m64\u001B[0m)                          \u001B[38;5;34m0\u001B[0m \n",
       "\n",
       " dense_33 (\u001B[38;5;33mDense\u001B[0m)                 (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m32\u001B[0m)                      \u001B[38;5;34m2,080\u001B[0m \n",
       "\n",
       " batch_normalization_23           (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m32\u001B[0m)                        \u001B[38;5;34m128\u001B[0m \n",
       " (\u001B[38;5;33mBatchNormalization\u001B[0m)                                                   \n",
       "\n",
       " dense_34 (\u001B[38;5;33mDense\u001B[0m)                 (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m1\u001B[0m)                          \u001B[38;5;34m33\u001B[0m \n",
       "\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\"> Layer (type)                    </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">       Param # </span>\n",
       "\n",
       " dense_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">6,144</span> \n",
       "\n",
       " batch_normalization_20           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                   \n",
       "\n",
       " dropout_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " dense_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> \n",
       "\n",
       " batch_normalization_21           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                       <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                   \n",
       "\n",
       " dropout_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " dense_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> \n",
       "\n",
       " batch_normalization_22           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                   \n",
       "\n",
       " dropout_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " dense_33 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> \n",
       "\n",
       " batch_normalization_23           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                   \n",
       "\n",
       " dense_34 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> \n",
       "\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m51,329\u001B[0m (200.50 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">51,329</span> (200.50 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m50,369\u001B[0m (196.75 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">50,369</span> (196.75 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m960\u001B[0m (3.75 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">960</span> (3.75 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 6ms/step - accuracy: 0.5301 - loss: 4.1722 - val_accuracy: 0.8005 - val_loss: 3.6435 - learning_rate: 1.0000e-04\n",
      "Epoch 2/10\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 5ms/step - accuracy: 0.6440 - loss: 3.6758 - val_accuracy: 0.8082 - val_loss: 3.2760 - learning_rate: 1.0000e-04\n",
      "Epoch 3/10\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 5ms/step - accuracy: 0.7129 - loss: 3.2915 - val_accuracy: 0.8119 - val_loss: 2.9473 - learning_rate: 1.0000e-04\n",
      "Epoch 4/10\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 7ms/step - accuracy: 0.7570 - loss: 2.9409 - val_accuracy: 0.8077 - val_loss: 2.6465 - learning_rate: 1.0000e-04\n",
      "Epoch 5/10\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 6ms/step - accuracy: 0.7789 - loss: 2.6387 - val_accuracy: 0.8072 - val_loss: 2.3781 - learning_rate: 1.0000e-04\n",
      "Epoch 6/10\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 6ms/step - accuracy: 0.7948 - loss: 2.3612 - val_accuracy: 0.8075 - val_loss: 2.1402 - learning_rate: 1.0000e-04\n",
      "Epoch 7/10\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 5ms/step - accuracy: 0.7964 - loss: 2.1236 - val_accuracy: 0.8082 - val_loss: 1.9314 - learning_rate: 1.0000e-04\n",
      "Epoch 8/10\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 6ms/step - accuracy: 0.8002 - loss: 1.9172 - val_accuracy: 0.8088 - val_loss: 1.7487 - learning_rate: 1.0000e-04\n",
      "Epoch 9/10\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 6ms/step - accuracy: 0.8046 - loss: 1.7335 - val_accuracy: 0.8110 - val_loss: 1.5909 - learning_rate: 1.0000e-04\n",
      "Epoch 10/10\n",
      "\u001B[1m169/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - accuracy: 0.8004 - loss: 1.5997"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[16], line 98\u001B[0m\n\u001B[1;32m     95\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\r\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m metrics, end\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m)  \u001B[38;5;66;03m# Print metrics on the same line\u001B[39;00m\n\u001B[1;32m     97\u001B[0m \u001B[38;5;66;03m# Train the model using the fit method with training and validation data\u001B[39;00m\n\u001B[0;32m---> 98\u001B[0m history \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mfit(X_train_scaled, y_train, \n\u001B[1;32m     99\u001B[0m                     validation_data\u001B[38;5;241m=\u001B[39m(X_val_scaled, y_val),\n\u001B[1;32m    100\u001B[0m                     epochs\u001B[38;5;241m=\u001B[39mepochs, batch_size\u001B[38;5;241m=\u001B[39mbatch_size, \n\u001B[1;32m    101\u001B[0m                     callbacks\u001B[38;5;241m=\u001B[39m[early_stopping, reduce_lr, ProgressBar()],\n\u001B[1;32m    102\u001B[0m                     verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)  \u001B[38;5;66;03m# Set verbose to 0 to suppress output\u001B[39;00m\n\u001B[1;32m    104\u001B[0m \u001B[38;5;66;03m# Evaluate the model's performance on the validation set\u001B[39;00m\n\u001B[1;32m    105\u001B[0m val_loss, val_accuracy \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mevaluate(X_val_scaled, y_val, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    115\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    116\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 117\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    118\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    119\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:320\u001B[0m, in \u001B[0;36mTensorFlowTrainer.fit\u001B[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001B[0m\n\u001B[1;32m    318\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m step, iterator \u001B[38;5;129;01min\u001B[39;00m epoch_iterator\u001B[38;5;241m.\u001B[39menumerate_epoch():\n\u001B[1;32m    319\u001B[0m     callbacks\u001B[38;5;241m.\u001B[39mon_train_batch_begin(step)\n\u001B[0;32m--> 320\u001B[0m     logs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_function(iterator)\n\u001B[1;32m    321\u001B[0m     callbacks\u001B[38;5;241m.\u001B[39mon_train_batch_end(step, logs)\n\u001B[1;32m    322\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstop_training:\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    148\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    149\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 150\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    830\u001B[0m compiler \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxla\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnonXla\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    832\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m OptionalXlaContext(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile):\n\u001B[0;32m--> 833\u001B[0m   result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[1;32m    835\u001B[0m new_tracing_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexperimental_get_tracing_count()\n\u001B[1;32m    836\u001B[0m without_tracing \u001B[38;5;241m=\u001B[39m (tracing_count \u001B[38;5;241m==\u001B[39m new_tracing_count)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001B[0m, in \u001B[0;36mFunction._call\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    875\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n\u001B[1;32m    876\u001B[0m \u001B[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001B[39;00m\n\u001B[1;32m    877\u001B[0m \u001B[38;5;66;03m# run the first trace but we should fail if variables are created.\u001B[39;00m\n\u001B[0;32m--> 878\u001B[0m results \u001B[38;5;241m=\u001B[39m tracing_compilation\u001B[38;5;241m.\u001B[39mcall_function(\n\u001B[1;32m    879\u001B[0m     args, kwds, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_variable_creation_config\n\u001B[1;32m    880\u001B[0m )\n\u001B[1;32m    881\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_created_variables:\n\u001B[1;32m    882\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCreating variables on a non-first call to a function\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    883\u001B[0m                    \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m decorated with tf.function.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001B[0m, in \u001B[0;36mcall_function\u001B[0;34m(args, kwargs, tracing_options)\u001B[0m\n\u001B[1;32m    137\u001B[0m bound_args \u001B[38;5;241m=\u001B[39m function\u001B[38;5;241m.\u001B[39mfunction_type\u001B[38;5;241m.\u001B[39mbind(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    138\u001B[0m flat_inputs \u001B[38;5;241m=\u001B[39m function\u001B[38;5;241m.\u001B[39mfunction_type\u001B[38;5;241m.\u001B[39munpack_inputs(bound_args)\n\u001B[0;32m--> 139\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m function\u001B[38;5;241m.\u001B[39m_call_flat(  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n\u001B[1;32m    140\u001B[0m     flat_inputs, captured_inputs\u001B[38;5;241m=\u001B[39mfunction\u001B[38;5;241m.\u001B[39mcaptured_inputs\n\u001B[1;32m    141\u001B[0m )\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001B[0m, in \u001B[0;36mConcreteFunction._call_flat\u001B[0;34m(self, tensor_inputs, captured_inputs)\u001B[0m\n\u001B[1;32m   1318\u001B[0m possible_gradient_type \u001B[38;5;241m=\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPossibleTapeGradientTypes(args)\n\u001B[1;32m   1319\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (possible_gradient_type \u001B[38;5;241m==\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001B[1;32m   1320\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m executing_eagerly):\n\u001B[1;32m   1321\u001B[0m   \u001B[38;5;66;03m# No tape is watching; skip to running the function.\u001B[39;00m\n\u001B[0;32m-> 1322\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_inference_function\u001B[38;5;241m.\u001B[39mcall_preflattened(args)\n\u001B[1;32m   1323\u001B[0m forward_backward \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_select_forward_and_backward_functions(\n\u001B[1;32m   1324\u001B[0m     args,\n\u001B[1;32m   1325\u001B[0m     possible_gradient_type,\n\u001B[1;32m   1326\u001B[0m     executing_eagerly)\n\u001B[1;32m   1327\u001B[0m forward_function, args_with_tangents \u001B[38;5;241m=\u001B[39m forward_backward\u001B[38;5;241m.\u001B[39mforward()\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001B[0m, in \u001B[0;36mAtomicFunction.call_preflattened\u001B[0;34m(self, args)\u001B[0m\n\u001B[1;32m    214\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcall_preflattened\u001B[39m(\u001B[38;5;28mself\u001B[39m, args: Sequence[core\u001B[38;5;241m.\u001B[39mTensor]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m    215\u001B[0m \u001B[38;5;250m  \u001B[39m\u001B[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 216\u001B[0m   flat_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcall_flat(\u001B[38;5;241m*\u001B[39margs)\n\u001B[1;32m    217\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfunction_type\u001B[38;5;241m.\u001B[39mpack_output(flat_outputs)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001B[0m, in \u001B[0;36mAtomicFunction.call_flat\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m    249\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m record\u001B[38;5;241m.\u001B[39mstop_recording():\n\u001B[1;32m    250\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_bound_context\u001B[38;5;241m.\u001B[39mexecuting_eagerly():\n\u001B[0;32m--> 251\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_bound_context\u001B[38;5;241m.\u001B[39mcall_function(\n\u001B[1;32m    252\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname,\n\u001B[1;32m    253\u001B[0m         \u001B[38;5;28mlist\u001B[39m(args),\n\u001B[1;32m    254\u001B[0m         \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfunction_type\u001B[38;5;241m.\u001B[39mflat_outputs),\n\u001B[1;32m    255\u001B[0m     )\n\u001B[1;32m    256\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    257\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m make_call_op_in_graph(\n\u001B[1;32m    258\u001B[0m         \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    259\u001B[0m         \u001B[38;5;28mlist\u001B[39m(args),\n\u001B[1;32m    260\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_bound_context\u001B[38;5;241m.\u001B[39mfunction_call_options\u001B[38;5;241m.\u001B[39mas_attrs(),\n\u001B[1;32m    261\u001B[0m     )\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1500\u001B[0m, in \u001B[0;36mContext.call_function\u001B[0;34m(self, name, tensor_inputs, num_outputs)\u001B[0m\n\u001B[1;32m   1498\u001B[0m cancellation_context \u001B[38;5;241m=\u001B[39m cancellation\u001B[38;5;241m.\u001B[39mcontext()\n\u001B[1;32m   1499\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m cancellation_context \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1500\u001B[0m   outputs \u001B[38;5;241m=\u001B[39m execute\u001B[38;5;241m.\u001B[39mexecute(\n\u001B[1;32m   1501\u001B[0m       name\u001B[38;5;241m.\u001B[39mdecode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m   1502\u001B[0m       num_outputs\u001B[38;5;241m=\u001B[39mnum_outputs,\n\u001B[1;32m   1503\u001B[0m       inputs\u001B[38;5;241m=\u001B[39mtensor_inputs,\n\u001B[1;32m   1504\u001B[0m       attrs\u001B[38;5;241m=\u001B[39mattrs,\n\u001B[1;32m   1505\u001B[0m       ctx\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   1506\u001B[0m   )\n\u001B[1;32m   1507\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1508\u001B[0m   outputs \u001B[38;5;241m=\u001B[39m execute\u001B[38;5;241m.\u001B[39mexecute_with_cancellation(\n\u001B[1;32m   1509\u001B[0m       name\u001B[38;5;241m.\u001B[39mdecode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m   1510\u001B[0m       num_outputs\u001B[38;5;241m=\u001B[39mnum_outputs,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1514\u001B[0m       cancellation_manager\u001B[38;5;241m=\u001B[39mcancellation_context,\n\u001B[1;32m   1515\u001B[0m   )\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001B[0m, in \u001B[0;36mquick_execute\u001B[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[1;32m     51\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     52\u001B[0m   ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[0;32m---> 53\u001B[0m   tensors \u001B[38;5;241m=\u001B[39m pywrap_tfe\u001B[38;5;241m.\u001B[39mTFE_Py_Execute(ctx\u001B[38;5;241m.\u001B[39m_handle, device_name, op_name,\n\u001B[1;32m     54\u001B[0m                                       inputs, attrs, num_outputs)\n\u001B[1;32m     55\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     56\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T09:17:30.251414Z",
     "start_time": "2024-10-24T09:14:46.049244Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "from keras.src.utils import plot_model\n",
    "from keras.src.optimizers import Adam\n",
    "from keras.src.layers import Dense, BatchNormalization, Dropout\n",
    "from keras import Sequential, Input\n",
    "# Importing necessary libraries and modules from Keras and Scikit-learn\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, Callback\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize a StandardScaler object for feature scaling\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the training data and transform it\n",
    "X_train_scaled = scaler.fit_transform(X_train)  \n",
    "\n",
    "# Transform the validation data using the same scaler (to avoid data leakage)\n",
    "X_val_scaled = scaler.transform(X_val) \n",
    "\n",
    "def build_model(input_shape):\n",
    "    \"\"\"\n",
    "    Builds a sequential neural network model.\n",
    "\n",
    "    Parameters:\n",
    "    input_shape (int): The number of input features.\n",
    "\n",
    "    Returns:\n",
    "    model (Sequential): A compiled Keras Sequential model.\n",
    "    \"\"\"\n",
    "    model = Sequential()  # Initialize a Sequential model\n",
    "    model.add(Input(shape=(input_shape,)))  # Input layer with the specified shape\n",
    "\n",
    "    # First hidden layer with 256 units, ReLU activation, and L2 regularization\n",
    "    model.add(Dense(256, activation='relu', kernel_regularizer='l2')) \n",
    "\n",
    "    # Second hidden layer with 128 units, ReLU activation, and L2 regularization\n",
    "    model.add(Dense(128, activation='relu', kernel_regularizer='l2'))\n",
    "\n",
    "\n",
    "    # Third hidden layer with 64 units, ReLU activation, and L2 regularization\n",
    "    model.add(Dense(64, activation='relu', kernel_regularizer='l2'))\n",
    "\n",
    "    # Fourth hidden layer with 32 units, Leaky ReLU activation, and L2 regularization\n",
    "    model.add(Dense(32, activation='relu', kernel_regularizer='l2'))\n",
    "\n",
    "    # Output layer with a single unit and sigmoid activation for binary classification\n",
    "    model.add(Dense(1, activation='sigmoid'))  \n",
    "\n",
    "    # Compile the model with Adam optimizer, binary cross-entropy loss, and accuracy metric\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001), \n",
    "                  loss='binary_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model  # Return the compiled model\n",
    "\n",
    "# Determine the input shape based on the scaled training data\n",
    "input_shape = X_train_scaled.shape[1]  \n",
    "# Build the model with the defined input shape\n",
    "model = build_model(input_shape)\n",
    "\n",
    "# Set batch size and number of epochs for training\n",
    "batch_size = 64\n",
    "epochs = 200\n",
    "\n",
    "# Outputs a model summary table\n",
    "model.summary()\n",
    "\n",
    "# Define an EarlyStopping callback to halt training when validation loss does not improve\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "\n",
    "# Define a ReduceLROnPlateau callback to reduce learning rate when validation loss plateaus\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-6)\n",
    "\n",
    "class ProgressBar(Callback):\n",
    "    \"\"\"\n",
    "    Custom callback to display training progress.\n",
    "\n",
    "    Inherits from Keras Callback to provide additional functionality during training.\n",
    "    \"\"\"\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.epoch = epoch  # Store the current epoch number\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Format and print training metrics at the end of each epoch\n",
    "        metrics = (f\"Epoch {self.epoch + 1}/{self.params['epochs']} - \"\n",
    "                   f\"Loss: {logs['loss']:.4f} - \"\n",
    "                   f\"Val Loss: {logs['val_loss']:.4f} - \"\n",
    "                   f\"Accuracy: {logs['accuracy']:.4f} - \"\n",
    "                   f\"Val Accuracy: {logs['val_accuracy']:.4f}\")\n",
    "        print(\"\\r\" + metrics, end='')  # Print metrics on the same line\n",
    "\n",
    "# Train the model using the fit method with training and validation data\n",
    "history = model.fit(X_train_scaled, y_train, \n",
    "                    validation_data=(X_val_scaled, y_val),\n",
    "                    epochs=epochs, batch_size=batch_size, \n",
    "                    callbacks=[early_stopping, reduce_lr, ProgressBar()],\n",
    "                    verbose=1)  # Set verbose to 0 to suppress output\n",
    "\n",
    "# Evaluate the model's performance on the validation set\n",
    "val_loss, val_accuracy = model.evaluate(X_val_scaled, y_val, verbose=0)\n",
    "\n",
    "# Print results for validation loss and accuracy\n",
    "print()\n",
    "print(\"\\nRESULTS:\")\n",
    "print(f\"{'Validation Loss:':<20} {val_loss:.4f}\")\n",
    "print(f\"{'Validation Accuracy:':<20} {val_accuracy:.4f}\")\n",
    "\n",
    "# Generate predictions on the validation set and convert probabilities to binary labels\n",
    "y_pred_prob = model.predict(X_val_scaled, verbose=0) \n",
    "y_pred = (y_pred_prob > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
    "\n",
    "# Calculate the F1 score to evaluate the model's performance\n",
    "f1 = f1_score(y_val, y_pred)  \n",
    "print(f\"{'F1 Score:':<20} {f1:.4f}\")  # Print the F1 score"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"sequential_4\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m\n",
       "\n",
       " dense_20 (\u001B[38;5;33mDense\u001B[0m)                 (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m256\u001B[0m)                     \u001B[38;5;34m6,144\u001B[0m \n",
       "\n",
       " dense_21 (\u001B[38;5;33mDense\u001B[0m)                 (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m128\u001B[0m)                    \u001B[38;5;34m32,896\u001B[0m \n",
       "\n",
       " dense_22 (\u001B[38;5;33mDense\u001B[0m)                 (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m64\u001B[0m)                      \u001B[38;5;34m8,256\u001B[0m \n",
       "\n",
       " dense_23 (\u001B[38;5;33mDense\u001B[0m)                 (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m32\u001B[0m)                      \u001B[38;5;34m2,080\u001B[0m \n",
       "\n",
       " dense_24 (\u001B[38;5;33mDense\u001B[0m)                 (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m1\u001B[0m)                          \u001B[38;5;34m33\u001B[0m \n",
       "\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\"> Layer (type)                    </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">       Param # </span>\n",
       "\n",
       " dense_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">6,144</span> \n",
       "\n",
       " dense_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> \n",
       "\n",
       " dense_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> \n",
       "\n",
       " dense_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> \n",
       "\n",
       " dense_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> \n",
       "\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m49,409\u001B[0m (193.00 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">49,409</span> (193.00 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m49,409\u001B[0m (193.00 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">49,409</span> (193.00 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 3ms/step - accuracy: 0.6989 - loss: 3.6457 - val_accuracy: 0.7993 - val_loss: 2.6429 - learning_rate: 1.0000e-04\n",
      "Epoch 2/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 2ms/step - accuracy: 0.8009 - loss: 2.4062 - val_accuracy: 0.8075 - val_loss: 1.8136 - learning_rate: 1.0000e-04\n",
      "Epoch 3/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8072 - loss: 1.6690 - val_accuracy: 0.8094 - val_loss: 1.3022 - learning_rate: 1.0000e-04\n",
      "Epoch 4/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8132 - loss: 1.2075 - val_accuracy: 0.8110 - val_loss: 0.9891 - learning_rate: 1.0000e-04\n",
      "Epoch 5/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8103 - loss: 0.9340 - val_accuracy: 0.8119 - val_loss: 0.7980 - learning_rate: 1.0000e-04\n",
      "Epoch 6/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8137 - loss: 0.7627 - val_accuracy: 0.8093 - val_loss: 0.6820 - learning_rate: 1.0000e-04\n",
      "Epoch 7/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8089 - loss: 0.6640 - val_accuracy: 0.8136 - val_loss: 0.6101 - learning_rate: 1.0000e-04\n",
      "Epoch 8/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8156 - loss: 0.5977 - val_accuracy: 0.8133 - val_loss: 0.5661 - learning_rate: 1.0000e-04\n",
      "Epoch 9/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8168 - loss: 0.5568 - val_accuracy: 0.8140 - val_loss: 0.5381 - learning_rate: 1.0000e-04\n",
      "Epoch 10/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8142 - loss: 0.5291 - val_accuracy: 0.8158 - val_loss: 0.5210 - learning_rate: 1.0000e-04\n",
      "Epoch 11/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8138 - loss: 0.5208 - val_accuracy: 0.8156 - val_loss: 0.5088 - learning_rate: 1.0000e-04\n",
      "Epoch 12/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8151 - loss: 0.5070 - val_accuracy: 0.8142 - val_loss: 0.5013 - learning_rate: 1.0000e-04\n",
      "Epoch 13/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8095 - loss: 0.5051 - val_accuracy: 0.8170 - val_loss: 0.4961 - learning_rate: 1.0000e-04\n",
      "Epoch 14/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8131 - loss: 0.4974 - val_accuracy: 0.8149 - val_loss: 0.4919 - learning_rate: 1.0000e-04\n",
      "Epoch 15/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8122 - loss: 0.4922 - val_accuracy: 0.8171 - val_loss: 0.4893 - learning_rate: 1.0000e-04\n",
      "Epoch 16/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8224 - loss: 0.4792 - val_accuracy: 0.8137 - val_loss: 0.4872 - learning_rate: 1.0000e-04\n",
      "Epoch 17/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8108 - loss: 0.4923 - val_accuracy: 0.8156 - val_loss: 0.4856 - learning_rate: 1.0000e-04\n",
      "Epoch 18/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8190 - loss: 0.4806 - val_accuracy: 0.8161 - val_loss: 0.4837 - learning_rate: 1.0000e-04\n",
      "Epoch 19/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8179 - loss: 0.4805 - val_accuracy: 0.8166 - val_loss: 0.4829 - learning_rate: 1.0000e-04\n",
      "Epoch 20/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8143 - loss: 0.4849 - val_accuracy: 0.8167 - val_loss: 0.4814 - learning_rate: 1.0000e-04\n",
      "Epoch 21/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8190 - loss: 0.4742 - val_accuracy: 0.8146 - val_loss: 0.4807 - learning_rate: 1.0000e-04\n",
      "Epoch 22/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8123 - loss: 0.4844 - val_accuracy: 0.8118 - val_loss: 0.4809 - learning_rate: 1.0000e-04\n",
      "Epoch 23/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8100 - loss: 0.4876 - val_accuracy: 0.8166 - val_loss: 0.4792 - learning_rate: 1.0000e-04\n",
      "Epoch 24/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8144 - loss: 0.4784 - val_accuracy: 0.8161 - val_loss: 0.4790 - learning_rate: 1.0000e-04\n",
      "Epoch 25/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8142 - loss: 0.4796 - val_accuracy: 0.8174 - val_loss: 0.4783 - learning_rate: 1.0000e-04\n",
      "Epoch 26/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8160 - loss: 0.4771 - val_accuracy: 0.8131 - val_loss: 0.4796 - learning_rate: 1.0000e-04\n",
      "Epoch 27/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8134 - loss: 0.4815 - val_accuracy: 0.8173 - val_loss: 0.4779 - learning_rate: 1.0000e-04\n",
      "Epoch 28/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8186 - loss: 0.4736 - val_accuracy: 0.8153 - val_loss: 0.4770 - learning_rate: 1.0000e-04\n",
      "Epoch 29/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8140 - loss: 0.4774 - val_accuracy: 0.8129 - val_loss: 0.4769 - learning_rate: 1.0000e-04\n",
      "Epoch 30/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8076 - loss: 0.4890 - val_accuracy: 0.8182 - val_loss: 0.4758 - learning_rate: 1.0000e-04\n",
      "Epoch 31/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8164 - loss: 0.4784 - val_accuracy: 0.8179 - val_loss: 0.4768 - learning_rate: 1.0000e-04\n",
      "Epoch 32/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8103 - loss: 0.4848 - val_accuracy: 0.8165 - val_loss: 0.4749 - learning_rate: 1.0000e-04\n",
      "Epoch 33/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8124 - loss: 0.4785 - val_accuracy: 0.8153 - val_loss: 0.4754 - learning_rate: 1.0000e-04\n",
      "Epoch 34/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8107 - loss: 0.4826 - val_accuracy: 0.8170 - val_loss: 0.4744 - learning_rate: 1.0000e-04\n",
      "Epoch 35/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8196 - loss: 0.4685 - val_accuracy: 0.8180 - val_loss: 0.4745 - learning_rate: 1.0000e-04\n",
      "Epoch 36/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8223 - loss: 0.4655 - val_accuracy: 0.8156 - val_loss: 0.4743 - learning_rate: 1.0000e-04\n",
      "Epoch 37/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8186 - loss: 0.4720 - val_accuracy: 0.8183 - val_loss: 0.4746 - learning_rate: 1.0000e-04\n",
      "Epoch 38/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8215 - loss: 0.4674 - val_accuracy: 0.8192 - val_loss: 0.4750 - learning_rate: 1.0000e-04\n",
      "Epoch 39/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8165 - loss: 0.4754 - val_accuracy: 0.8159 - val_loss: 0.4733 - learning_rate: 1.0000e-04\n",
      "Epoch 40/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8152 - loss: 0.4750 - val_accuracy: 0.8188 - val_loss: 0.4742 - learning_rate: 1.0000e-04\n",
      "Epoch 41/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8189 - loss: 0.4727 - val_accuracy: 0.8182 - val_loss: 0.4727 - learning_rate: 1.0000e-04\n",
      "Epoch 42/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8145 - loss: 0.4747 - val_accuracy: 0.8182 - val_loss: 0.4729 - learning_rate: 1.0000e-04\n",
      "Epoch 43/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8149 - loss: 0.4765 - val_accuracy: 0.8158 - val_loss: 0.4731 - learning_rate: 1.0000e-04\n",
      "Epoch 44/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8227 - loss: 0.4634 - val_accuracy: 0.8146 - val_loss: 0.4729 - learning_rate: 1.0000e-04\n",
      "Epoch 45/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8165 - loss: 0.4732 - val_accuracy: 0.8182 - val_loss: 0.4725 - learning_rate: 1.0000e-04\n",
      "Epoch 46/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8170 - loss: 0.4757 - val_accuracy: 0.8146 - val_loss: 0.4721 - learning_rate: 1.0000e-04\n",
      "Epoch 47/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8158 - loss: 0.4748 - val_accuracy: 0.8148 - val_loss: 0.4722 - learning_rate: 1.0000e-04\n",
      "Epoch 48/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8155 - loss: 0.4787 - val_accuracy: 0.8179 - val_loss: 0.4719 - learning_rate: 1.0000e-04\n",
      "Epoch 49/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8168 - loss: 0.4725 - val_accuracy: 0.8162 - val_loss: 0.4714 - learning_rate: 1.0000e-04\n",
      "Epoch 50/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8169 - loss: 0.4686 - val_accuracy: 0.8163 - val_loss: 0.4714 - learning_rate: 1.0000e-04\n",
      "Epoch 51/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8134 - loss: 0.4765 - val_accuracy: 0.8178 - val_loss: 0.4726 - learning_rate: 1.0000e-04\n",
      "Epoch 52/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8205 - loss: 0.4684 - val_accuracy: 0.8182 - val_loss: 0.4705 - learning_rate: 1.0000e-04\n",
      "Epoch 53/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8179 - loss: 0.4702 - val_accuracy: 0.8178 - val_loss: 0.4720 - learning_rate: 1.0000e-04\n",
      "Epoch 54/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8209 - loss: 0.4662 - val_accuracy: 0.8174 - val_loss: 0.4702 - learning_rate: 1.0000e-04\n",
      "Epoch 55/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8121 - loss: 0.4802 - val_accuracy: 0.8184 - val_loss: 0.4701 - learning_rate: 1.0000e-04\n",
      "Epoch 56/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8148 - loss: 0.4759 - val_accuracy: 0.8170 - val_loss: 0.4702 - learning_rate: 1.0000e-04\n",
      "Epoch 57/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8124 - loss: 0.4761 - val_accuracy: 0.8184 - val_loss: 0.4702 - learning_rate: 1.0000e-04\n",
      "Epoch 58/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8203 - loss: 0.4689 - val_accuracy: 0.8169 - val_loss: 0.4702 - learning_rate: 1.0000e-04\n",
      "Epoch 59/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8204 - loss: 0.4656 - val_accuracy: 0.8183 - val_loss: 0.4696 - learning_rate: 1.0000e-04\n",
      "Epoch 60/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8157 - loss: 0.4760 - val_accuracy: 0.8175 - val_loss: 0.4705 - learning_rate: 1.0000e-04\n",
      "Epoch 61/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 4ms/step - accuracy: 0.8153 - loss: 0.4733 - val_accuracy: 0.8167 - val_loss: 0.4693 - learning_rate: 1.0000e-04\n",
      "Epoch 62/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 4ms/step - accuracy: 0.8166 - loss: 0.4700 - val_accuracy: 0.8167 - val_loss: 0.4692 - learning_rate: 1.0000e-04\n",
      "Epoch 63/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8148 - loss: 0.4711 - val_accuracy: 0.8180 - val_loss: 0.4692 - learning_rate: 1.0000e-04\n",
      "Epoch 64/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8192 - loss: 0.4680 - val_accuracy: 0.8190 - val_loss: 0.4688 - learning_rate: 1.0000e-04\n",
      "Epoch 65/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8171 - loss: 0.4722 - val_accuracy: 0.8166 - val_loss: 0.4692 - learning_rate: 1.0000e-04\n",
      "Epoch 66/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8176 - loss: 0.4711 - val_accuracy: 0.8174 - val_loss: 0.4688 - learning_rate: 1.0000e-04\n",
      "Epoch 67/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8202 - loss: 0.4665 - val_accuracy: 0.8186 - val_loss: 0.4687 - learning_rate: 1.0000e-04\n",
      "Epoch 68/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8203 - loss: 0.4665 - val_accuracy: 0.8170 - val_loss: 0.4688 - learning_rate: 1.0000e-04\n",
      "Epoch 69/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8194 - loss: 0.4621 - val_accuracy: 0.8169 - val_loss: 0.4684 - learning_rate: 1.0000e-04\n",
      "Epoch 70/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8149 - loss: 0.4711 - val_accuracy: 0.8188 - val_loss: 0.4682 - learning_rate: 1.0000e-04\n",
      "Epoch 71/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8212 - loss: 0.4634 - val_accuracy: 0.8142 - val_loss: 0.4697 - learning_rate: 1.0000e-04\n",
      "Epoch 72/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8156 - loss: 0.4696 - val_accuracy: 0.8191 - val_loss: 0.4681 - learning_rate: 1.0000e-04\n",
      "Epoch 73/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8133 - loss: 0.4760 - val_accuracy: 0.8182 - val_loss: 0.4679 - learning_rate: 1.0000e-04\n",
      "Epoch 74/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8149 - loss: 0.4732 - val_accuracy: 0.8184 - val_loss: 0.4681 - learning_rate: 1.0000e-04\n",
      "Epoch 75/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8218 - loss: 0.4661 - val_accuracy: 0.8183 - val_loss: 0.4681 - learning_rate: 1.0000e-04\n",
      "Epoch 76/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8180 - loss: 0.4705 - val_accuracy: 0.8167 - val_loss: 0.4678 - learning_rate: 1.0000e-04\n",
      "Epoch 77/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8253 - loss: 0.4593 - val_accuracy: 0.8158 - val_loss: 0.4696 - learning_rate: 1.0000e-04\n",
      "Epoch 78/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8119 - loss: 0.4726 - val_accuracy: 0.8178 - val_loss: 0.4676 - learning_rate: 1.0000e-04\n",
      "Epoch 79/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8164 - loss: 0.4700 - val_accuracy: 0.8192 - val_loss: 0.4683 - learning_rate: 1.0000e-04\n",
      "Epoch 80/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8212 - loss: 0.4663 - val_accuracy: 0.8173 - val_loss: 0.4674 - learning_rate: 1.0000e-04\n",
      "Epoch 81/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8195 - loss: 0.4619 - val_accuracy: 0.8175 - val_loss: 0.4669 - learning_rate: 1.0000e-04\n",
      "Epoch 82/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8173 - loss: 0.4688 - val_accuracy: 0.8165 - val_loss: 0.4677 - learning_rate: 1.0000e-04\n",
      "Epoch 83/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8181 - loss: 0.4664 - val_accuracy: 0.8184 - val_loss: 0.4669 - learning_rate: 1.0000e-04\n",
      "Epoch 84/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8221 - loss: 0.4643 - val_accuracy: 0.8179 - val_loss: 0.4668 - learning_rate: 1.0000e-04\n",
      "Epoch 85/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8169 - loss: 0.4693 - val_accuracy: 0.8190 - val_loss: 0.4670 - learning_rate: 1.0000e-04\n",
      "Epoch 86/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8235 - loss: 0.4607 - val_accuracy: 0.8173 - val_loss: 0.4670 - learning_rate: 1.0000e-04\n",
      "Epoch 87/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8163 - loss: 0.4734 - val_accuracy: 0.8187 - val_loss: 0.4667 - learning_rate: 1.0000e-04\n",
      "Epoch 88/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8163 - loss: 0.4702 - val_accuracy: 0.8195 - val_loss: 0.4674 - learning_rate: 1.0000e-04\n",
      "Epoch 89/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8174 - loss: 0.4718 - val_accuracy: 0.8187 - val_loss: 0.4662 - learning_rate: 1.0000e-04\n",
      "Epoch 90/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8171 - loss: 0.4682 - val_accuracy: 0.8195 - val_loss: 0.4666 - learning_rate: 1.0000e-04\n",
      "Epoch 91/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8153 - loss: 0.4714 - val_accuracy: 0.8188 - val_loss: 0.4669 - learning_rate: 1.0000e-04\n",
      "Epoch 92/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8158 - loss: 0.4708 - val_accuracy: 0.8187 - val_loss: 0.4661 - learning_rate: 1.0000e-04\n",
      "Epoch 93/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8153 - loss: 0.4740 - val_accuracy: 0.8187 - val_loss: 0.4660 - learning_rate: 1.0000e-04\n",
      "Epoch 94/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8215 - loss: 0.4624 - val_accuracy: 0.8167 - val_loss: 0.4675 - learning_rate: 1.0000e-04\n",
      "Epoch 95/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8120 - loss: 0.4739 - val_accuracy: 0.8174 - val_loss: 0.4659 - learning_rate: 1.0000e-04\n",
      "Epoch 96/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8153 - loss: 0.4711 - val_accuracy: 0.8192 - val_loss: 0.4669 - learning_rate: 1.0000e-04\n",
      "Epoch 97/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8166 - loss: 0.4700 - val_accuracy: 0.8187 - val_loss: 0.4659 - learning_rate: 1.0000e-04\n",
      "Epoch 98/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8180 - loss: 0.4677 - val_accuracy: 0.8180 - val_loss: 0.4665 - learning_rate: 1.0000e-04\n",
      "Epoch 99/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8201 - loss: 0.4617 - val_accuracy: 0.8173 - val_loss: 0.4658 - learning_rate: 1.0000e-04\n",
      "Epoch 100/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8145 - loss: 0.4678 - val_accuracy: 0.8178 - val_loss: 0.4660 - learning_rate: 1.0000e-04\n",
      "Epoch 101/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8259 - loss: 0.4538 - val_accuracy: 0.8184 - val_loss: 0.4666 - learning_rate: 1.0000e-04\n",
      "Epoch 102/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8209 - loss: 0.4606 - val_accuracy: 0.8187 - val_loss: 0.4653 - learning_rate: 1.0000e-04\n",
      "Epoch 103/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8151 - loss: 0.4678 - val_accuracy: 0.8179 - val_loss: 0.4652 - learning_rate: 1.0000e-04\n",
      "Epoch 104/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8169 - loss: 0.4706 - val_accuracy: 0.8187 - val_loss: 0.4651 - learning_rate: 1.0000e-04\n",
      "Epoch 105/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8182 - loss: 0.4634 - val_accuracy: 0.8175 - val_loss: 0.4652 - learning_rate: 1.0000e-04\n",
      "Epoch 106/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8158 - loss: 0.4670 - val_accuracy: 0.8184 - val_loss: 0.4654 - learning_rate: 1.0000e-04\n",
      "Epoch 107/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8175 - loss: 0.4682 - val_accuracy: 0.8191 - val_loss: 0.4650 - learning_rate: 1.0000e-04\n",
      "Epoch 108/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8136 - loss: 0.4698 - val_accuracy: 0.8184 - val_loss: 0.4649 - learning_rate: 1.0000e-04\n",
      "Epoch 109/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8147 - loss: 0.4706 - val_accuracy: 0.8184 - val_loss: 0.4650 - learning_rate: 1.0000e-04\n",
      "Epoch 110/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8161 - loss: 0.4705 - val_accuracy: 0.8187 - val_loss: 0.4648 - learning_rate: 1.0000e-04\n",
      "Epoch 111/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8183 - loss: 0.4633 - val_accuracy: 0.8192 - val_loss: 0.4649 - learning_rate: 1.0000e-04\n",
      "Epoch 112/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8178 - loss: 0.4657 - val_accuracy: 0.8190 - val_loss: 0.4650 - learning_rate: 1.0000e-04\n",
      "Epoch 113/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8191 - loss: 0.4672 - val_accuracy: 0.8178 - val_loss: 0.4646 - learning_rate: 1.0000e-04\n",
      "Epoch 114/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8211 - loss: 0.4626 - val_accuracy: 0.8186 - val_loss: 0.4648 - learning_rate: 1.0000e-04\n",
      "Epoch 115/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8144 - loss: 0.4698 - val_accuracy: 0.8193 - val_loss: 0.4644 - learning_rate: 1.0000e-04\n",
      "Epoch 116/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8186 - loss: 0.4654 - val_accuracy: 0.8175 - val_loss: 0.4650 - learning_rate: 1.0000e-04\n",
      "Epoch 117/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8175 - loss: 0.4682 - val_accuracy: 0.8188 - val_loss: 0.4645 - learning_rate: 1.0000e-04\n",
      "Epoch 118/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8214 - loss: 0.4617 - val_accuracy: 0.8187 - val_loss: 0.4643 - learning_rate: 1.0000e-04\n",
      "Epoch 119/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8171 - loss: 0.4639 - val_accuracy: 0.8197 - val_loss: 0.4645 - learning_rate: 1.0000e-04\n",
      "Epoch 120/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8166 - loss: 0.4648 - val_accuracy: 0.8186 - val_loss: 0.4641 - learning_rate: 1.0000e-04\n",
      "Epoch 121/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8157 - loss: 0.4671 - val_accuracy: 0.8187 - val_loss: 0.4642 - learning_rate: 1.0000e-04\n",
      "Epoch 122/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8223 - loss: 0.4618 - val_accuracy: 0.8178 - val_loss: 0.4646 - learning_rate: 1.0000e-04\n",
      "Epoch 123/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8206 - loss: 0.4610 - val_accuracy: 0.8178 - val_loss: 0.4643 - learning_rate: 1.0000e-04\n",
      "Epoch 124/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8168 - loss: 0.4666 - val_accuracy: 0.8178 - val_loss: 0.4640 - learning_rate: 1.0000e-04\n",
      "Epoch 125/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8202 - loss: 0.4652 - val_accuracy: 0.8175 - val_loss: 0.4652 - learning_rate: 1.0000e-04\n",
      "Epoch 126/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8186 - loss: 0.4674 - val_accuracy: 0.8188 - val_loss: 0.4639 - learning_rate: 1.0000e-04\n",
      "Epoch 127/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8165 - loss: 0.4671 - val_accuracy: 0.8180 - val_loss: 0.4641 - learning_rate: 1.0000e-04\n",
      "Epoch 128/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8165 - loss: 0.4648 - val_accuracy: 0.8183 - val_loss: 0.4643 - learning_rate: 1.0000e-04\n",
      "Epoch 129/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8172 - loss: 0.4658 - val_accuracy: 0.8184 - val_loss: 0.4638 - learning_rate: 1.0000e-04\n",
      "Epoch 130/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8213 - loss: 0.4622 - val_accuracy: 0.8178 - val_loss: 0.4642 - learning_rate: 1.0000e-04\n",
      "Epoch 131/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8146 - loss: 0.4698 - val_accuracy: 0.8178 - val_loss: 0.4668 - learning_rate: 1.0000e-04\n",
      "Epoch 132/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8212 - loss: 0.4630 - val_accuracy: 0.8180 - val_loss: 0.4650 - learning_rate: 1.0000e-04\n",
      "Epoch 133/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8180 - loss: 0.4654 - val_accuracy: 0.8179 - val_loss: 0.4635 - learning_rate: 1.0000e-04\n",
      "Epoch 134/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8179 - loss: 0.4646 - val_accuracy: 0.8171 - val_loss: 0.4650 - learning_rate: 1.0000e-04\n",
      "Epoch 135/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8171 - loss: 0.4686 - val_accuracy: 0.8184 - val_loss: 0.4636 - learning_rate: 1.0000e-04\n",
      "Epoch 136/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8175 - loss: 0.4642 - val_accuracy: 0.8180 - val_loss: 0.4641 - learning_rate: 1.0000e-04\n",
      "Epoch 137/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8203 - loss: 0.4627 - val_accuracy: 0.8186 - val_loss: 0.4633 - learning_rate: 1.0000e-04\n",
      "Epoch 138/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8174 - loss: 0.4686 - val_accuracy: 0.8201 - val_loss: 0.4637 - learning_rate: 1.0000e-04\n",
      "Epoch 139/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8131 - loss: 0.4691 - val_accuracy: 0.8180 - val_loss: 0.4635 - learning_rate: 1.0000e-04\n",
      "Epoch 140/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8149 - loss: 0.4676 - val_accuracy: 0.8186 - val_loss: 0.4631 - learning_rate: 1.0000e-04\n",
      "Epoch 141/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8192 - loss: 0.4637 - val_accuracy: 0.8186 - val_loss: 0.4632 - learning_rate: 1.0000e-04\n",
      "Epoch 142/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8167 - loss: 0.4657 - val_accuracy: 0.8187 - val_loss: 0.4635 - learning_rate: 1.0000e-04\n",
      "Epoch 143/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8129 - loss: 0.4697 - val_accuracy: 0.8182 - val_loss: 0.4629 - learning_rate: 1.0000e-04\n",
      "Epoch 144/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8199 - loss: 0.4620 - val_accuracy: 0.8187 - val_loss: 0.4630 - learning_rate: 1.0000e-04\n",
      "Epoch 145/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8180 - loss: 0.4649 - val_accuracy: 0.8176 - val_loss: 0.4632 - learning_rate: 1.0000e-04\n",
      "Epoch 146/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8152 - loss: 0.4684 - val_accuracy: 0.8184 - val_loss: 0.4628 - learning_rate: 1.0000e-04\n",
      "Epoch 147/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8182 - loss: 0.4660 - val_accuracy: 0.8193 - val_loss: 0.4636 - learning_rate: 1.0000e-04\n",
      "Epoch 148/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 2ms/step - accuracy: 0.8156 - loss: 0.4686 - val_accuracy: 0.8180 - val_loss: 0.4632 - learning_rate: 1.0000e-04\n",
      "Epoch 149/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8206 - loss: 0.4615 - val_accuracy: 0.8182 - val_loss: 0.4640 - learning_rate: 1.0000e-04\n",
      "Epoch 150/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8204 - loss: 0.4610 - val_accuracy: 0.8179 - val_loss: 0.4632 - learning_rate: 1.0000e-04\n",
      "Epoch 151/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8225 - loss: 0.4594 - val_accuracy: 0.8178 - val_loss: 0.4628 - learning_rate: 1.0000e-04\n",
      "Epoch 152/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8200 - loss: 0.4587 - val_accuracy: 0.8192 - val_loss: 0.4627 - learning_rate: 1.0000e-04\n",
      "Epoch 153/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8187 - loss: 0.4636 - val_accuracy: 0.8186 - val_loss: 0.4625 - learning_rate: 1.0000e-04\n",
      "Epoch 154/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8255 - loss: 0.4536 - val_accuracy: 0.8191 - val_loss: 0.4639 - learning_rate: 1.0000e-04\n",
      "Epoch 155/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8192 - loss: 0.4603 - val_accuracy: 0.8180 - val_loss: 0.4626 - learning_rate: 1.0000e-04\n",
      "Epoch 156/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8120 - loss: 0.4721 - val_accuracy: 0.8166 - val_loss: 0.4631 - learning_rate: 1.0000e-04\n",
      "Epoch 157/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8195 - loss: 0.4580 - val_accuracy: 0.8191 - val_loss: 0.4632 - learning_rate: 1.0000e-04\n",
      "Epoch 158/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8164 - loss: 0.4672 - val_accuracy: 0.8187 - val_loss: 0.4628 - learning_rate: 1.0000e-04\n",
      "Epoch 159/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 2ms/step - accuracy: 0.8170 - loss: 0.4647 - val_accuracy: 0.8183 - val_loss: 0.4625 - learning_rate: 1.0000e-04\n",
      "Epoch 160/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8209 - loss: 0.4594 - val_accuracy: 0.8183 - val_loss: 0.4629 - learning_rate: 1.0000e-04\n",
      "Epoch 161/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8128 - loss: 0.4697 - val_accuracy: 0.8190 - val_loss: 0.4626 - learning_rate: 1.0000e-04\n",
      "Epoch 162/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8197 - loss: 0.4617 - val_accuracy: 0.8180 - val_loss: 0.4624 - learning_rate: 1.0000e-04\n",
      "Epoch 163/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8168 - loss: 0.4661 - val_accuracy: 0.8178 - val_loss: 0.4624 - learning_rate: 1.0000e-04\n",
      "Epoch 164/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8131 - loss: 0.4711 - val_accuracy: 0.8199 - val_loss: 0.4624 - learning_rate: 1.0000e-04\n",
      "Epoch 165/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8131 - loss: 0.4685 - val_accuracy: 0.8192 - val_loss: 0.4627 - learning_rate: 1.0000e-04\n",
      "Epoch 166/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8217 - loss: 0.4591 - val_accuracy: 0.8173 - val_loss: 0.4640 - learning_rate: 1.0000e-04\n",
      "Epoch 167/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8177 - loss: 0.4628 - val_accuracy: 0.8183 - val_loss: 0.4622 - learning_rate: 1.0000e-04\n",
      "Epoch 168/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8184 - loss: 0.4634 - val_accuracy: 0.8188 - val_loss: 0.4620 - learning_rate: 1.0000e-04\n",
      "Epoch 169/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8195 - loss: 0.4592 - val_accuracy: 0.8191 - val_loss: 0.4634 - learning_rate: 1.0000e-04\n",
      "Epoch 170/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8186 - loss: 0.4631 - val_accuracy: 0.8179 - val_loss: 0.4620 - learning_rate: 1.0000e-04\n",
      "Epoch 171/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8095 - loss: 0.4748 - val_accuracy: 0.8184 - val_loss: 0.4620 - learning_rate: 1.0000e-04\n",
      "Epoch 172/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8135 - loss: 0.4692 - val_accuracy: 0.8183 - val_loss: 0.4619 - learning_rate: 1.0000e-04\n",
      "Epoch 173/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8182 - loss: 0.4640 - val_accuracy: 0.8186 - val_loss: 0.4625 - learning_rate: 1.0000e-04\n",
      "Epoch 174/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8200 - loss: 0.4609 - val_accuracy: 0.8184 - val_loss: 0.4620 - learning_rate: 1.0000e-04\n",
      "Epoch 175/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8220 - loss: 0.4559 - val_accuracy: 0.8195 - val_loss: 0.4624 - learning_rate: 1.0000e-04\n",
      "Epoch 176/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8189 - loss: 0.4604 - val_accuracy: 0.8188 - val_loss: 0.4623 - learning_rate: 1.0000e-04\n",
      "Epoch 177/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8213 - loss: 0.4558 - val_accuracy: 0.8180 - val_loss: 0.4623 - learning_rate: 1.0000e-04\n",
      "Epoch 178/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8217 - loss: 0.4577 - val_accuracy: 0.8187 - val_loss: 0.4620 - learning_rate: 1.0000e-04\n",
      "Epoch 179/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8137 - loss: 0.4685 - val_accuracy: 0.8188 - val_loss: 0.4618 - learning_rate: 1.0000e-04\n",
      "Epoch 180/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8160 - loss: 0.4610 - val_accuracy: 0.8183 - val_loss: 0.4617 - learning_rate: 1.0000e-04\n",
      "Epoch 181/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8155 - loss: 0.4649 - val_accuracy: 0.8191 - val_loss: 0.4621 - learning_rate: 1.0000e-04\n",
      "Epoch 182/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8158 - loss: 0.4610 - val_accuracy: 0.8187 - val_loss: 0.4618 - learning_rate: 1.0000e-04\n",
      "Epoch 183/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8118 - loss: 0.4690 - val_accuracy: 0.8184 - val_loss: 0.4617 - learning_rate: 1.0000e-04\n",
      "Epoch 184/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8150 - loss: 0.4664 - val_accuracy: 0.8182 - val_loss: 0.4624 - learning_rate: 1.0000e-04\n",
      "Epoch 185/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8154 - loss: 0.4635 - val_accuracy: 0.8188 - val_loss: 0.4619 - learning_rate: 1.0000e-04\n",
      "Epoch 186/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8180 - loss: 0.4678 - val_accuracy: 0.8175 - val_loss: 0.4621 - learning_rate: 1.0000e-04\n",
      "Epoch 187/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8144 - loss: 0.4697 - val_accuracy: 0.8179 - val_loss: 0.4616 - learning_rate: 1.0000e-04\n",
      "Epoch 188/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8201 - loss: 0.4598 - val_accuracy: 0.8184 - val_loss: 0.4615 - learning_rate: 1.0000e-04\n",
      "Epoch 189/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8140 - loss: 0.4641 - val_accuracy: 0.8184 - val_loss: 0.4615 - learning_rate: 1.0000e-04\n",
      "Epoch 190/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 2ms/step - accuracy: 0.8189 - loss: 0.4626 - val_accuracy: 0.8191 - val_loss: 0.4624 - learning_rate: 1.0000e-04\n",
      "Epoch 191/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8170 - loss: 0.4643 - val_accuracy: 0.8182 - val_loss: 0.4618 - learning_rate: 1.0000e-04\n",
      "Epoch 192/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 2ms/step - accuracy: 0.8200 - loss: 0.4581 - val_accuracy: 0.8195 - val_loss: 0.4615 - learning_rate: 1.0000e-04\n",
      "Epoch 193/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8181 - loss: 0.4620 - val_accuracy: 0.8195 - val_loss: 0.4613 - learning_rate: 1.0000e-04\n",
      "Epoch 194/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 2ms/step - accuracy: 0.8176 - loss: 0.4611 - val_accuracy: 0.8184 - val_loss: 0.4615 - learning_rate: 1.0000e-04\n",
      "Epoch 195/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8183 - loss: 0.4619 - val_accuracy: 0.8182 - val_loss: 0.4612 - learning_rate: 1.0000e-04\n",
      "Epoch 196/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8194 - loss: 0.4597 - val_accuracy: 0.8184 - val_loss: 0.4613 - learning_rate: 1.0000e-04\n",
      "Epoch 197/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 2ms/step - accuracy: 0.8116 - loss: 0.4689 - val_accuracy: 0.8182 - val_loss: 0.4610 - learning_rate: 1.0000e-04\n",
      "Epoch 198/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8248 - loss: 0.4515 - val_accuracy: 0.8191 - val_loss: 0.4622 - learning_rate: 1.0000e-04\n",
      "Epoch 199/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8166 - loss: 0.4658 - val_accuracy: 0.8186 - val_loss: 0.4618 - learning_rate: 1.0000e-04\n",
      "Epoch 200/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 2ms/step - accuracy: 0.8168 - loss: 0.4624 - val_accuracy: 0.8179 - val_loss: 0.4612 - learning_rate: 1.0000e-04\n",
      "\n",
      "\n",
      "RESULTS:\n",
      "Validation Loss:     0.4610\n",
      "Validation Accuracy: 0.8182\n",
      "F1 Score:            0.4689\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8w7UIdtIWuCD"
   },
   "source": [
    "## Analyze and comment the training results\n",
    "\n",
    "here goes any comment/visualization of the training history and any initial consideration on the training results  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2jiOZzvyJbsN"
   },
   "source": [
    "## Validate the model and comment the results\n",
    "\n",
    "please describe the evaluation procedure on a validation set, commenting the generalization capability of your model (e.g. under/overfitting). You may also describe the performance metrics that you choose: what is the most suitable performance measure (or set of performance measures) in this case/dataset, according to you? Why?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "sgGlAIaEJbsO",
    "ExecuteTime": {
     "end_time": "2024-10-24T08:45:44.658898Z",
     "start_time": "2024-10-24T08:45:44.290902Z"
    }
   },
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "\n",
    "# Step 1: Generate predictions on the validation set\n",
    "y_val_pred_prob = model.predict(X_val_scaled)\n",
    "y_val_pred = (y_val_pred_prob > 0.5).astype(int)  # Threshold of 0.5 for binary classification\n",
    "\n",
    "# Step 2: Confusion Matrix\n",
    "confusion = confusion_matrix(y_val, y_val_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion)\n",
    "\n",
    "# Step 3: Classification Report\n",
    "class_report = classification_report(y_val, y_val_pred, target_names=[\"No Default\", \"Default\"])\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n",
    "\n",
    "# Step 4: Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_val, y_val_pred_prob)\n",
    "print(f\"ROC AUC Score: {roc_auc:.4f}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m240/240\u001B[0m \u001B[32m\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 789us/step\n",
      "Confusion Matrix:\n",
      "[[5530  415]\n",
      " [1040  665]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  No Default       0.84      0.93      0.88      5945\n",
      "     Default       0.62      0.39      0.48      1705\n",
      "\n",
      "    accuracy                           0.81      7650\n",
      "   macro avg       0.73      0.66      0.68      7650\n",
      "weighted avg       0.79      0.81      0.79      7650\n",
      "\n",
      "ROC AUC Score: 0.7469\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5MhCwXroWmf9"
   },
   "source": [
    "## Make predictions (on the provided test set)\n",
    "\n",
    "Based on the results obtained and analyzed during the training and the validation phases, what are your (rather _personal_) expectations with respect to the performances of your model on the blind external test set? Briefly motivate your answer."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fbtA2vJRWpMY",
    "ExecuteTime": {
     "end_time": "2024-10-24T08:45:44.686122Z",
     "start_time": "2024-10-24T08:45:44.660219Z"
    }
   },
   "source": [
    "# Load the test data\n",
    "X_test, test_ids = load_data(url_test, train=False)\n",
    "\n",
    "# Preprocess the test data\n",
    "X_test_scaled, _ = preprocess_data(X_test)\n",
    "\n",
    "# Step 1: Make predictions on the test set\n",
    "y_test_pred_prob = model.predict(X_test_scaled)\n",
    "y_test_pred = (y_test_pred_prob > 0.5).astype(int)  # Threshold of 0.5 for binary classification\n",
    "\n",
    "# Step 2: Create a DataFrame for submission or further analysis\n",
    "results = pd.DataFrame({\n",
    "    'Default_Prediction': y_test_pred.flatten()\n",
    "})\n",
    "\n",
    "# Step 3: Save predictions to a CSV file (optional)\n",
    "results.to_csv('test_predictions.csv', index=False, header=False)\n",
    "\n",
    "# Display predictions\n",
    "print(\"Predictions on Test Set:\")\n",
    "print(results.head())\n"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'url_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[58], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Load the test data\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m X_test, test_ids \u001B[38;5;241m=\u001B[39m load_data(url_test, train\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Preprocess the test data\u001B[39;00m\n\u001B[1;32m      5\u001B[0m X_test_scaled, _ \u001B[38;5;241m=\u001B[39m preprocess_data(X_test)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'url_test' is not defined"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9w-sa4AlaBJg"
   },
   "source": [
    "# OPTIONAL -- Export the predictions in the format indicated in the assignment release page and verify you prediction on the [assessment page](https://aml-assignmentone-2425.streamlit.app/)."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T08:45:44.689622Z",
     "start_time": "2024-10-24T08:45:44.689388Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ]
}
