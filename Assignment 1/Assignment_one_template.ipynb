{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "colab": {
   "provenance": []
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FH5ja_uiJbr6"
   },
   "source": [
    "# Predicting Default Payments with Fully-Connected NNs\n",
    "\n",
    "The dataset contains information on default payments, demographic factors, credit data, history of payment, and bill statements of credit card clients in Taiwan from April 2005 to September 2005."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0-FgzT_cJbsH"
   },
   "source": [
    "## Inspecting the data\n",
    "\n",
    "any comment about data dimensionality/distribution goes here"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8vSiz47HXYYM",
    "ExecuteTime": {
     "end_time": "2024-10-24T09:45:58.777749Z",
     "start_time": "2024-10-24T09:45:58.432066Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Suppress TensorFlow logging warnings to reduce console output\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "\n",
    "def load_data(path, train=True):\n",
    "    df = pd.read_csv(path, encoding=\"ISO-8859-2\")\n",
    "    \n",
    "    if train:\n",
    "        data = df.sample(frac=1, random_state=42).values\n",
    "        # Restituisci le feature (colonne da 1 a -1) e i label (ultima colonna)\n",
    "        return data[:, :-1].astype(np.float32), data[:, -1]\n",
    "    \n",
    "    # Nel caso del test, restituisci tutto tranne la prima colonna come feature\n",
    "    return df.iloc[:, 1:].values.astype(np.float32), df.iloc[:, 0].astype(str)\n",
    "\n",
    "# Carica i dati di training e test\n",
    "X_train, labels = load_data('./train.csv')\n",
    "X_test, ids = load_data('./test.csv', train=False)\n",
    "\n",
    "print(f\"Training set dimensions: {X_train.shape}\\nFeatures count: {X_train.shape[1]}\")\n",
    "print(\"\\nFirst 5 training samples:\\n\", pd.DataFrame(X_train).head())\n",
    "print(\"\\nLabels distribution:\\n\", pd.Series(labels).value_counts())\n",
    "print(\"\\nMissing values:\\n\", pd.read_csv('./train.csv').isnull().sum())\n",
    "print(\"\\nSummary statistics:\\n\", pd.read_csv('./train.csv').describe())\n",
    "print(\"\\nTest set dimensions:\", X_test.shape)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set dimensions: (25500, 24)\n",
      "Features count: 24\n",
      "\n",
      "First 5 training samples:\n",
      "         0         1    2    3    4     5    6    7    8    9   ...       14  \\\n",
      "0  21870.0   70000.0  2.0  3.0  2.0  26.0  0.0  0.0  0.0  0.0  ...   8948.0   \n",
      "1  15211.0  320000.0  2.0  2.0  2.0  28.0  0.0  0.0  0.0  0.0  ...    944.0   \n",
      "2  20041.0   30000.0  2.0  2.0  2.0  36.0  0.0  0.0  0.0  0.0  ...  30452.0   \n",
      "3   9190.0   20000.0  2.0  3.0  1.0  35.0  0.0  0.0  2.0  2.0  ...  18621.0   \n",
      "4   6260.0   80000.0  1.0  2.0  2.0  32.0  1.0  2.0  0.0  0.0  ...  28242.0   \n",
      "\n",
      "        15       16       17      18       19      20      21      22      23  \n",
      "0   9006.0  10570.0  11421.0  2000.0   1200.0  1500.0  2000.0  1000.0  2000.0  \n",
      "1    473.0   1747.0   1193.0   390.0    944.0   473.0  5000.0  1200.0   980.0  \n",
      "2  29667.0  28596.0  29180.0   490.0  33299.0  1400.0   572.0   584.0   400.0  \n",
      "3  18024.0  18434.0  19826.0  3000.0   1000.0     0.0   700.0  1700.0     0.0  \n",
      "4  21400.0      0.0      0.0     7.0   1200.0  1408.0     0.0     0.0     0.0  \n",
      "\n",
      "[5 rows x 24 columns]\n",
      "\n",
      "Labels distribution:\n",
      " 0    19815\n",
      "1     5685\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Missing values:\n",
      " ID                            0\n",
      "LIMIT_BAL                     0\n",
      "SEX                           0\n",
      "EDUCATION                     0\n",
      "MARRIAGE                      0\n",
      "AGE                           0\n",
      "PAY_0                         0\n",
      "PAY_2                         0\n",
      "PAY_3                         0\n",
      "PAY_4                         0\n",
      "PAY_5                         0\n",
      "PAY_6                         0\n",
      "BILL_AMT1                     0\n",
      "BILL_AMT2                     0\n",
      "BILL_AMT3                     0\n",
      "BILL_AMT4                     0\n",
      "BILL_AMT5                     0\n",
      "BILL_AMT6                     0\n",
      "PAY_AMT1                      0\n",
      "PAY_AMT2                      0\n",
      "PAY_AMT3                      0\n",
      "PAY_AMT4                      0\n",
      "PAY_AMT5                      0\n",
      "PAY_AMT6                      0\n",
      "default payment next month    0\n",
      "dtype: int64\n",
      "\n",
      "Summary statistics:\n",
      "                 ID       LIMIT_BAL           SEX     EDUCATION      MARRIAGE  \\\n",
      "count  25500.00000    25500.000000  25500.000000  25500.000000  25500.000000   \n",
      "mean   14956.95702   167569.007059      1.604118      1.852353      1.550392   \n",
      "std     8667.36982   130002.156470      0.489049      0.787991      0.522757   \n",
      "min        1.00000    10000.000000      1.000000      0.000000      0.000000   \n",
      "25%     7432.75000    50000.000000      1.000000      1.000000      1.000000   \n",
      "50%    14942.50000   140000.000000      2.000000      2.000000      2.000000   \n",
      "75%    22431.25000   240000.000000      2.000000      2.000000      2.000000   \n",
      "max    30000.00000  1000000.000000      2.000000      6.000000      3.000000   \n",
      "\n",
      "                AGE         PAY_0         PAY_2         PAY_3         PAY_4  \\\n",
      "count  25500.000000  25500.000000  25500.000000  25500.000000  25500.000000   \n",
      "mean      35.509294     -0.013098     -0.130784     -0.163294     -0.218235   \n",
      "std        9.200408      1.126314      1.199481      1.199697      1.169681   \n",
      "min       21.000000     -2.000000     -2.000000     -2.000000     -2.000000   \n",
      "25%       28.000000     -1.000000     -1.000000     -1.000000     -1.000000   \n",
      "50%       34.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "75%       41.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "max       79.000000      8.000000      8.000000      8.000000      8.000000   \n",
      "\n",
      "       ...      BILL_AMT4      BILL_AMT5      BILL_AMT6       PAY_AMT1  \\\n",
      "count  ...   25500.000000   25500.000000   25500.000000   25500.000000   \n",
      "mean   ...   43336.952196   40307.121059   38924.328157    5594.010863   \n",
      "std    ...   64433.082446   60870.691089   59659.509920   16235.253410   \n",
      "min    ... -170000.000000  -81334.000000 -339603.000000       0.000000   \n",
      "25%    ...    2338.750000    1767.250000    1266.750000    1000.000000   \n",
      "50%    ...   19111.000000   18112.500000   17150.000000    2100.000000   \n",
      "75%    ...   54475.000000   50178.250000   49132.500000    5006.000000   \n",
      "max    ...  891586.000000  927171.000000  961664.000000  873552.000000   \n",
      "\n",
      "           PAY_AMT2       PAY_AMT3       PAY_AMT4       PAY_AMT5  \\\n",
      "count  2.550000e+04   25500.000000   25500.000000   25500.000000   \n",
      "mean   5.934389e+03    5319.529647    4812.161373    4812.480431   \n",
      "std    2.381277e+04   18157.653215   15560.524538   15206.108094   \n",
      "min    0.000000e+00       0.000000       0.000000       0.000000   \n",
      "25%    8.270000e+02     396.000000     291.000000     251.000000   \n",
      "50%    2.002000e+03    1800.000000    1500.000000    1500.000000   \n",
      "75%    5.000000e+03    4560.500000    4000.000000    4071.500000   \n",
      "max    1.684259e+06  896040.000000  621000.000000  426529.000000   \n",
      "\n",
      "            PAY_AMT6  default payment next month  \n",
      "count   25500.000000                25500.000000  \n",
      "mean     5236.509176                    0.222941  \n",
      "std     17958.888070                    0.416227  \n",
      "min         0.000000                    0.000000  \n",
      "25%       125.750000                    0.000000  \n",
      "50%      1500.000000                    0.000000  \n",
      "75%      4000.000000                    0.000000  \n",
      "max    528666.000000                    1.000000  \n",
      "\n",
      "[8 rows x 25 columns]\n",
      "\n",
      "Test set dimensions: (4500, 23)\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PjWrQr5vWTTG"
   },
   "source": [
    "## Preparing the data\n",
    "\n",
    "describe the choice made during the preprocessing operations, also taking into account the previous considerations during the data inspection."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "J84aUJVUJbsI",
    "ExecuteTime": {
     "end_time": "2024-10-24T09:46:00.914375Z",
     "start_time": "2024-10-24T09:46:00.532991Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def preprocess_data(X, scaler=None):\n",
    "    if scaler is None:\n",
    "        scaler = StandardScaler().fit(X)\n",
    "    return scaler.transform(X), scaler\n",
    "\n",
    "# Carica i dati di training e test\n",
    "df_train = pd.read_csv('./train.csv')\n",
    "\n",
    "# Separare le etichette\n",
    "labels_raw = df_train['default payment next month']\n",
    "\n",
    "# Colonne da eliminare (inclusa la colonna ID)\n",
    "columns_to_drop = [\"ID\"]\n",
    "\n",
    "# Creare il DataFrame delle features\n",
    "X_train_raw = df_train.drop(columns=columns_to_drop + ['default payment next month'])\n",
    "\n",
    "# Controlla valori mancanti\n",
    "print(\"Missing values:\\n\", df_train.isnull().sum())\n",
    "\n",
    "# Visualizza la varianza delle feature\n",
    "plt.figure(figsize=(10, 6))\n",
    "X_train_raw.var().plot(kind='bar')\n",
    "plt.title('Variance of Each Feature')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Variance')\n",
    "plt.xticks(rotation=90)\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Divisione in set di training e validazione\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_raw, labels_raw, \n",
    "                                                  test_size=0.3, random_state=42, stratify=labels_raw)\n",
    "\n",
    "# Ridimensionamento delle feature\n",
    "X_train_scaled, scaler = preprocess_data(X_train)\n",
    "X_val_scaled, _ = preprocess_data(X_val, scaler)\n",
    "\n",
    "# Mostra la dimensione dei set\n",
    "print(f\"Shapes -> Train: {X_train_scaled.shape}, Val: {X_val_scaled.shape}, \"\n",
    "      f\"Encoded Train: {y_train.shape}, Encoded Val: {y_val.shape}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values:\n",
      " ID                            0\n",
      "LIMIT_BAL                     0\n",
      "SEX                           0\n",
      "EDUCATION                     0\n",
      "MARRIAGE                      0\n",
      "AGE                           0\n",
      "PAY_0                         0\n",
      "PAY_2                         0\n",
      "PAY_3                         0\n",
      "PAY_4                         0\n",
      "PAY_5                         0\n",
      "PAY_6                         0\n",
      "BILL_AMT1                     0\n",
      "BILL_AMT2                     0\n",
      "BILL_AMT3                     0\n",
      "BILL_AMT4                     0\n",
      "BILL_AMT5                     0\n",
      "BILL_AMT6                     0\n",
      "PAY_AMT1                      0\n",
      "PAY_AMT2                      0\n",
      "PAY_AMT3                      0\n",
      "PAY_AMT4                      0\n",
      "PAY_AMT5                      0\n",
      "PAY_AMT6                      0\n",
      "default payment next month    0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes -> Train: (17850, 23), Val: (7650, 23), Encoded Train: (17850,), Encoded Val: (7650,)\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eb9aljYxJbsK"
   },
   "source": [
    "## Building the network\n",
    "\n",
    "any description/comment about the procedure you followed in the choice of the network structure and hyperparameters goes here, together with consideration about the training/optimization procedure (e.g. optimizer choice, final activations, loss functions, training metrics)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "AS3zsRlBYHKW",
    "ExecuteTime": {
     "end_time": "2024-10-24T09:48:29.321839Z",
     "start_time": "2024-10-24T09:46:19.801986Z"
    }
   },
   "source": [
    "from keras.src.utils import plot_model\n",
    "from keras.src.optimizers import Adam\n",
    "from keras.src.layers import Dense, BatchNormalization, Dropout\n",
    "from keras import Sequential, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, Callback\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Definizione del modello\n",
    "def build_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(input_shape,)))\n",
    "    model.add(Dense(256, activation='relu', kernel_regularizer='l2'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(128, activation='relu', kernel_regularizer='l2'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(64, activation='relu', kernel_regularizer='l2'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(32, activation='relu', kernel_regularizer='l2'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Uscita per classificazione binaria\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001), \n",
    "                  loss='binary_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "input_shape = X_train_scaled.shape[1]  \n",
    "model = build_model(input_shape)\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 150\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Definizione delle callback per l'early stopping e la riduzione del learning rate\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-6)\n",
    "\n",
    "# Callback personalizzata per visualizzare i progressi durante l'addestramento\n",
    "class ProgressBar(Callback):\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.epoch = epoch\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        metrics = (f\"Epoch {self.epoch + 1}/{self.params['epochs']} - \"\n",
    "                   f\"Loss: {logs['loss']:.4f} - \"\n",
    "                   f\"Val Loss: {logs['val_loss']:.4f} - \"\n",
    "                   f\"Accuracy: {logs['accuracy']:.4f} - \"\n",
    "                   f\"Val Accuracy: {logs['val_accuracy']:.4f}\")\n",
    "        print(\"\\r\" + metrics, end='')\n",
    "\n",
    "# Addestramento del modello\n",
    "history = model.fit(X_train_scaled, y_train, \n",
    "                    validation_data=(X_val_scaled, y_val),\n",
    "                    epochs=epochs, batch_size=batch_size, \n",
    "                    callbacks=[early_stopping, reduce_lr, ProgressBar()],\n",
    "                    verbose=1)\n",
    "\n",
    "# Valutazione del modello sul set di validazione\n",
    "val_loss, val_accuracy = model.evaluate(X_val_scaled, y_val, verbose=0)\n",
    "\n",
    "print()\n",
    "print(\"\\nRESULTS:\")\n",
    "print(f\"{'Validation Loss:':<20} {val_loss:.4f}\")\n",
    "print(f\"{'Validation Accuracy:':<20} {val_accuracy:.4f}\")\n",
    "\n",
    "# Previsione e calcolo dell'F1 Score\n",
    "y_pred_prob = model.predict(X_val_scaled, verbose=0) \n",
    "y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "f1 = f1_score(y_val, y_pred)\n",
    "print(f\"{'F1 Score:':<20} {f1:.4f}\")\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"sequential_4\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_20 (\u001B[38;5;33mDense\u001B[0m)                │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m256\u001B[0m)            │         \u001B[38;5;34m6,144\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_16          │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m256\u001B[0m)            │         \u001B[38;5;34m1,024\u001B[0m │\n",
       "│ (\u001B[38;5;33mBatchNormalization\u001B[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_12 (\u001B[38;5;33mDropout\u001B[0m)            │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m256\u001B[0m)            │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_21 (\u001B[38;5;33mDense\u001B[0m)                │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m128\u001B[0m)            │        \u001B[38;5;34m32,896\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_17          │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m128\u001B[0m)            │           \u001B[38;5;34m512\u001B[0m │\n",
       "│ (\u001B[38;5;33mBatchNormalization\u001B[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_13 (\u001B[38;5;33mDropout\u001B[0m)            │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m128\u001B[0m)            │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_22 (\u001B[38;5;33mDense\u001B[0m)                │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m64\u001B[0m)             │         \u001B[38;5;34m8,256\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_18          │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m64\u001B[0m)             │           \u001B[38;5;34m256\u001B[0m │\n",
       "│ (\u001B[38;5;33mBatchNormalization\u001B[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_14 (\u001B[38;5;33mDropout\u001B[0m)            │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m64\u001B[0m)             │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_23 (\u001B[38;5;33mDense\u001B[0m)                │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m32\u001B[0m)             │         \u001B[38;5;34m2,080\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_19          │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m32\u001B[0m)             │           \u001B[38;5;34m128\u001B[0m │\n",
       "│ (\u001B[38;5;33mBatchNormalization\u001B[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_24 (\u001B[38;5;33mDense\u001B[0m)                │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m1\u001B[0m)              │            \u001B[38;5;34m33\u001B[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,144</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_16          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_17          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_18          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_19          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m51,329\u001B[0m (200.50 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">51,329</span> (200.50 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m50,369\u001B[0m (196.75 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">50,369</span> (196.75 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m960\u001B[0m (3.75 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">960</span> (3.75 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 6ms/step - accuracy: 0.5420 - loss: 4.1530 - val_accuracy: 0.7817 - val_loss: 3.7830 - learning_rate: 1.0000e-04\n",
      "Epoch 2/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 5ms/step - accuracy: 0.6276 - loss: 3.8259 - val_accuracy: 0.8047 - val_loss: 3.5196 - learning_rate: 1.0000e-04\n",
      "Epoch 3/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 5ms/step - accuracy: 0.6771 - loss: 3.5654 - val_accuracy: 0.8075 - val_loss: 3.2989 - learning_rate: 1.0000e-04\n",
      "Epoch 4/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 7ms/step - accuracy: 0.7159 - loss: 3.3230 - val_accuracy: 0.8105 - val_loss: 3.0897 - learning_rate: 1.0000e-04\n",
      "Epoch 5/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 7ms/step - accuracy: 0.7400 - loss: 3.1107 - val_accuracy: 0.8144 - val_loss: 2.8870 - learning_rate: 1.0000e-04\n",
      "Epoch 6/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.7545 - loss: 2.9056 - val_accuracy: 0.8149 - val_loss: 2.6960 - learning_rate: 1.0000e-04\n",
      "Epoch 7/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 5ms/step - accuracy: 0.7717 - loss: 2.7061 - val_accuracy: 0.8167 - val_loss: 2.5184 - learning_rate: 1.0000e-04\n",
      "Epoch 8/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 5ms/step - accuracy: 0.7825 - loss: 2.5301 - val_accuracy: 0.8166 - val_loss: 2.3534 - learning_rate: 1.0000e-04\n",
      "Epoch 9/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 4ms/step - accuracy: 0.7868 - loss: 2.3684 - val_accuracy: 0.8158 - val_loss: 2.2032 - learning_rate: 1.0000e-04\n",
      "Epoch 10/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 5ms/step - accuracy: 0.7910 - loss: 2.2124 - val_accuracy: 0.8174 - val_loss: 2.0608 - learning_rate: 1.0000e-04\n",
      "Epoch 11/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 5ms/step - accuracy: 0.7993 - loss: 2.0712 - val_accuracy: 0.8165 - val_loss: 1.9313 - learning_rate: 1.0000e-04\n",
      "Epoch 12/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 5ms/step - accuracy: 0.7991 - loss: 1.9388 - val_accuracy: 0.8157 - val_loss: 1.8114 - learning_rate: 1.0000e-04\n",
      "Epoch 13/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 5ms/step - accuracy: 0.8049 - loss: 1.8182 - val_accuracy: 0.8145 - val_loss: 1.7025 - learning_rate: 1.0000e-04\n",
      "Epoch 14/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.8003 - loss: 1.7158 - val_accuracy: 0.8161 - val_loss: 1.6020 - learning_rate: 1.0000e-04\n",
      "Epoch 15/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 5ms/step - accuracy: 0.8097 - loss: 1.6035 - val_accuracy: 0.8161 - val_loss: 1.5116 - learning_rate: 1.0000e-04\n",
      "Epoch 16/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.8014 - loss: 1.5212 - val_accuracy: 0.8166 - val_loss: 1.4273 - learning_rate: 1.0000e-04\n",
      "Epoch 17/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.8124 - loss: 1.4230 - val_accuracy: 0.8169 - val_loss: 1.3498 - learning_rate: 1.0000e-04\n",
      "Epoch 18/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 5ms/step - accuracy: 0.8033 - loss: 1.3552 - val_accuracy: 0.8167 - val_loss: 1.2795 - learning_rate: 1.0000e-04\n",
      "Epoch 19/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.8061 - loss: 1.2832 - val_accuracy: 0.8169 - val_loss: 1.2137 - learning_rate: 1.0000e-04\n",
      "Epoch 20/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 5ms/step - accuracy: 0.8037 - loss: 1.2173 - val_accuracy: 0.8152 - val_loss: 1.1542 - learning_rate: 1.0000e-04\n",
      "Epoch 21/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 5ms/step - accuracy: 0.8078 - loss: 1.1508 - val_accuracy: 0.8119 - val_loss: 1.1005 - learning_rate: 1.0000e-04\n",
      "Epoch 22/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 5ms/step - accuracy: 0.8088 - loss: 1.1010 - val_accuracy: 0.8157 - val_loss: 1.0488 - learning_rate: 1.0000e-04\n",
      "Epoch 23/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.8018 - loss: 1.0543 - val_accuracy: 0.8161 - val_loss: 1.0025 - learning_rate: 1.0000e-04\n",
      "Epoch 24/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 5ms/step - accuracy: 0.8139 - loss: 0.9944 - val_accuracy: 0.8132 - val_loss: 0.9604 - learning_rate: 1.0000e-04\n",
      "Epoch 25/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 5ms/step - accuracy: 0.8096 - loss: 0.9602 - val_accuracy: 0.8170 - val_loss: 0.9214 - learning_rate: 1.0000e-04\n",
      "Epoch 26/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 5ms/step - accuracy: 0.8149 - loss: 0.9144 - val_accuracy: 0.8171 - val_loss: 0.8844 - learning_rate: 1.0000e-04\n",
      "Epoch 27/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 7ms/step - accuracy: 0.8043 - loss: 0.8947 - val_accuracy: 0.8182 - val_loss: 0.8511 - learning_rate: 1.0000e-04\n",
      "Epoch 28/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 8ms/step - accuracy: 0.8095 - loss: 0.8514 - val_accuracy: 0.8187 - val_loss: 0.8201 - learning_rate: 1.0000e-04\n",
      "Epoch 29/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 7ms/step - accuracy: 0.8122 - loss: 0.8208 - val_accuracy: 0.8180 - val_loss: 0.7936 - learning_rate: 1.0000e-04\n",
      "Epoch 30/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 7ms/step - accuracy: 0.8085 - loss: 0.7998 - val_accuracy: 0.8180 - val_loss: 0.7669 - learning_rate: 1.0000e-04\n",
      "Epoch 31/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.8066 - loss: 0.7734 - val_accuracy: 0.8184 - val_loss: 0.7423 - learning_rate: 1.0000e-04\n",
      "Epoch 32/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.8144 - loss: 0.7433 - val_accuracy: 0.8180 - val_loss: 0.7195 - learning_rate: 1.0000e-04\n",
      "Epoch 33/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.8140 - loss: 0.7208 - val_accuracy: 0.8187 - val_loss: 0.7002 - learning_rate: 1.0000e-04\n",
      "Epoch 34/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 8ms/step - accuracy: 0.8125 - loss: 0.7028 - val_accuracy: 0.8178 - val_loss: 0.6825 - learning_rate: 1.0000e-04\n",
      "Epoch 35/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 8ms/step - accuracy: 0.8120 - loss: 0.6835 - val_accuracy: 0.8178 - val_loss: 0.6659 - learning_rate: 1.0000e-04\n",
      "Epoch 36/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 7ms/step - accuracy: 0.8133 - loss: 0.6664 - val_accuracy: 0.8186 - val_loss: 0.6501 - learning_rate: 1.0000e-04\n",
      "Epoch 37/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 7ms/step - accuracy: 0.8081 - loss: 0.6605 - val_accuracy: 0.8196 - val_loss: 0.6348 - learning_rate: 1.0000e-04\n",
      "Epoch 38/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.8134 - loss: 0.6375 - val_accuracy: 0.8191 - val_loss: 0.6210 - learning_rate: 1.0000e-04\n",
      "Epoch 39/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.8107 - loss: 0.6350 - val_accuracy: 0.8195 - val_loss: 0.6086 - learning_rate: 1.0000e-04\n",
      "Epoch 40/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.8101 - loss: 0.6179 - val_accuracy: 0.8186 - val_loss: 0.5975 - learning_rate: 1.0000e-04\n",
      "Epoch 41/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 9ms/step - accuracy: 0.8136 - loss: 0.6032 - val_accuracy: 0.8196 - val_loss: 0.5870 - learning_rate: 1.0000e-04\n",
      "Epoch 42/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 7ms/step - accuracy: 0.8212 - loss: 0.5807 - val_accuracy: 0.8191 - val_loss: 0.5782 - learning_rate: 1.0000e-04\n",
      "Epoch 43/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.8138 - loss: 0.5802 - val_accuracy: 0.8186 - val_loss: 0.5698 - learning_rate: 1.0000e-04\n",
      "Epoch 44/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.8111 - loss: 0.5759 - val_accuracy: 0.8180 - val_loss: 0.5616 - learning_rate: 1.0000e-04\n",
      "Epoch 45/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 8ms/step - accuracy: 0.8189 - loss: 0.5596 - val_accuracy: 0.8187 - val_loss: 0.5533 - learning_rate: 1.0000e-04\n",
      "Epoch 46/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 7ms/step - accuracy: 0.8163 - loss: 0.5556 - val_accuracy: 0.8197 - val_loss: 0.5471 - learning_rate: 1.0000e-04\n",
      "Epoch 47/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.8109 - loss: 0.5571 - val_accuracy: 0.8193 - val_loss: 0.5404 - learning_rate: 1.0000e-04\n",
      "Epoch 48/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 8ms/step - accuracy: 0.8145 - loss: 0.5393 - val_accuracy: 0.8183 - val_loss: 0.5345 - learning_rate: 1.0000e-04\n",
      "Epoch 49/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.8155 - loss: 0.5383 - val_accuracy: 0.8186 - val_loss: 0.5289 - learning_rate: 1.0000e-04\n",
      "Epoch 50/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 8ms/step - accuracy: 0.8219 - loss: 0.5235 - val_accuracy: 0.8183 - val_loss: 0.5246 - learning_rate: 1.0000e-04\n",
      "Epoch 51/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 7ms/step - accuracy: 0.8126 - loss: 0.5276 - val_accuracy: 0.8180 - val_loss: 0.5203 - learning_rate: 1.0000e-04\n",
      "Epoch 52/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 7ms/step - accuracy: 0.8196 - loss: 0.5160 - val_accuracy: 0.8171 - val_loss: 0.5151 - learning_rate: 1.0000e-04\n",
      "Epoch 53/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.8161 - loss: 0.5159 - val_accuracy: 0.8184 - val_loss: 0.5105 - learning_rate: 1.0000e-04\n",
      "Epoch 54/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.8128 - loss: 0.5159 - val_accuracy: 0.8197 - val_loss: 0.5065 - learning_rate: 1.0000e-04\n",
      "Epoch 55/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.8158 - loss: 0.5038 - val_accuracy: 0.8182 - val_loss: 0.5032 - learning_rate: 1.0000e-04\n",
      "Epoch 56/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.8223 - loss: 0.4976 - val_accuracy: 0.8175 - val_loss: 0.4996 - learning_rate: 1.0000e-04\n",
      "Epoch 57/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.8167 - loss: 0.5004 - val_accuracy: 0.8184 - val_loss: 0.4966 - learning_rate: 1.0000e-04\n",
      "Epoch 58/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 5ms/step - accuracy: 0.8159 - loss: 0.5002 - val_accuracy: 0.8199 - val_loss: 0.4940 - learning_rate: 1.0000e-04\n",
      "Epoch 59/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 7ms/step - accuracy: 0.8162 - loss: 0.4952 - val_accuracy: 0.8187 - val_loss: 0.4913 - learning_rate: 1.0000e-04\n",
      "Epoch 60/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 7ms/step - accuracy: 0.8163 - loss: 0.4897 - val_accuracy: 0.8199 - val_loss: 0.4897 - learning_rate: 1.0000e-04\n",
      "Epoch 61/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.8184 - loss: 0.4854 - val_accuracy: 0.8186 - val_loss: 0.4870 - learning_rate: 1.0000e-04\n",
      "Epoch 62/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.8174 - loss: 0.4846 - val_accuracy: 0.8195 - val_loss: 0.4840 - learning_rate: 1.0000e-04\n",
      "Epoch 63/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.8085 - loss: 0.5005 - val_accuracy: 0.8192 - val_loss: 0.4836 - learning_rate: 1.0000e-04\n",
      "Epoch 64/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.8174 - loss: 0.4816 - val_accuracy: 0.8187 - val_loss: 0.4796 - learning_rate: 1.0000e-04\n",
      "Epoch 65/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 7ms/step - accuracy: 0.8167 - loss: 0.4826 - val_accuracy: 0.8176 - val_loss: 0.4781 - learning_rate: 1.0000e-04\n",
      "Epoch 66/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 8ms/step - accuracy: 0.8183 - loss: 0.4813 - val_accuracy: 0.8178 - val_loss: 0.4761 - learning_rate: 1.0000e-04\n",
      "Epoch 67/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 8ms/step - accuracy: 0.8152 - loss: 0.4764 - val_accuracy: 0.8182 - val_loss: 0.4752 - learning_rate: 1.0000e-04\n",
      "Epoch 68/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 7ms/step - accuracy: 0.8198 - loss: 0.4722 - val_accuracy: 0.8175 - val_loss: 0.4744 - learning_rate: 1.0000e-04\n",
      "Epoch 69/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 7ms/step - accuracy: 0.8161 - loss: 0.4727 - val_accuracy: 0.8182 - val_loss: 0.4722 - learning_rate: 1.0000e-04\n",
      "Epoch 70/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.8167 - loss: 0.4685 - val_accuracy: 0.8188 - val_loss: 0.4717 - learning_rate: 1.0000e-04\n",
      "Epoch 71/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 5ms/step - accuracy: 0.8146 - loss: 0.4699 - val_accuracy: 0.8178 - val_loss: 0.4707 - learning_rate: 1.0000e-04\n",
      "Epoch 72/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.8172 - loss: 0.4674 - val_accuracy: 0.8190 - val_loss: 0.4696 - learning_rate: 1.0000e-04\n",
      "Epoch 73/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.8171 - loss: 0.4679 - val_accuracy: 0.8196 - val_loss: 0.4685 - learning_rate: 1.0000e-04\n",
      "Epoch 74/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.8186 - loss: 0.4641 - val_accuracy: 0.8199 - val_loss: 0.4658 - learning_rate: 1.0000e-04\n",
      "Epoch 75/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.8196 - loss: 0.4609 - val_accuracy: 0.8196 - val_loss: 0.4660 - learning_rate: 1.0000e-04\n",
      "Epoch 76/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.8161 - loss: 0.4646 - val_accuracy: 0.8191 - val_loss: 0.4644 - learning_rate: 1.0000e-04\n",
      "Epoch 77/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 5ms/step - accuracy: 0.8145 - loss: 0.4637 - val_accuracy: 0.8186 - val_loss: 0.4646 - learning_rate: 1.0000e-04\n",
      "Epoch 78/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.8205 - loss: 0.4538 - val_accuracy: 0.8203 - val_loss: 0.4634 - learning_rate: 1.0000e-04\n",
      "Epoch 79/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 5ms/step - accuracy: 0.8183 - loss: 0.4622 - val_accuracy: 0.8200 - val_loss: 0.4631 - learning_rate: 1.0000e-04\n",
      "Epoch 80/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 7ms/step - accuracy: 0.8156 - loss: 0.4619 - val_accuracy: 0.8203 - val_loss: 0.4635 - learning_rate: 1.0000e-04\n",
      "Epoch 81/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 7ms/step - accuracy: 0.8217 - loss: 0.4535 - val_accuracy: 0.8200 - val_loss: 0.4626 - learning_rate: 1.0000e-04\n",
      "Epoch 82/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 8ms/step - accuracy: 0.8158 - loss: 0.4610 - val_accuracy: 0.8187 - val_loss: 0.4617 - learning_rate: 1.0000e-04\n",
      "Epoch 83/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 7ms/step - accuracy: 0.8184 - loss: 0.4582 - val_accuracy: 0.8183 - val_loss: 0.4611 - learning_rate: 1.0000e-04\n",
      "Epoch 84/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 7ms/step - accuracy: 0.8187 - loss: 0.4545 - val_accuracy: 0.8197 - val_loss: 0.4598 - learning_rate: 1.0000e-04\n",
      "Epoch 85/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 7ms/step - accuracy: 0.8181 - loss: 0.4544 - val_accuracy: 0.8190 - val_loss: 0.4593 - learning_rate: 1.0000e-04\n",
      "Epoch 86/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 7ms/step - accuracy: 0.8176 - loss: 0.4483 - val_accuracy: 0.8203 - val_loss: 0.4598 - learning_rate: 1.0000e-04\n",
      "Epoch 87/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 7ms/step - accuracy: 0.8238 - loss: 0.4514 - val_accuracy: 0.8204 - val_loss: 0.4588 - learning_rate: 1.0000e-04\n",
      "Epoch 88/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 7ms/step - accuracy: 0.8146 - loss: 0.4570 - val_accuracy: 0.8190 - val_loss: 0.4577 - learning_rate: 1.0000e-04\n",
      "Epoch 89/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 7ms/step - accuracy: 0.8248 - loss: 0.4424 - val_accuracy: 0.8196 - val_loss: 0.4577 - learning_rate: 1.0000e-04\n",
      "Epoch 90/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 7ms/step - accuracy: 0.8199 - loss: 0.4514 - val_accuracy: 0.8196 - val_loss: 0.4587 - learning_rate: 1.0000e-04\n",
      "Epoch 91/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 7ms/step - accuracy: 0.8166 - loss: 0.4547 - val_accuracy: 0.8183 - val_loss: 0.4586 - learning_rate: 1.0000e-04\n",
      "Epoch 92/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 8ms/step - accuracy: 0.8208 - loss: 0.4499 - val_accuracy: 0.8186 - val_loss: 0.4575 - learning_rate: 1.0000e-04\n",
      "Epoch 93/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.8215 - loss: 0.4490 - val_accuracy: 0.8188 - val_loss: 0.4568 - learning_rate: 1.0000e-04\n",
      "Epoch 94/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 7ms/step - accuracy: 0.8167 - loss: 0.4491 - val_accuracy: 0.8196 - val_loss: 0.4566 - learning_rate: 1.0000e-04\n",
      "Epoch 95/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.8207 - loss: 0.4437 - val_accuracy: 0.8187 - val_loss: 0.4576 - learning_rate: 1.0000e-04\n",
      "Epoch 96/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.8171 - loss: 0.4486 - val_accuracy: 0.8183 - val_loss: 0.4566 - learning_rate: 1.0000e-04\n",
      "Epoch 97/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.8206 - loss: 0.4457 - val_accuracy: 0.8184 - val_loss: 0.4578 - learning_rate: 1.0000e-04\n",
      "Epoch 98/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.8233 - loss: 0.4430 - val_accuracy: 0.8175 - val_loss: 0.4587 - learning_rate: 1.0000e-04\n",
      "Epoch 99/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.8165 - loss: 0.4512 - val_accuracy: 0.8178 - val_loss: 0.4551 - learning_rate: 1.0000e-04\n",
      "Epoch 100/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.8208 - loss: 0.4439 - val_accuracy: 0.8174 - val_loss: 0.4549 - learning_rate: 1.0000e-04\n",
      "Epoch 101/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.8185 - loss: 0.4447 - val_accuracy: 0.8171 - val_loss: 0.4559 - learning_rate: 1.0000e-04\n",
      "Epoch 102/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.8167 - loss: 0.4472 - val_accuracy: 0.8187 - val_loss: 0.4536 - learning_rate: 1.0000e-04\n",
      "Epoch 103/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.8231 - loss: 0.4434 - val_accuracy: 0.8201 - val_loss: 0.4554 - learning_rate: 1.0000e-04\n",
      "Epoch 104/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.8248 - loss: 0.4361 - val_accuracy: 0.8188 - val_loss: 0.4545 - learning_rate: 1.0000e-04\n",
      "Epoch 105/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.8177 - loss: 0.4506 - val_accuracy: 0.8195 - val_loss: 0.4543 - learning_rate: 1.0000e-04\n",
      "Epoch 106/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.8216 - loss: 0.4464 - val_accuracy: 0.8188 - val_loss: 0.4539 - learning_rate: 1.0000e-04\n",
      "Epoch 107/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.8192 - loss: 0.4421 - val_accuracy: 0.8210 - val_loss: 0.4539 - learning_rate: 1.0000e-04\n",
      "Epoch 108/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.8191 - loss: 0.4410 - val_accuracy: 0.8195 - val_loss: 0.4561 - learning_rate: 1.0000e-04\n",
      "Epoch 109/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.8200 - loss: 0.4392 - val_accuracy: 0.8186 - val_loss: 0.4553 - learning_rate: 1.0000e-04\n",
      "Epoch 110/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.8213 - loss: 0.4376 - val_accuracy: 0.8199 - val_loss: 0.4551 - learning_rate: 1.0000e-04\n",
      "Epoch 111/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 7ms/step - accuracy: 0.8220 - loss: 0.4396 - val_accuracy: 0.8199 - val_loss: 0.4546 - learning_rate: 1.0000e-04\n",
      "Epoch 112/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 8ms/step - accuracy: 0.8256 - loss: 0.4356 - val_accuracy: 0.8191 - val_loss: 0.4539 - learning_rate: 1.0000e-04\n",
      "Epoch 113/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 7ms/step - accuracy: 0.8219 - loss: 0.4347 - val_accuracy: 0.8200 - val_loss: 0.4529 - learning_rate: 5.0000e-05\n",
      "Epoch 114/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.8212 - loss: 0.4385 - val_accuracy: 0.8196 - val_loss: 0.4539 - learning_rate: 5.0000e-05\n",
      "Epoch 115/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.8206 - loss: 0.4364 - val_accuracy: 0.8204 - val_loss: 0.4534 - learning_rate: 5.0000e-05\n",
      "Epoch 116/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 7ms/step - accuracy: 0.8211 - loss: 0.4331 - val_accuracy: 0.8208 - val_loss: 0.4523 - learning_rate: 5.0000e-05\n",
      "Epoch 117/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.8241 - loss: 0.4292 - val_accuracy: 0.8200 - val_loss: 0.4546 - learning_rate: 5.0000e-05\n",
      "Epoch 118/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 7ms/step - accuracy: 0.8212 - loss: 0.4322 - val_accuracy: 0.8207 - val_loss: 0.4537 - learning_rate: 5.0000e-05\n",
      "Epoch 119/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 7ms/step - accuracy: 0.8236 - loss: 0.4333 - val_accuracy: 0.8192 - val_loss: 0.4536 - learning_rate: 5.0000e-05\n",
      "Epoch 120/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 7ms/step - accuracy: 0.8254 - loss: 0.4327 - val_accuracy: 0.8196 - val_loss: 0.4546 - learning_rate: 5.0000e-05\n",
      "Epoch 121/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.8244 - loss: 0.4305 - val_accuracy: 0.8190 - val_loss: 0.4546 - learning_rate: 5.0000e-05\n",
      "Epoch 122/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 7ms/step - accuracy: 0.8246 - loss: 0.4271 - val_accuracy: 0.8205 - val_loss: 0.4545 - learning_rate: 5.0000e-05\n",
      "Epoch 123/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 7ms/step - accuracy: 0.8286 - loss: 0.4267 - val_accuracy: 0.8208 - val_loss: 0.4547 - learning_rate: 5.0000e-05\n",
      "Epoch 124/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 7ms/step - accuracy: 0.8211 - loss: 0.4338 - val_accuracy: 0.8203 - val_loss: 0.4557 - learning_rate: 5.0000e-05\n",
      "Epoch 125/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 8ms/step - accuracy: 0.8244 - loss: 0.4282 - val_accuracy: 0.8209 - val_loss: 0.4543 - learning_rate: 5.0000e-05\n",
      "Epoch 126/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.8209 - loss: 0.4305 - val_accuracy: 0.8203 - val_loss: 0.4552 - learning_rate: 5.0000e-05\n",
      "Epoch 127/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.8228 - loss: 0.4258 - val_accuracy: 0.8203 - val_loss: 0.4545 - learning_rate: 2.5000e-05\n",
      "Epoch 128/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.8283 - loss: 0.4241 - val_accuracy: 0.8184 - val_loss: 0.4549 - learning_rate: 2.5000e-05\n",
      "Epoch 129/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.8286 - loss: 0.4235 - val_accuracy: 0.8195 - val_loss: 0.4563 - learning_rate: 2.5000e-05\n",
      "Epoch 130/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.8234 - loss: 0.4233 - val_accuracy: 0.8180 - val_loss: 0.4559 - learning_rate: 2.5000e-05\n",
      "Epoch 131/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.8273 - loss: 0.4228 - val_accuracy: 0.8186 - val_loss: 0.4560 - learning_rate: 2.5000e-05\n",
      "Epoch 132/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 5ms/step - accuracy: 0.8252 - loss: 0.4232 - val_accuracy: 0.8199 - val_loss: 0.4555 - learning_rate: 2.5000e-05\n",
      "Epoch 133/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.8258 - loss: 0.4262 - val_accuracy: 0.8199 - val_loss: 0.4556 - learning_rate: 2.5000e-05\n",
      "Epoch 134/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.8251 - loss: 0.4275 - val_accuracy: 0.8201 - val_loss: 0.4558 - learning_rate: 2.5000e-05\n",
      "Epoch 135/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.8237 - loss: 0.4289 - val_accuracy: 0.8204 - val_loss: 0.4574 - learning_rate: 2.5000e-05\n",
      "Epoch 136/150\n",
      "\u001B[1m140/140\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 6ms/step - accuracy: 0.8236 - loss: 0.4251 - val_accuracy: 0.8187 - val_loss: 0.4564 - learning_rate: 2.5000e-05\n",
      "\n",
      "\n",
      "RESULTS:\n",
      "Validation Loss:     0.4523\n",
      "Validation Accuracy: 0.8208\n",
      "F1 Score:            0.4840\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T09:17:30.251414Z",
     "start_time": "2024-10-24T09:14:46.049244Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "from keras.src.utils import plot_model\n",
    "from keras.src.optimizers import Adam\n",
    "from keras.src.layers import Dense, BatchNormalization, Dropout\n",
    "from keras import Sequential, Input\n",
    "# Importing necessary libraries and modules from Keras and Scikit-learn\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, Callback\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize a StandardScaler object for feature scaling\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the training data and transform it\n",
    "X_train_scaled = scaler.fit_transform(X_train)  \n",
    "\n",
    "# Transform the validation data using the same scaler (to avoid data leakage)\n",
    "X_val_scaled = scaler.transform(X_val) \n",
    "\n",
    "def build_model(input_shape):\n",
    "    \"\"\"\n",
    "    Builds a sequential neural network model.\n",
    "\n",
    "    Parameters:\n",
    "    input_shape (int): The number of input features.\n",
    "\n",
    "    Returns:\n",
    "    model (Sequential): A compiled Keras Sequential model.\n",
    "    \"\"\"\n",
    "    model = Sequential()  # Initialize a Sequential model\n",
    "    model.add(Input(shape=(input_shape,)))  # Input layer with the specified shape\n",
    "\n",
    "    # First hidden layer with 256 units, ReLU activation, and L2 regularization\n",
    "    model.add(Dense(256, activation='relu', kernel_regularizer='l2')) \n",
    "\n",
    "    # Second hidden layer with 128 units, ReLU activation, and L2 regularization\n",
    "    model.add(Dense(128, activation='relu', kernel_regularizer='l2'))\n",
    "\n",
    "\n",
    "    # Third hidden layer with 64 units, ReLU activation, and L2 regularization\n",
    "    model.add(Dense(64, activation='relu', kernel_regularizer='l2'))\n",
    "\n",
    "    # Fourth hidden layer with 32 units, Leaky ReLU activation, and L2 regularization\n",
    "    model.add(Dense(32, activation='relu', kernel_regularizer='l2'))\n",
    "\n",
    "    # Output layer with a single unit and sigmoid activation for binary classification\n",
    "    model.add(Dense(1, activation='sigmoid'))  \n",
    "\n",
    "    # Compile the model with Adam optimizer, binary cross-entropy loss, and accuracy metric\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001), \n",
    "                  loss='binary_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model  # Return the compiled model\n",
    "\n",
    "# Determine the input shape based on the scaled training data\n",
    "input_shape = X_train_scaled.shape[1]  \n",
    "# Build the model with the defined input shape\n",
    "model = build_model(input_shape)\n",
    "\n",
    "# Set batch size and number of epochs for training\n",
    "batch_size = 64\n",
    "epochs = 200\n",
    "\n",
    "# Outputs a model summary table\n",
    "model.summary()\n",
    "\n",
    "# Define an EarlyStopping callback to halt training when validation loss does not improve\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "\n",
    "# Define a ReduceLROnPlateau callback to reduce learning rate when validation loss plateaus\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-6)\n",
    "\n",
    "class ProgressBar(Callback):\n",
    "    \"\"\"\n",
    "    Custom callback to display training progress.\n",
    "\n",
    "    Inherits from Keras Callback to provide additional functionality during training.\n",
    "    \"\"\"\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.epoch = epoch  # Store the current epoch number\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Format and print training metrics at the end of each epoch\n",
    "        metrics = (f\"Epoch {self.epoch + 1}/{self.params['epochs']} - \"\n",
    "                   f\"Loss: {logs['loss']:.4f} - \"\n",
    "                   f\"Val Loss: {logs['val_loss']:.4f} - \"\n",
    "                   f\"Accuracy: {logs['accuracy']:.4f} - \"\n",
    "                   f\"Val Accuracy: {logs['val_accuracy']:.4f}\")\n",
    "        print(\"\\r\" + metrics, end='')  # Print metrics on the same line\n",
    "\n",
    "# Train the model using the fit method with training and validation data\n",
    "history = model.fit(X_train_scaled, y_train, \n",
    "                    validation_data=(X_val_scaled, y_val),\n",
    "                    epochs=epochs, batch_size=batch_size, \n",
    "                    callbacks=[early_stopping, reduce_lr, ProgressBar()],\n",
    "                    verbose=1)  # Set verbose to 0 to suppress output\n",
    "\n",
    "# Evaluate the model's performance on the validation set\n",
    "val_loss, val_accuracy = model.evaluate(X_val_scaled, y_val, verbose=0)\n",
    "\n",
    "# Print results for validation loss and accuracy\n",
    "print()\n",
    "print(\"\\nRESULTS:\")\n",
    "print(f\"{'Validation Loss:':<20} {val_loss:.4f}\")\n",
    "print(f\"{'Validation Accuracy:':<20} {val_accuracy:.4f}\")\n",
    "\n",
    "# Generate predictions on the validation set and convert probabilities to binary labels\n",
    "y_pred_prob = model.predict(X_val_scaled, verbose=0) \n",
    "y_pred = (y_pred_prob > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
    "\n",
    "# Calculate the F1 score to evaluate the model's performance\n",
    "f1 = f1_score(y_val, y_pred)  \n",
    "print(f\"{'F1 Score:':<20} {f1:.4f}\")  # Print the F1 score"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"sequential_4\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_20 (\u001B[38;5;33mDense\u001B[0m)                │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m256\u001B[0m)            │         \u001B[38;5;34m6,144\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_21 (\u001B[38;5;33mDense\u001B[0m)                │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m128\u001B[0m)            │        \u001B[38;5;34m32,896\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_22 (\u001B[38;5;33mDense\u001B[0m)                │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m64\u001B[0m)             │         \u001B[38;5;34m8,256\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_23 (\u001B[38;5;33mDense\u001B[0m)                │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m32\u001B[0m)             │         \u001B[38;5;34m2,080\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_24 (\u001B[38;5;33mDense\u001B[0m)                │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m1\u001B[0m)              │            \u001B[38;5;34m33\u001B[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,144</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m49,409\u001B[0m (193.00 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">49,409</span> (193.00 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m49,409\u001B[0m (193.00 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">49,409</span> (193.00 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 3ms/step - accuracy: 0.6989 - loss: 3.6457 - val_accuracy: 0.7993 - val_loss: 2.6429 - learning_rate: 1.0000e-04\n",
      "Epoch 2/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 2ms/step - accuracy: 0.8009 - loss: 2.4062 - val_accuracy: 0.8075 - val_loss: 1.8136 - learning_rate: 1.0000e-04\n",
      "Epoch 3/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8072 - loss: 1.6690 - val_accuracy: 0.8094 - val_loss: 1.3022 - learning_rate: 1.0000e-04\n",
      "Epoch 4/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8132 - loss: 1.2075 - val_accuracy: 0.8110 - val_loss: 0.9891 - learning_rate: 1.0000e-04\n",
      "Epoch 5/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8103 - loss: 0.9340 - val_accuracy: 0.8119 - val_loss: 0.7980 - learning_rate: 1.0000e-04\n",
      "Epoch 6/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8137 - loss: 0.7627 - val_accuracy: 0.8093 - val_loss: 0.6820 - learning_rate: 1.0000e-04\n",
      "Epoch 7/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8089 - loss: 0.6640 - val_accuracy: 0.8136 - val_loss: 0.6101 - learning_rate: 1.0000e-04\n",
      "Epoch 8/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8156 - loss: 0.5977 - val_accuracy: 0.8133 - val_loss: 0.5661 - learning_rate: 1.0000e-04\n",
      "Epoch 9/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8168 - loss: 0.5568 - val_accuracy: 0.8140 - val_loss: 0.5381 - learning_rate: 1.0000e-04\n",
      "Epoch 10/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8142 - loss: 0.5291 - val_accuracy: 0.8158 - val_loss: 0.5210 - learning_rate: 1.0000e-04\n",
      "Epoch 11/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8138 - loss: 0.5208 - val_accuracy: 0.8156 - val_loss: 0.5088 - learning_rate: 1.0000e-04\n",
      "Epoch 12/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8151 - loss: 0.5070 - val_accuracy: 0.8142 - val_loss: 0.5013 - learning_rate: 1.0000e-04\n",
      "Epoch 13/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8095 - loss: 0.5051 - val_accuracy: 0.8170 - val_loss: 0.4961 - learning_rate: 1.0000e-04\n",
      "Epoch 14/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8131 - loss: 0.4974 - val_accuracy: 0.8149 - val_loss: 0.4919 - learning_rate: 1.0000e-04\n",
      "Epoch 15/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8122 - loss: 0.4922 - val_accuracy: 0.8171 - val_loss: 0.4893 - learning_rate: 1.0000e-04\n",
      "Epoch 16/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8224 - loss: 0.4792 - val_accuracy: 0.8137 - val_loss: 0.4872 - learning_rate: 1.0000e-04\n",
      "Epoch 17/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8108 - loss: 0.4923 - val_accuracy: 0.8156 - val_loss: 0.4856 - learning_rate: 1.0000e-04\n",
      "Epoch 18/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8190 - loss: 0.4806 - val_accuracy: 0.8161 - val_loss: 0.4837 - learning_rate: 1.0000e-04\n",
      "Epoch 19/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8179 - loss: 0.4805 - val_accuracy: 0.8166 - val_loss: 0.4829 - learning_rate: 1.0000e-04\n",
      "Epoch 20/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8143 - loss: 0.4849 - val_accuracy: 0.8167 - val_loss: 0.4814 - learning_rate: 1.0000e-04\n",
      "Epoch 21/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8190 - loss: 0.4742 - val_accuracy: 0.8146 - val_loss: 0.4807 - learning_rate: 1.0000e-04\n",
      "Epoch 22/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8123 - loss: 0.4844 - val_accuracy: 0.8118 - val_loss: 0.4809 - learning_rate: 1.0000e-04\n",
      "Epoch 23/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8100 - loss: 0.4876 - val_accuracy: 0.8166 - val_loss: 0.4792 - learning_rate: 1.0000e-04\n",
      "Epoch 24/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8144 - loss: 0.4784 - val_accuracy: 0.8161 - val_loss: 0.4790 - learning_rate: 1.0000e-04\n",
      "Epoch 25/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8142 - loss: 0.4796 - val_accuracy: 0.8174 - val_loss: 0.4783 - learning_rate: 1.0000e-04\n",
      "Epoch 26/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8160 - loss: 0.4771 - val_accuracy: 0.8131 - val_loss: 0.4796 - learning_rate: 1.0000e-04\n",
      "Epoch 27/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8134 - loss: 0.4815 - val_accuracy: 0.8173 - val_loss: 0.4779 - learning_rate: 1.0000e-04\n",
      "Epoch 28/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8186 - loss: 0.4736 - val_accuracy: 0.8153 - val_loss: 0.4770 - learning_rate: 1.0000e-04\n",
      "Epoch 29/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8140 - loss: 0.4774 - val_accuracy: 0.8129 - val_loss: 0.4769 - learning_rate: 1.0000e-04\n",
      "Epoch 30/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8076 - loss: 0.4890 - val_accuracy: 0.8182 - val_loss: 0.4758 - learning_rate: 1.0000e-04\n",
      "Epoch 31/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8164 - loss: 0.4784 - val_accuracy: 0.8179 - val_loss: 0.4768 - learning_rate: 1.0000e-04\n",
      "Epoch 32/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8103 - loss: 0.4848 - val_accuracy: 0.8165 - val_loss: 0.4749 - learning_rate: 1.0000e-04\n",
      "Epoch 33/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8124 - loss: 0.4785 - val_accuracy: 0.8153 - val_loss: 0.4754 - learning_rate: 1.0000e-04\n",
      "Epoch 34/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8107 - loss: 0.4826 - val_accuracy: 0.8170 - val_loss: 0.4744 - learning_rate: 1.0000e-04\n",
      "Epoch 35/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8196 - loss: 0.4685 - val_accuracy: 0.8180 - val_loss: 0.4745 - learning_rate: 1.0000e-04\n",
      "Epoch 36/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8223 - loss: 0.4655 - val_accuracy: 0.8156 - val_loss: 0.4743 - learning_rate: 1.0000e-04\n",
      "Epoch 37/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8186 - loss: 0.4720 - val_accuracy: 0.8183 - val_loss: 0.4746 - learning_rate: 1.0000e-04\n",
      "Epoch 38/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8215 - loss: 0.4674 - val_accuracy: 0.8192 - val_loss: 0.4750 - learning_rate: 1.0000e-04\n",
      "Epoch 39/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8165 - loss: 0.4754 - val_accuracy: 0.8159 - val_loss: 0.4733 - learning_rate: 1.0000e-04\n",
      "Epoch 40/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8152 - loss: 0.4750 - val_accuracy: 0.8188 - val_loss: 0.4742 - learning_rate: 1.0000e-04\n",
      "Epoch 41/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8189 - loss: 0.4727 - val_accuracy: 0.8182 - val_loss: 0.4727 - learning_rate: 1.0000e-04\n",
      "Epoch 42/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8145 - loss: 0.4747 - val_accuracy: 0.8182 - val_loss: 0.4729 - learning_rate: 1.0000e-04\n",
      "Epoch 43/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8149 - loss: 0.4765 - val_accuracy: 0.8158 - val_loss: 0.4731 - learning_rate: 1.0000e-04\n",
      "Epoch 44/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8227 - loss: 0.4634 - val_accuracy: 0.8146 - val_loss: 0.4729 - learning_rate: 1.0000e-04\n",
      "Epoch 45/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8165 - loss: 0.4732 - val_accuracy: 0.8182 - val_loss: 0.4725 - learning_rate: 1.0000e-04\n",
      "Epoch 46/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8170 - loss: 0.4757 - val_accuracy: 0.8146 - val_loss: 0.4721 - learning_rate: 1.0000e-04\n",
      "Epoch 47/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8158 - loss: 0.4748 - val_accuracy: 0.8148 - val_loss: 0.4722 - learning_rate: 1.0000e-04\n",
      "Epoch 48/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8155 - loss: 0.4787 - val_accuracy: 0.8179 - val_loss: 0.4719 - learning_rate: 1.0000e-04\n",
      "Epoch 49/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8168 - loss: 0.4725 - val_accuracy: 0.8162 - val_loss: 0.4714 - learning_rate: 1.0000e-04\n",
      "Epoch 50/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8169 - loss: 0.4686 - val_accuracy: 0.8163 - val_loss: 0.4714 - learning_rate: 1.0000e-04\n",
      "Epoch 51/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8134 - loss: 0.4765 - val_accuracy: 0.8178 - val_loss: 0.4726 - learning_rate: 1.0000e-04\n",
      "Epoch 52/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8205 - loss: 0.4684 - val_accuracy: 0.8182 - val_loss: 0.4705 - learning_rate: 1.0000e-04\n",
      "Epoch 53/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8179 - loss: 0.4702 - val_accuracy: 0.8178 - val_loss: 0.4720 - learning_rate: 1.0000e-04\n",
      "Epoch 54/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8209 - loss: 0.4662 - val_accuracy: 0.8174 - val_loss: 0.4702 - learning_rate: 1.0000e-04\n",
      "Epoch 55/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8121 - loss: 0.4802 - val_accuracy: 0.8184 - val_loss: 0.4701 - learning_rate: 1.0000e-04\n",
      "Epoch 56/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8148 - loss: 0.4759 - val_accuracy: 0.8170 - val_loss: 0.4702 - learning_rate: 1.0000e-04\n",
      "Epoch 57/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8124 - loss: 0.4761 - val_accuracy: 0.8184 - val_loss: 0.4702 - learning_rate: 1.0000e-04\n",
      "Epoch 58/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8203 - loss: 0.4689 - val_accuracy: 0.8169 - val_loss: 0.4702 - learning_rate: 1.0000e-04\n",
      "Epoch 59/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8204 - loss: 0.4656 - val_accuracy: 0.8183 - val_loss: 0.4696 - learning_rate: 1.0000e-04\n",
      "Epoch 60/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8157 - loss: 0.4760 - val_accuracy: 0.8175 - val_loss: 0.4705 - learning_rate: 1.0000e-04\n",
      "Epoch 61/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 4ms/step - accuracy: 0.8153 - loss: 0.4733 - val_accuracy: 0.8167 - val_loss: 0.4693 - learning_rate: 1.0000e-04\n",
      "Epoch 62/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 4ms/step - accuracy: 0.8166 - loss: 0.4700 - val_accuracy: 0.8167 - val_loss: 0.4692 - learning_rate: 1.0000e-04\n",
      "Epoch 63/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8148 - loss: 0.4711 - val_accuracy: 0.8180 - val_loss: 0.4692 - learning_rate: 1.0000e-04\n",
      "Epoch 64/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8192 - loss: 0.4680 - val_accuracy: 0.8190 - val_loss: 0.4688 - learning_rate: 1.0000e-04\n",
      "Epoch 65/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8171 - loss: 0.4722 - val_accuracy: 0.8166 - val_loss: 0.4692 - learning_rate: 1.0000e-04\n",
      "Epoch 66/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8176 - loss: 0.4711 - val_accuracy: 0.8174 - val_loss: 0.4688 - learning_rate: 1.0000e-04\n",
      "Epoch 67/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8202 - loss: 0.4665 - val_accuracy: 0.8186 - val_loss: 0.4687 - learning_rate: 1.0000e-04\n",
      "Epoch 68/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8203 - loss: 0.4665 - val_accuracy: 0.8170 - val_loss: 0.4688 - learning_rate: 1.0000e-04\n",
      "Epoch 69/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8194 - loss: 0.4621 - val_accuracy: 0.8169 - val_loss: 0.4684 - learning_rate: 1.0000e-04\n",
      "Epoch 70/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8149 - loss: 0.4711 - val_accuracy: 0.8188 - val_loss: 0.4682 - learning_rate: 1.0000e-04\n",
      "Epoch 71/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8212 - loss: 0.4634 - val_accuracy: 0.8142 - val_loss: 0.4697 - learning_rate: 1.0000e-04\n",
      "Epoch 72/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8156 - loss: 0.4696 - val_accuracy: 0.8191 - val_loss: 0.4681 - learning_rate: 1.0000e-04\n",
      "Epoch 73/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8133 - loss: 0.4760 - val_accuracy: 0.8182 - val_loss: 0.4679 - learning_rate: 1.0000e-04\n",
      "Epoch 74/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8149 - loss: 0.4732 - val_accuracy: 0.8184 - val_loss: 0.4681 - learning_rate: 1.0000e-04\n",
      "Epoch 75/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8218 - loss: 0.4661 - val_accuracy: 0.8183 - val_loss: 0.4681 - learning_rate: 1.0000e-04\n",
      "Epoch 76/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8180 - loss: 0.4705 - val_accuracy: 0.8167 - val_loss: 0.4678 - learning_rate: 1.0000e-04\n",
      "Epoch 77/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8253 - loss: 0.4593 - val_accuracy: 0.8158 - val_loss: 0.4696 - learning_rate: 1.0000e-04\n",
      "Epoch 78/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8119 - loss: 0.4726 - val_accuracy: 0.8178 - val_loss: 0.4676 - learning_rate: 1.0000e-04\n",
      "Epoch 79/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8164 - loss: 0.4700 - val_accuracy: 0.8192 - val_loss: 0.4683 - learning_rate: 1.0000e-04\n",
      "Epoch 80/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8212 - loss: 0.4663 - val_accuracy: 0.8173 - val_loss: 0.4674 - learning_rate: 1.0000e-04\n",
      "Epoch 81/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8195 - loss: 0.4619 - val_accuracy: 0.8175 - val_loss: 0.4669 - learning_rate: 1.0000e-04\n",
      "Epoch 82/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8173 - loss: 0.4688 - val_accuracy: 0.8165 - val_loss: 0.4677 - learning_rate: 1.0000e-04\n",
      "Epoch 83/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8181 - loss: 0.4664 - val_accuracy: 0.8184 - val_loss: 0.4669 - learning_rate: 1.0000e-04\n",
      "Epoch 84/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8221 - loss: 0.4643 - val_accuracy: 0.8179 - val_loss: 0.4668 - learning_rate: 1.0000e-04\n",
      "Epoch 85/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8169 - loss: 0.4693 - val_accuracy: 0.8190 - val_loss: 0.4670 - learning_rate: 1.0000e-04\n",
      "Epoch 86/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8235 - loss: 0.4607 - val_accuracy: 0.8173 - val_loss: 0.4670 - learning_rate: 1.0000e-04\n",
      "Epoch 87/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8163 - loss: 0.4734 - val_accuracy: 0.8187 - val_loss: 0.4667 - learning_rate: 1.0000e-04\n",
      "Epoch 88/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8163 - loss: 0.4702 - val_accuracy: 0.8195 - val_loss: 0.4674 - learning_rate: 1.0000e-04\n",
      "Epoch 89/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8174 - loss: 0.4718 - val_accuracy: 0.8187 - val_loss: 0.4662 - learning_rate: 1.0000e-04\n",
      "Epoch 90/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8171 - loss: 0.4682 - val_accuracy: 0.8195 - val_loss: 0.4666 - learning_rate: 1.0000e-04\n",
      "Epoch 91/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8153 - loss: 0.4714 - val_accuracy: 0.8188 - val_loss: 0.4669 - learning_rate: 1.0000e-04\n",
      "Epoch 92/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8158 - loss: 0.4708 - val_accuracy: 0.8187 - val_loss: 0.4661 - learning_rate: 1.0000e-04\n",
      "Epoch 93/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8153 - loss: 0.4740 - val_accuracy: 0.8187 - val_loss: 0.4660 - learning_rate: 1.0000e-04\n",
      "Epoch 94/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8215 - loss: 0.4624 - val_accuracy: 0.8167 - val_loss: 0.4675 - learning_rate: 1.0000e-04\n",
      "Epoch 95/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8120 - loss: 0.4739 - val_accuracy: 0.8174 - val_loss: 0.4659 - learning_rate: 1.0000e-04\n",
      "Epoch 96/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8153 - loss: 0.4711 - val_accuracy: 0.8192 - val_loss: 0.4669 - learning_rate: 1.0000e-04\n",
      "Epoch 97/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8166 - loss: 0.4700 - val_accuracy: 0.8187 - val_loss: 0.4659 - learning_rate: 1.0000e-04\n",
      "Epoch 98/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8180 - loss: 0.4677 - val_accuracy: 0.8180 - val_loss: 0.4665 - learning_rate: 1.0000e-04\n",
      "Epoch 99/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8201 - loss: 0.4617 - val_accuracy: 0.8173 - val_loss: 0.4658 - learning_rate: 1.0000e-04\n",
      "Epoch 100/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8145 - loss: 0.4678 - val_accuracy: 0.8178 - val_loss: 0.4660 - learning_rate: 1.0000e-04\n",
      "Epoch 101/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8259 - loss: 0.4538 - val_accuracy: 0.8184 - val_loss: 0.4666 - learning_rate: 1.0000e-04\n",
      "Epoch 102/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8209 - loss: 0.4606 - val_accuracy: 0.8187 - val_loss: 0.4653 - learning_rate: 1.0000e-04\n",
      "Epoch 103/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8151 - loss: 0.4678 - val_accuracy: 0.8179 - val_loss: 0.4652 - learning_rate: 1.0000e-04\n",
      "Epoch 104/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8169 - loss: 0.4706 - val_accuracy: 0.8187 - val_loss: 0.4651 - learning_rate: 1.0000e-04\n",
      "Epoch 105/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8182 - loss: 0.4634 - val_accuracy: 0.8175 - val_loss: 0.4652 - learning_rate: 1.0000e-04\n",
      "Epoch 106/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8158 - loss: 0.4670 - val_accuracy: 0.8184 - val_loss: 0.4654 - learning_rate: 1.0000e-04\n",
      "Epoch 107/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8175 - loss: 0.4682 - val_accuracy: 0.8191 - val_loss: 0.4650 - learning_rate: 1.0000e-04\n",
      "Epoch 108/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8136 - loss: 0.4698 - val_accuracy: 0.8184 - val_loss: 0.4649 - learning_rate: 1.0000e-04\n",
      "Epoch 109/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8147 - loss: 0.4706 - val_accuracy: 0.8184 - val_loss: 0.4650 - learning_rate: 1.0000e-04\n",
      "Epoch 110/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8161 - loss: 0.4705 - val_accuracy: 0.8187 - val_loss: 0.4648 - learning_rate: 1.0000e-04\n",
      "Epoch 111/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8183 - loss: 0.4633 - val_accuracy: 0.8192 - val_loss: 0.4649 - learning_rate: 1.0000e-04\n",
      "Epoch 112/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8178 - loss: 0.4657 - val_accuracy: 0.8190 - val_loss: 0.4650 - learning_rate: 1.0000e-04\n",
      "Epoch 113/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8191 - loss: 0.4672 - val_accuracy: 0.8178 - val_loss: 0.4646 - learning_rate: 1.0000e-04\n",
      "Epoch 114/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8211 - loss: 0.4626 - val_accuracy: 0.8186 - val_loss: 0.4648 - learning_rate: 1.0000e-04\n",
      "Epoch 115/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8144 - loss: 0.4698 - val_accuracy: 0.8193 - val_loss: 0.4644 - learning_rate: 1.0000e-04\n",
      "Epoch 116/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8186 - loss: 0.4654 - val_accuracy: 0.8175 - val_loss: 0.4650 - learning_rate: 1.0000e-04\n",
      "Epoch 117/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8175 - loss: 0.4682 - val_accuracy: 0.8188 - val_loss: 0.4645 - learning_rate: 1.0000e-04\n",
      "Epoch 118/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8214 - loss: 0.4617 - val_accuracy: 0.8187 - val_loss: 0.4643 - learning_rate: 1.0000e-04\n",
      "Epoch 119/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8171 - loss: 0.4639 - val_accuracy: 0.8197 - val_loss: 0.4645 - learning_rate: 1.0000e-04\n",
      "Epoch 120/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8166 - loss: 0.4648 - val_accuracy: 0.8186 - val_loss: 0.4641 - learning_rate: 1.0000e-04\n",
      "Epoch 121/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8157 - loss: 0.4671 - val_accuracy: 0.8187 - val_loss: 0.4642 - learning_rate: 1.0000e-04\n",
      "Epoch 122/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8223 - loss: 0.4618 - val_accuracy: 0.8178 - val_loss: 0.4646 - learning_rate: 1.0000e-04\n",
      "Epoch 123/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8206 - loss: 0.4610 - val_accuracy: 0.8178 - val_loss: 0.4643 - learning_rate: 1.0000e-04\n",
      "Epoch 124/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8168 - loss: 0.4666 - val_accuracy: 0.8178 - val_loss: 0.4640 - learning_rate: 1.0000e-04\n",
      "Epoch 125/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8202 - loss: 0.4652 - val_accuracy: 0.8175 - val_loss: 0.4652 - learning_rate: 1.0000e-04\n",
      "Epoch 126/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8186 - loss: 0.4674 - val_accuracy: 0.8188 - val_loss: 0.4639 - learning_rate: 1.0000e-04\n",
      "Epoch 127/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8165 - loss: 0.4671 - val_accuracy: 0.8180 - val_loss: 0.4641 - learning_rate: 1.0000e-04\n",
      "Epoch 128/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8165 - loss: 0.4648 - val_accuracy: 0.8183 - val_loss: 0.4643 - learning_rate: 1.0000e-04\n",
      "Epoch 129/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8172 - loss: 0.4658 - val_accuracy: 0.8184 - val_loss: 0.4638 - learning_rate: 1.0000e-04\n",
      "Epoch 130/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8213 - loss: 0.4622 - val_accuracy: 0.8178 - val_loss: 0.4642 - learning_rate: 1.0000e-04\n",
      "Epoch 131/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8146 - loss: 0.4698 - val_accuracy: 0.8178 - val_loss: 0.4668 - learning_rate: 1.0000e-04\n",
      "Epoch 132/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8212 - loss: 0.4630 - val_accuracy: 0.8180 - val_loss: 0.4650 - learning_rate: 1.0000e-04\n",
      "Epoch 133/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8180 - loss: 0.4654 - val_accuracy: 0.8179 - val_loss: 0.4635 - learning_rate: 1.0000e-04\n",
      "Epoch 134/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8179 - loss: 0.4646 - val_accuracy: 0.8171 - val_loss: 0.4650 - learning_rate: 1.0000e-04\n",
      "Epoch 135/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8171 - loss: 0.4686 - val_accuracy: 0.8184 - val_loss: 0.4636 - learning_rate: 1.0000e-04\n",
      "Epoch 136/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8175 - loss: 0.4642 - val_accuracy: 0.8180 - val_loss: 0.4641 - learning_rate: 1.0000e-04\n",
      "Epoch 137/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8203 - loss: 0.4627 - val_accuracy: 0.8186 - val_loss: 0.4633 - learning_rate: 1.0000e-04\n",
      "Epoch 138/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8174 - loss: 0.4686 - val_accuracy: 0.8201 - val_loss: 0.4637 - learning_rate: 1.0000e-04\n",
      "Epoch 139/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8131 - loss: 0.4691 - val_accuracy: 0.8180 - val_loss: 0.4635 - learning_rate: 1.0000e-04\n",
      "Epoch 140/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8149 - loss: 0.4676 - val_accuracy: 0.8186 - val_loss: 0.4631 - learning_rate: 1.0000e-04\n",
      "Epoch 141/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8192 - loss: 0.4637 - val_accuracy: 0.8186 - val_loss: 0.4632 - learning_rate: 1.0000e-04\n",
      "Epoch 142/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8167 - loss: 0.4657 - val_accuracy: 0.8187 - val_loss: 0.4635 - learning_rate: 1.0000e-04\n",
      "Epoch 143/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8129 - loss: 0.4697 - val_accuracy: 0.8182 - val_loss: 0.4629 - learning_rate: 1.0000e-04\n",
      "Epoch 144/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8199 - loss: 0.4620 - val_accuracy: 0.8187 - val_loss: 0.4630 - learning_rate: 1.0000e-04\n",
      "Epoch 145/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8180 - loss: 0.4649 - val_accuracy: 0.8176 - val_loss: 0.4632 - learning_rate: 1.0000e-04\n",
      "Epoch 146/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8152 - loss: 0.4684 - val_accuracy: 0.8184 - val_loss: 0.4628 - learning_rate: 1.0000e-04\n",
      "Epoch 147/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8182 - loss: 0.4660 - val_accuracy: 0.8193 - val_loss: 0.4636 - learning_rate: 1.0000e-04\n",
      "Epoch 148/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 2ms/step - accuracy: 0.8156 - loss: 0.4686 - val_accuracy: 0.8180 - val_loss: 0.4632 - learning_rate: 1.0000e-04\n",
      "Epoch 149/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8206 - loss: 0.4615 - val_accuracy: 0.8182 - val_loss: 0.4640 - learning_rate: 1.0000e-04\n",
      "Epoch 150/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8204 - loss: 0.4610 - val_accuracy: 0.8179 - val_loss: 0.4632 - learning_rate: 1.0000e-04\n",
      "Epoch 151/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8225 - loss: 0.4594 - val_accuracy: 0.8178 - val_loss: 0.4628 - learning_rate: 1.0000e-04\n",
      "Epoch 152/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8200 - loss: 0.4587 - val_accuracy: 0.8192 - val_loss: 0.4627 - learning_rate: 1.0000e-04\n",
      "Epoch 153/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8187 - loss: 0.4636 - val_accuracy: 0.8186 - val_loss: 0.4625 - learning_rate: 1.0000e-04\n",
      "Epoch 154/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8255 - loss: 0.4536 - val_accuracy: 0.8191 - val_loss: 0.4639 - learning_rate: 1.0000e-04\n",
      "Epoch 155/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8192 - loss: 0.4603 - val_accuracy: 0.8180 - val_loss: 0.4626 - learning_rate: 1.0000e-04\n",
      "Epoch 156/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8120 - loss: 0.4721 - val_accuracy: 0.8166 - val_loss: 0.4631 - learning_rate: 1.0000e-04\n",
      "Epoch 157/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8195 - loss: 0.4580 - val_accuracy: 0.8191 - val_loss: 0.4632 - learning_rate: 1.0000e-04\n",
      "Epoch 158/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8164 - loss: 0.4672 - val_accuracy: 0.8187 - val_loss: 0.4628 - learning_rate: 1.0000e-04\n",
      "Epoch 159/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 2ms/step - accuracy: 0.8170 - loss: 0.4647 - val_accuracy: 0.8183 - val_loss: 0.4625 - learning_rate: 1.0000e-04\n",
      "Epoch 160/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8209 - loss: 0.4594 - val_accuracy: 0.8183 - val_loss: 0.4629 - learning_rate: 1.0000e-04\n",
      "Epoch 161/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8128 - loss: 0.4697 - val_accuracy: 0.8190 - val_loss: 0.4626 - learning_rate: 1.0000e-04\n",
      "Epoch 162/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8197 - loss: 0.4617 - val_accuracy: 0.8180 - val_loss: 0.4624 - learning_rate: 1.0000e-04\n",
      "Epoch 163/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8168 - loss: 0.4661 - val_accuracy: 0.8178 - val_loss: 0.4624 - learning_rate: 1.0000e-04\n",
      "Epoch 164/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8131 - loss: 0.4711 - val_accuracy: 0.8199 - val_loss: 0.4624 - learning_rate: 1.0000e-04\n",
      "Epoch 165/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8131 - loss: 0.4685 - val_accuracy: 0.8192 - val_loss: 0.4627 - learning_rate: 1.0000e-04\n",
      "Epoch 166/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8217 - loss: 0.4591 - val_accuracy: 0.8173 - val_loss: 0.4640 - learning_rate: 1.0000e-04\n",
      "Epoch 167/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8177 - loss: 0.4628 - val_accuracy: 0.8183 - val_loss: 0.4622 - learning_rate: 1.0000e-04\n",
      "Epoch 168/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8184 - loss: 0.4634 - val_accuracy: 0.8188 - val_loss: 0.4620 - learning_rate: 1.0000e-04\n",
      "Epoch 169/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8195 - loss: 0.4592 - val_accuracy: 0.8191 - val_loss: 0.4634 - learning_rate: 1.0000e-04\n",
      "Epoch 170/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8186 - loss: 0.4631 - val_accuracy: 0.8179 - val_loss: 0.4620 - learning_rate: 1.0000e-04\n",
      "Epoch 171/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8095 - loss: 0.4748 - val_accuracy: 0.8184 - val_loss: 0.4620 - learning_rate: 1.0000e-04\n",
      "Epoch 172/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8135 - loss: 0.4692 - val_accuracy: 0.8183 - val_loss: 0.4619 - learning_rate: 1.0000e-04\n",
      "Epoch 173/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8182 - loss: 0.4640 - val_accuracy: 0.8186 - val_loss: 0.4625 - learning_rate: 1.0000e-04\n",
      "Epoch 174/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8200 - loss: 0.4609 - val_accuracy: 0.8184 - val_loss: 0.4620 - learning_rate: 1.0000e-04\n",
      "Epoch 175/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8220 - loss: 0.4559 - val_accuracy: 0.8195 - val_loss: 0.4624 - learning_rate: 1.0000e-04\n",
      "Epoch 176/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8189 - loss: 0.4604 - val_accuracy: 0.8188 - val_loss: 0.4623 - learning_rate: 1.0000e-04\n",
      "Epoch 177/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8213 - loss: 0.4558 - val_accuracy: 0.8180 - val_loss: 0.4623 - learning_rate: 1.0000e-04\n",
      "Epoch 178/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8217 - loss: 0.4577 - val_accuracy: 0.8187 - val_loss: 0.4620 - learning_rate: 1.0000e-04\n",
      "Epoch 179/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8137 - loss: 0.4685 - val_accuracy: 0.8188 - val_loss: 0.4618 - learning_rate: 1.0000e-04\n",
      "Epoch 180/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8160 - loss: 0.4610 - val_accuracy: 0.8183 - val_loss: 0.4617 - learning_rate: 1.0000e-04\n",
      "Epoch 181/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8155 - loss: 0.4649 - val_accuracy: 0.8191 - val_loss: 0.4621 - learning_rate: 1.0000e-04\n",
      "Epoch 182/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8158 - loss: 0.4610 - val_accuracy: 0.8187 - val_loss: 0.4618 - learning_rate: 1.0000e-04\n",
      "Epoch 183/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8118 - loss: 0.4690 - val_accuracy: 0.8184 - val_loss: 0.4617 - learning_rate: 1.0000e-04\n",
      "Epoch 184/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8150 - loss: 0.4664 - val_accuracy: 0.8182 - val_loss: 0.4624 - learning_rate: 1.0000e-04\n",
      "Epoch 185/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8154 - loss: 0.4635 - val_accuracy: 0.8188 - val_loss: 0.4619 - learning_rate: 1.0000e-04\n",
      "Epoch 186/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8180 - loss: 0.4678 - val_accuracy: 0.8175 - val_loss: 0.4621 - learning_rate: 1.0000e-04\n",
      "Epoch 187/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8144 - loss: 0.4697 - val_accuracy: 0.8179 - val_loss: 0.4616 - learning_rate: 1.0000e-04\n",
      "Epoch 188/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8201 - loss: 0.4598 - val_accuracy: 0.8184 - val_loss: 0.4615 - learning_rate: 1.0000e-04\n",
      "Epoch 189/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8140 - loss: 0.4641 - val_accuracy: 0.8184 - val_loss: 0.4615 - learning_rate: 1.0000e-04\n",
      "Epoch 190/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 2ms/step - accuracy: 0.8189 - loss: 0.4626 - val_accuracy: 0.8191 - val_loss: 0.4624 - learning_rate: 1.0000e-04\n",
      "Epoch 191/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8170 - loss: 0.4643 - val_accuracy: 0.8182 - val_loss: 0.4618 - learning_rate: 1.0000e-04\n",
      "Epoch 192/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 2ms/step - accuracy: 0.8200 - loss: 0.4581 - val_accuracy: 0.8195 - val_loss: 0.4615 - learning_rate: 1.0000e-04\n",
      "Epoch 193/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8181 - loss: 0.4620 - val_accuracy: 0.8195 - val_loss: 0.4613 - learning_rate: 1.0000e-04\n",
      "Epoch 194/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 2ms/step - accuracy: 0.8176 - loss: 0.4611 - val_accuracy: 0.8184 - val_loss: 0.4615 - learning_rate: 1.0000e-04\n",
      "Epoch 195/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8183 - loss: 0.4619 - val_accuracy: 0.8182 - val_loss: 0.4612 - learning_rate: 1.0000e-04\n",
      "Epoch 196/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8194 - loss: 0.4597 - val_accuracy: 0.8184 - val_loss: 0.4613 - learning_rate: 1.0000e-04\n",
      "Epoch 197/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 2ms/step - accuracy: 0.8116 - loss: 0.4689 - val_accuracy: 0.8182 - val_loss: 0.4610 - learning_rate: 1.0000e-04\n",
      "Epoch 198/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8248 - loss: 0.4515 - val_accuracy: 0.8191 - val_loss: 0.4622 - learning_rate: 1.0000e-04\n",
      "Epoch 199/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 3ms/step - accuracy: 0.8166 - loss: 0.4658 - val_accuracy: 0.8186 - val_loss: 0.4618 - learning_rate: 1.0000e-04\n",
      "Epoch 200/200\n",
      "\u001B[1m279/279\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 2ms/step - accuracy: 0.8168 - loss: 0.4624 - val_accuracy: 0.8179 - val_loss: 0.4612 - learning_rate: 1.0000e-04\n",
      "\n",
      "\n",
      "RESULTS:\n",
      "Validation Loss:     0.4610\n",
      "Validation Accuracy: 0.8182\n",
      "F1 Score:            0.4689\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8w7UIdtIWuCD"
   },
   "source": [
    "## Analyze and comment the training results\n",
    "\n",
    "here goes any comment/visualization of the training history and any initial consideration on the training results  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2jiOZzvyJbsN"
   },
   "source": [
    "## Validate the model and comment the results\n",
    "\n",
    "please describe the evaluation procedure on a validation set, commenting the generalization capability of your model (e.g. under/overfitting). You may also describe the performance metrics that you choose: what is the most suitable performance measure (or set of performance measures) in this case/dataset, according to you? Why?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "sgGlAIaEJbsO",
    "ExecuteTime": {
     "end_time": "2024-10-24T08:45:44.658898Z",
     "start_time": "2024-10-24T08:45:44.290902Z"
    }
   },
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "\n",
    "# Step 1: Generate predictions on the validation set\n",
    "y_val_pred_prob = model.predict(X_val_scaled)\n",
    "y_val_pred = (y_val_pred_prob > 0.5).astype(int)  # Threshold of 0.5 for binary classification\n",
    "\n",
    "# Step 2: Confusion Matrix\n",
    "confusion = confusion_matrix(y_val, y_val_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion)\n",
    "\n",
    "# Step 3: Classification Report\n",
    "class_report = classification_report(y_val, y_val_pred, target_names=[\"No Default\", \"Default\"])\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n",
    "\n",
    "# Step 4: Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_val, y_val_pred_prob)\n",
    "print(f\"ROC AUC Score: {roc_auc:.4f}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m240/240\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 789us/step\n",
      "Confusion Matrix:\n",
      "[[5530  415]\n",
      " [1040  665]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  No Default       0.84      0.93      0.88      5945\n",
      "     Default       0.62      0.39      0.48      1705\n",
      "\n",
      "    accuracy                           0.81      7650\n",
      "   macro avg       0.73      0.66      0.68      7650\n",
      "weighted avg       0.79      0.81      0.79      7650\n",
      "\n",
      "ROC AUC Score: 0.7469\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5MhCwXroWmf9"
   },
   "source": [
    "## Make predictions (on the provided test set)\n",
    "\n",
    "Based on the results obtained and analyzed during the training and the validation phases, what are your (rather _personal_) expectations with respect to the performances of your model on the blind external test set? Briefly motivate your answer."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fbtA2vJRWpMY",
    "ExecuteTime": {
     "end_time": "2024-10-24T08:45:44.686122Z",
     "start_time": "2024-10-24T08:45:44.660219Z"
    }
   },
   "source": [
    "# Load the test data\n",
    "X_test, test_ids = load_data(url_test, train=False)\n",
    "\n",
    "# Preprocess the test data\n",
    "X_test_scaled, _ = preprocess_data(X_test)\n",
    "\n",
    "# Step 1: Make predictions on the test set\n",
    "y_test_pred_prob = model.predict(X_test_scaled)\n",
    "y_test_pred = (y_test_pred_prob > 0.5).astype(int)  # Threshold of 0.5 for binary classification\n",
    "\n",
    "# Step 2: Create a DataFrame for submission or further analysis\n",
    "results = pd.DataFrame({\n",
    "    'Default_Prediction': y_test_pred.flatten()\n",
    "})\n",
    "\n",
    "# Step 3: Save predictions to a CSV file (optional)\n",
    "results.to_csv('test_predictions.csv', index=False, header=False)\n",
    "\n",
    "# Display predictions\n",
    "print(\"Predictions on Test Set:\")\n",
    "print(results.head())\n"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'url_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[58], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Load the test data\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m X_test, test_ids \u001B[38;5;241m=\u001B[39m load_data(url_test, train\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Preprocess the test data\u001B[39;00m\n\u001B[1;32m      5\u001B[0m X_test_scaled, _ \u001B[38;5;241m=\u001B[39m preprocess_data(X_test)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'url_test' is not defined"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9w-sa4AlaBJg"
   },
   "source": [
    "# OPTIONAL -- Export the predictions in the format indicated in the assignment release page and verify you prediction on the [assessment page](https://aml-assignmentone-2425.streamlit.app/)."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T08:45:44.689622Z",
     "start_time": "2024-10-24T08:45:44.689388Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ]
}
